https://www.youtube.com/watch?v=13r9QY6cmjc

# Introduction
0:00
0:10
OK. Shall we start? This is the second lecture on eigenvalues. So the first lecture was --
0:18
reached the key equation, A x equal lambda x. x is the eigenvector and lambda's the eigenvalue.
0:27
Now to use that. And the, the good way to, after we've found --
0:36
so, so job one is to find the eigenvalues and find the eigenvectors.
0:41
Now after we've found them, what do we do with them? Well, the good way to see that is diagonalize the matrix.
0:50
So the matrix is A. And I want to show -- first of all, this is like the basic fact.
0:57
This, this formula. That's, that's the key to today's lecture.
1:03
This matrix A, I put its eigenvectors in the columns of a matrix S.
1:09
So S will be the eigenvector matrix. And I want to look at this magic combination S inverse A S.
1:19
So can I show you how that -- what happens there? And notice, there's an S inverse.
1:27
We have to be able to invert this eigenvector matrix S.
1:33
So for that, we need n independent eigenvectors. So that's the, that's the case.
1:41
OK. So suppose we have n linearly independent eigenvectors
1:53
of A. Put them in the columns of this matrix S.
2:11
So I'm naturally going to call that the eigenvector matrix, because it's got the eigenvectors in its columns.
2:18
And all I want to do is show you what happens when you multiply A times S.
2:24
So A times S. So this is A times the matrix with the first eigenvector
2:34
in its first column, the second eigenvector in its second column, the n-th eigenvector in its n-th column.
2:42
And how I going to do this matrix multiplication? Well, certainly I'll do it a column at a time.
2:49
And what do I get. A times the first column gives me the first column
2:55
of the answer, but what is it? That's an eigenvector. A times x1 is equal to the lambda times the x1.
3:04
And that lambda's we're -- we'll call lambda one, of course.
3:09
So that's the first column. Ax1 is the same as lambda one x1. A x2 is lambda two x2.
3:16
So on, along to in the n-th column we now how lambda n xn.
3:22
Looking good, but the next step is even better.
3:28
So for the next step, I want to separate out those eigenvalues, those, those multiplying numbers,
3:36
from the x-s. So then I'll have just what I want.
3:41
OK. So how, how I going to separate out? So that, that number lambda one is
3:47
multiplying the first column. So if I want to factor it out of the first column,
3:52
I better put -- here is going to be x1, and that's
3:58
going to multiply this matrix lambda one in the first entry and all zeros.
4:05
Do you see that that, that's going to come out right for the first column?
4:10
Because w- we remember how -- how we're going back to that original punchline.
4:15
That if I want a number to multiply x1 then
4:21
I can do it by putting x1 in that column, in the first column, and putting that number there.
4:27
Th- u- what I going to have here? I'm going to have lambda -- I'm going to have x1, x2, ...
4:33
,xn. These are going to be my columns again. I'm getting S back again.
4:39
I'm getting S back again. But now what's it multiplied by, on the right it's multiplied by?
4:48
If I want lambda n xn in the last column, how do I do it?
4:54
Well, the last column here will be -- I'll take the last column, use these coefficients,
5:02
put the lambda n down there, and it will multiply that n-th column and give me lambda n xn.
5:09
There, there you see matrix multiplication just working for us. So I started with A S.
5:17
I wrote down what it meant, A times each eigenvector. That gave me lambda time the eigenvector.
5:23
And then when I peeled off the lambdas, they were on the right-hand side, so I've got S, my matrix,
5:30
back again. And this matrix, this diagonal matrix, the eigenvalue matrix,
5:45
and I call it capital lambda. Using capital letters for matrices and lambda
5:52
to prompt me that it's, that it's eigenvalues that are in there. So you see that the eigenvalues are just
5:58
sitting down that diagonal? If I had a column x2 here, I would want the lambda two
6:07
in the two two position, in the diagonal position, to multiply that x2 and give me the lambda two x2.
6:15
That's my formula. A S is S lambda.
6:23
OK. That's the -- you see, it's just a calculation. Now -- I mentioned, and I have to mention again,
6:31
this business about n independent eigenvectors.
6:37
As it stands, this is all fine, whether -- I mean, I could be repeating the same eigenvector, but --
6:43
I'm not interested in that. I want to be able to invert S, and that's where this comes in.
6:50
This n independent eigenvectors business comes in to tell me that that matrix is invertible.
6:56
So let me, on the next board, write down what I've got. A S equals S lambda.
7:06
And now I'm, I can multiply on the left by S inverse.
7:11
So this is really --
7:17
I can do that, provided S is invertible. Provided my assumption of n independent eigenvectors is
7:26
satisfied. And I mentioned at the end of last time, and I'll say again,
7:31
that there's a small number of matrices for -- that don't have n independent eigenvectors.
7:38
So I've got to discuss that, that technical point. But the great -- the most matrices that we see have n di-
7:48
n independent eigenvectors, and we can diagonalize. This is diagonalization.
7:54
I could also write it, and I often will, the other way round.
7:59
If I multiply on the right by S inverse, if I took this equation at the top and multiplied on the right
8:06
by S inverse, I could -- I would have A left here. Now S inverse is coming from the right.
8:15
So can you keep those two straight? A multiplies its eigenvectors, that's how I keep them
8:21
straight. So A multiplies S. A multiplies S. And then this S inverse makes the whole thing diagonal.
8:30
And this is another way of saying the same thing, putting the Ss on the other side of the equation.
8:36
A is S lambda S inverse. So that's the, that's the new factorization.
8:44
That's the replacement for L U from elimination or Q R for --
8:50
from Gram-Schmidt. And notice that the matrix -- so it's, it's a matrix times
8:56
a diagonal matrix times the inverse of the first one. It's, that's the combination that we'll
9:02
see throughout this chapter. This combination with an S and an S inverse.
9:09
OK. Can I just begin to use that? For example, what about A squared?
9:17
What are the eigenvalues and eigenvectors of A squared? That's a straightforward question with a,
9:23
with an absolutely clean answer. So let me, let me consider A squared.
9:28
So I start with A x equal lambda x. And I'm headed for A squared.
9:36
So let me multiply both sides by A. That's one way to get A squared on the left.
9:41
So -- I should write these if-s in here. If A x equals lambda x, then I multiply by A,
9:51
so I get A squared x equals -- well, I'm multiplying by A,
9:56
so that's lambda A x. That lambda was a number, so I just put it on the left.
10:02
And what do I -- tell me how to make that look better. What have I got here for if, if A
10:10
has the eigenvalue lambda and eigenvector x, what's up with A squared?
10:16
A squared x, I just multiplied by A, but now for Ax I'm going to substitute lambda x.
10:23
So I've got lambda squared x.
# Conclusion
10:28
So from that simple calculation, I -- my conclusion is that the eigenvalues of A squared are
10:37
lambda squared. And the eigenvectors -- I always think about both of those. What can I say about the eigenvalues?
10:44
They're squared. What can I say about the eigenvectors? They're the same.
10:49
The same x as in -- as for A. Now let me see that also from this formula.
11:00
How can I see what A squared is looking like from this formula? So let me -- that was one way to do it.
11:08
Let me do it by just taking A squared from that. A squared is S lambda S inverse -- that's A --
11:18
times S lambda S inverse -- that's A, which is?
11:25
This is the beauty of eigenvalues, eigenvectors. Having that S inverse and S is the identity,
11:34
so I've got S lambda squared S inverse.
11:40
Do you see what that's telling me? It's, it's telling me the same thing that I just learned here,
11:46
but in the -- in a matrix form. It's telling me that the S is the same,
11:52
the eigenvectors are the same, but the eigenvalues are squared.
11:58
Because this is -- what's lambda squared? That's still diagonal. It's got little lambda one squared,
12:05
lambda two squared, down to lambda n squared o- on that diagonal. Those are the eigenvalues, as we just learned, of A squared.
12:13
OK. So -- somehow those eigenvalues and eigenvectors are really
12:19
giving you a way to --
12:25
see what's going on inside a matrix. Of course I can continue that for --
12:32
to the K-th power, A to the K-th power. If I multiply, if I have K of these together,
12:39
do you see how S inverse S will keep canceling in the, in the inside?
12:44
I'll have the S outside at the far left, and lambda will be in there K times, and S inverse.
12:54
So what's that telling me? That's telling me that the eigenvalues
12:59
of A, of A to the K-th power are the K-th powers. The eigenvalues of A cubed are the cubes of the eigenvalues of
13:08
A. And the eigenvectors are the same, the same.
13:15
OK. In other words, eigenvalues and eigenvectors
13:20
give a great way to understand the powers of a matrix.
13:25
If I take the square of a matrix, or the hundredth power of a matrix, the pivots are all over the place.
13:34
L U, if I multiply L U times L U times L U times L U a hundred times, I've got a hundred L Us.
13:44
I can't do anything with them. But when I multiply S lambda S inverse by itself,
13:50
when I look at the eigenvector picture a hundred times, I get a hundred or ninety-nine of these guys canceling out
13:59
inside, and I get A to the hundredth is S lambda to the hundredth S inverse.
14:05
I mean, eigenvalues tell you about powers of a matrix in a way that we had no way to approach previously.
# Theorem
14:16
For example, when does -- when do the powers of a matrix go to zero?
14:24
I would call that matrix stable, maybe. So I could write down a theorem.
14:31
I'll write it as a theorem just to use that word to emphasize that here I'm getting this great fact
14:40
from this eigenvalue picture. OK. A to the K approaches zero as K goes, as K gets bigger if what?
14:53
What's the w- how can I tell, for a matrix A, if its powers go to zero?
14:59
What's -- somewhere inside that matrix is that information.
15:07
That information is not present in the pivots. It's present in the eigenvalues.
15:13
What do I need for the -- to know that if I take higher and higher powers of A, that this matrix gets smaller
15:20
and smaller? Well, S and S inverse are not moving. So it's this guy that has to get small.
15:26
And that's easy to -- to understand. The requirement is all eigenvalues --
15:32
so what is the requirement?
15:38
The eigenvalues have to be less than one. Now I have to wrote that absolute value,
15:45
because those eigenvalues could be negative, they could be complex numbers.
15:50
So I'm taking the absolute value. If all of those are below one.
15:56
That's, in fact, we practically see why.
16:05
And let me just say that I'm operating on one assumption
16:13
here, and I got to keep remembering that that assumption is still present.
16:18
That assumption was that I had a full set of, of n independent eigenvectors.
16:24
If I don't have that, then this approach is not working.
16:30
So again, a pure eigenvalue approach, eigenvector approach,
16:37
needs n independent eigenvectors. If we don't have n independent eigenvectors,
16:42
we can't diagonalize the matrix. We can't get to a diagonal matrix.
16:50
This diagonalization is only possible
16:55
if S inverse makes sense. OK. Can I, can I follow up on that point now?
17:02
So you see why -- what we get and, and why we want it, because we get information about the powers of a matrix just
17:11
immediately from the eigenvalues. OK. Now let me follow up on this, business of which matrices
# Diagonalizable matrices
17:22
are diagonalizable. Sorry about that long word.
17:28
So a matrix is, is sure -- so here's, here's the main point. A is sure to be --
17:37
to have N independent eigenvectors and, and be --
17:50
now here comes that word -- diagonalizable if, if --
18:00
so we might as well get the nice case out in the open.
18:05
The nice case is when -- if all the lambdas are different.
18:13
18:19
That means, that means no repeated eigenvalues.
18:28
OK. That's the nice case.
18:34
If my matrix, and most -- if I do a random matrix in Matlab
18:39
and compute its eigenvalues -- so if I computed if I took eig of rand of ten ten, gave,
18:55
gave that Matlab command, the -- we'd get a random ten by ten matrix,
19:01
we would get a list of its ten eigenvalues, and they would be different.
19:08
They would be distinct is the best word. I would have -- a random matrix will have ten distinct --
19:13
a ten by ten matrix will have ten distinct eigenvalues. And if it does, the eigenvectors are automatically independent.
# Repeated eigenvalues
19:25
So that's a nice fact. I'll refer you to the text for the proof. That, that A is sure to have n independent eigenvectors
19:36
if the eigenvalues are different, if.
19:41
If all the, if all eigenvalues are different. It's just if some lambdas are repeated,
19:47
then I have to look more closely. If an eigenvalue is repeated, I have to look, I have to count,
19:55
I have to check. Has it got -- say it's repeated three times. So what's a possibility for the --
20:02
so here is the, here is the repeated possibility.
20:11
And, and let me emphasize the conclusion.
20:16
That if I have repeated eigenvalues, I may or may not, I may or may not have, have n independent eigenvectors.
20:35
I might. I, I, you know, this isn't a completely negative case.
20:41
The identity matrix -- suppose I take the ten by ten identity matrix.
20:46
What are the eigenvalues of that matrix? So just, just take the easiest matrix, the identity.
20:55
If I look for its eigenvalues, they're all ones.
21:00
So that eigenvalue one is repeated ten times. But there's no shortage of eigenvectors for the identity
21:07
matrix. In fact, every vector is an eigenvector. So I can take ten independent vectors.
21:13
Oh, well, what happens to everything -- if A is the identity matrix, let's just think that one through in our head.
21:21
If A is the identity matrix, then it's
21:27
got plenty of eigenvectors. I choose ten independent vectors. They're the columns of S.
21:32
And, and what do I get from S inverse A S? I get I again, right?
21:39
If A is the identity -- and of course that's the correct lambda. The matrix was already diagonal.
21:46
So if the matrix is already diagonal, then the, the lambda is the same as the matrix.
21:53
A diagonal matrix has got its eigenvalues sitting right there in front of you.
21:59
Now if it's triangular, the eigenvalues are still sitting there, but so let's
22:04
take a case where it's triangular. Suppose A is like, two one two zero.
22:14
So there's a case that's going to be trouble.
22:23
There's a case that's going to be trouble. First of all, what are the -- I mean, we just --
22:29
if we start with a matrix, the first thing we do, practically without thinking is compute the eigenvalues and eigenvectors.
22:36
OK. So what are the eigenvalues? You can tell me right away what they are. They're two and two, right.
22:43
It's a triangular matrix, so when I do this determinant, shall I do this determinant of A minus lambda I?
22:51
I'll get this two minus lambda one zero two minus lambda,
22:59
right? I take that determinant, so I make those into vertical bars
23:06
to mean determinant. And what's the determinant? It's two minus lambda squared.
23:13
What are the roots? Lambda equal two twice. So the eigenvalues are lambda equals two and two.
23:22
OK, fine. Now the next step, find the eigenvectors. So I look for eigenvectors, and what do I find for this guy?
23:31
Eigenvectors for this guy, when I subtract two minus the identity, so A minus two
23:38
I has zeros here.
23:45
And I'm looking for the null space. What's, what are the eigenvectors? They're the -- the null space of A minus lambda I.
23:56
The null space is only one dimensional. This is a case where I don't have enough eigenvectors.
24:03
My algebraic multiplicity is two. I would say, when I see, when I count
24:10
how often the eigenvalue is repeated,
24:16
that's the algebraic multiplicity. That's the multiplicity, how many times is it the root of the polynomial?
24:22
My polynomial is two minus lambda squared.
24:28
It's a double root. So my algebraic multiplicity is two. But the geometric multiplicity, which looks for vectors,
24:37
looks for eigenvectors, and -- which means the null space
24:42
of this thing, and the only eigenvector is one zero.
24:47
That's in the null space. Zero one is not in the null space.
24:52
The null space is only one dimensional. So there's a matrix, my -- this A or the original A,
24:58
that are not diagonalizable. I can't find two independent eigenvectors.
25:06
There's only one. OK. So that's the case that I'm --
25:11
that's a case that I'm not really handling. For example, when I wrote down up here
25:19
that the powers went to zero if the eigenvalues were below one, I didn't really handle that case of repeated eigenvalues,
25:29
because my reasoning was based on this formula. And this formula is based on n independent eigenvectors.
25:36
OK. Just to say then, there are some matrices that we're, that,
25:45
that we don't cover through diagonalization, but the great majority we do.
25:51
OK. And we, we're always OK if we have different distinct eigenvalues.
25:56
OK, that's the, like, the typical case.
26:02
Because for each eigenvalue there's at least one eigenvector. The algebraic multiplicity here is one for every eigenvalue
26:11
and the geometric multiplicity is one. There's one eigenvector. And they are independent.
26:17
OK. OK. Now let me come back to the important case, when,
26:26
when we're OK. The important case, when we are diagonalizable.
26:31
Let me, look at --
26:38
so -- let me solve this equation.
26:46
The equation will be each -- I start with some -- start with a given vector u0.
26:57
27:02
And then my equation is at every step, I multiply what I have by A.
27:11
That, that equation ought to be simple to handle.
27:19
And I'd like to be able to solve it. How would I find -- if I start with a vector u0 and I multiply
27:26
by A a hundred times, what have I got? Well, I could certainly write down a formula for the answer,
27:35
so what, what -- so u1 is A u0.
27:42
And u2 is -- what's u2 then? u2, I multiply -- u2 I get from u1 by another multiplying by A,
27:52
so I've got A twice. And my formula is uk, after k steps,
28:02
I've multiplied by A k times the original u0.
28:07
You see what I'm doing? The next section is going to solve systems
28:14
of differential equations. I'm going to have derivatives.
# Difference equations
28:19
This section is the nice one. It solves difference equations.
28:26
I would call that a difference equation. It's -- at first order, I would call that a first-order system,
28:33
because it connects only -- it only goes up one level.
28:40
And I -- it's a system because these are vectors and that's a matrix.
28:45
And the solution is just that. OK. But, that's a nice formula.
28:55
That's the, like, the most compact formula I could ever get. u100 would be A to the one hundred u0.
29:01
But how would I actually find u100? How would I find -- how would I discover what u100 is?
29:11
Let me, let me show you how.
29:16
Here's the idea. If -- so to solve, to really solve -- shall I say,
29:23
to really solve -- to really solve it, I would take this initial vector u0
29:34
and I would write it as a combination of eigenvectors. To really solve, write u nought as a combination,
29:47
say certain amount of the first eigenvector plus a certain amount of the second eigenvector
29:53
plus a certain amount of the last eigenvector.
30:01
Now multiply by A. You want to -- you got to see the magic of eigenvectors
30:07
working here. Multiply by A. So Au0 is what?
30:13
So A times that. A times -- so what's A -- I can separate it out into n separate pieces,
30:21
and that's the whole point. That each of those pieces is going in its own merry way.
30:28
Each of those pieces is an eigenvector, and when I multiply by A, what does this piece become?
30:35
So that's some amount of the first -- let's suppose the eigenvectors are normalized to be unit
30:41
vectors. So that says what the eigenvector is.
30:48
It's a -- And I need some multiple of it to produce u0.
30:55
OK. Now when I multiply by A, what do I get? I get c1, which is just a factor, times Ax1,
31:04
but Ax1 is lambda one x1.
31:10
When I multiply this by A, I get c2 lambda two x2.
31:17
And here I get cn lambda n xn. And suppose I multiply by A to the hundredth power now.
31:27
Can we, having done it, multiplied by A, let's multiply by A to the hundredth. What happens to this first term when I multiply by A to the one
31:36
hundredth? It's got that factor lambda to the hundredth.
31:41
That's the key. That -- that's what I mean by going its own merry way.
31:48
It, it is pure eigenvector. It's exactly in a direction where multiplication by A
31:55
just brings in a scalar factor, lambda one. So a hundred times brings in this a hundred times.
32:02
Hundred times lambda two, hundred times lambda n. Actually, we're -- what are we seeing here?
32:08
We're seeing, this same, lambda capital
32:15
lambda to the hundredth as in the, as in the diagonalization. And we're seeing the S matrix, the,
32:22
the matrix S of eigenvectors. That's what this has got to -- this has got to amount to.
32:29
A lambda to the hundredth power times an S times this vector c
32:40
that's telling us how much of each one is in the original thing. So if, if I had to really find the hundredth power,
32:49
I would take u0, I would expand it as a combination
32:54
of eigenvectors -- this is really S, the eigenvector matrix, times c, the, the coefficient vector.
33:01
And then I would immediately then,
33:07
by inserting these hundredth powers of eigenvalues, I'd have the answer.
33:15
So -- huh, there must be -- oh, let's see, OK.
33:20
It's -- so, yeah. So if u100 is A to the hundredth times u0, and u0 is S c --
33:30
then you see this formula is just this formula,
33:36
which is the way I would actually get hold of this, of this u100, which is --
33:44
let me put it here. u100. The way I would actually get hold of that, see what,
33:51
what the solution is after a hundred steps, would be --
33:57
expand the initial vector into eigenvectors and let each
34:05
eigenvector go its own way, multiplying by a hundred at -- by lambda at every step, and therefore by lambda
34:13
to the hundredth power after a hundred steps. Can I do an example? So that's the formulas.
34:20
Now let me take an example. I'll use the Fibonacci sequence as an example.
# Fibonacci example
34:29
So, so Fibonacci example.
34:39
You remember the Fibonacci numbers? If we start with one and one as F0 -- oh,
34:48
I think I start with zero, maybe. Let zero and one be the first ones.
34:54
So there's F0 and F1, the first two Fibonacci numbers. Then what's the rule for Fibonacci numbers?
35:02
Ah, they're the sum. The next one is the sum of those, so it's one.
35:08
The next one is the sum of those, so it's two. The next one is the sum of those, so it's three.
35:14
Well, it looks like one two three four five, but somehow it's not going to do that way.
35:19
The next one is five, right. Two and three makes five. The next one is eight.
35:26
The next one is thirteen. And the one hundredth Fibonacci number is what?
35:33
That's my question. How could I get a formula for the hundredth number?
35:40
And, for example, how could I answer the question, how fast are they growing?
35:47
How fast are those Fibonacci numbers growing? They're certainly growing.
35:54
It's not a stable case. Whatever the eigenvalues of whatever matrix it is, they're not smaller than one.
36:00
These numbers are growing. But how fast are they growing? The answer lies in the eigenvalue.
36:10
So I've got to find the matrix, so let me write down the Fibonacci rule.
36:17
F(k+2) = F(k+1)+F k, right?
36:25
Now that's not in my -- I want to write that as uk plus one and Auk.
36:32
But right now what I've got is a single equation, not a system,
36:38
and it's second-order. It's like having a second-order differential equation
36:44
with second derivatives. I want to get first derivatives. Here I want to get first differences. So the way, the way to do it is to introduce uk will be
36:55
a vector -- see, a small trick.
37:01
Let uk be a vector, F(k+1) and Fk.
37:08
So I'm going to get a two by two system, first order, instead of a one -- instead of a scalar system, second order,
37:16
by a simple trick. I'm just going to add in an equation F(k+1) equals F(k+1).
37:22
That will be my second equation.
37:28
Then this is my system, this is my unknown, and what's my one step equation?
37:38
So, so now u(k+1), that's -- so u(k+1) is the left side,
37:45
and what have I got here on the right side? I've got some matrix multiplying uk.
37:52
Can you, do -- can you see that all right? if you can see it, then you can tell me what the matrix is.
37:59
Do you see that I'm taking my system here. I artificially made it into a system.
38:06
I artificially made the unknown into a vector. And now I'm ready to look at and see what the matrix
38:14
is. So do you see the left side, u(k+1) is F(k+2) F(k+1),
38:20
that's just what I want. On the right side, this remember, this uk here --
38:25
let me for the moment put it as F(k+1) Fk. So what's the matrix?
38:33
Well, that has a one and a one, and that has a one and a zero.
38:41
There's the matrix. Do you see that that gives me the right-hand side?
38:47
So there's the matrix A. And this is our friend uk.
38:56
So we've got -- so that simple trick -- changed the second-order scalar problem
39:03
to a first-order system. Two b- u- with two unknowns. With a matrix.
39:10
And now what do I do? Well, before I even think, I find its eigenvalues
39:16
and eigenvectors. So what are the eigenvalues and eigenvectors of that matrix?
39:23
Let's see. I always -- first let me just, like, think for a minute.
39:29
It's two by two, so this shouldn't be impossible to do.
39:35
Let's do it. OK. So my matrix, again, is one one one zero.
39:43
It's symmetric, by the way.
39:49
So what I will eventually know about symmetric matrices
39:56
is that the eigenvalues will come out real. I won't get any complex numbers here.
40:02
And the eigenvectors, once I get those, actually will be orthogonal.
40:08
But two by two, I'm more interested in what the actual numbers are.
40:13
What do I know about the two numbers? Well, should do you want me to find this determinant of A minus
40:19
lambda I? Sure. So it's the determinant of one minus lambda one one zero,
40:27
right? Minus lambda, yes.
40:33
God. OK. OK.
40:40
There'll be two eigenvalues. What will -- tell me again what I know about the two
40:45
eigenvalues before I go any further. Tell me something about these two eigenvalues. What do they add up to?
40:51
Lambda one plus lambda two is? Is the same as the trace down the diagonal of the matrix.
41:02
One and zero is one. So lambda one plus lambda two should come out to be one.
41:08
And lambda one times lambda one times lambda two should come out to be the determinant, which is minus one.
41:15
So I'm expecting the eigenvalues to add to one and to multiply to minus one.
41:20
But let's just see it happen here. If I multiply this out, I get -- that times that'll be a lambda
41:26
squared minus lambda minus one. Good.
41:33
Lambda squared minus lambda minus one. Actually, I -- you see the b- compare that with the original
41:43
equation that I started with.
41:48
F(k+2) - F(k+1)-Fk is zero. The recursion that -- that the Fibonacci numbers satisfy is
42:00
somehow showing up directly here for the eigenvalues when we set that to zero.
42:06
WK. Let's solve. Well, I would like to be able to factor that, that quadratic,
42:14
but I'm better off to use the quadratic formula. Lambda is -- let's see.
42:19
Minus b is one plus or minus the square root of b squared,
42:25
which is one, minus four times that times that, which is plus four, over two.
42:33
So that's the square root of five.
42:39
So the eigenvalues are lambda one is one half of one
42:50
plus square root of five, and lambda two is one half of one
42:57
minus square root of five. And sure enough, they -- those add up to one and they multiply
43:04
to give minus one. OK. Those are the two eigenvalues. How -- what are those numbers approximately?
43:12
Square root of five, well, it's more than two
43:18
but less than three. Hmm. It'd be nice to know these numbers.
43:25
I think, I think that -- so that number comes out bigger than
43:30
one, right? That's right. This number comes out bigger than one. It's about one point six one eight or something.
43:38
Not exactly, but.
43:44
And suppose it's one point six. Just, like, I think so.
43:52
Then what's lambda two? Is, is lambda two positive or negative?
43:57
Negative, right, because I'm -- it's, obviously negative, and I knew that the -- so it's minus --
44:07
and they add up to one, so minus point six one eight, I guess.
44:16
OK. A- and some more. Those are the two eigenvalues. One eigenvalue bigger than one, one eigenvalue smaller than one. Actually, that's a great situation to be in.
44:22
Of course, the eigenvalues are different, so there's no doubt whatever -- is this matrix diagonalizable?
44:29
Is this matrix diagonalizable, that original matrix A?
44:35
Sure. We've got two distinct eigenvalues and we can find the eigenvectors in a moment.
44:44
But they'll be independent, we'll be diagonalizable. And now, you, you can already answer my very first question.
44:54
How fast are those Fibonacci numbers increasing? How -- those -- they're increasing,
45:01
right? They're not doubling at every step. Let me -- let's look again at these numbers.
45:07
Five, eight, thirteen, it's not obvious. The next one would be twenty-one, thirty-four.
45:14
So to get some idea of what F one hundred is,
45:20
can you give me any -- I mean the crucial number -- so it -- these -- it's approximately --
45:32
what's controlling the growth of these Fibonacci numbers?
45:37
It's the eigenvalues. And which eigenvalue is controlling that growth?
45:43
The big one. So F100 will be approximately some constant, c1 I guess,
45:50
times this lambda one, this one plus square root of five
45:56
over two, to the hundredth power.
46:01
And the two hundredth F -- in other words, the eigenvalue -- the Fibonacci numbers are growing by about that factor.
46:08
Do you see that we, we've got precise information about the, about the Fibonacci numbers out of the eigenvalues?
46:18
OK. And again, why is that true? Let me go over to this board and s- show what I'm doing here.
46:26
The -- the original initial value is some combination of eigenvectors.
46:35
And then when we start -- when we start going out the theories of Fibonacci numbers, when we start multiplying by A
46:42
a hundred times, it's this lambda one to the hundredth. This term is, is the one that's taking over.
46:51
It's -- I mean, that's big, like one point six to the hundredth power. The second term is practically nothing, right?
47:00
The point six, or minus point six, to the hundredth power is an extremely small, extremely small number.
47:08
So this is -- there're only two terms, because we're two by two. This number is -- this piece of it is there,
47:16
but it's, it's disappearing, where this piece is there and it's growing and controlling everything.
47:23
So, so really the -- we're doing, like, problems that are evolving.
47:29
We're doing dynamic u- instead of Ax=b, that's a static problem.
47:35
We're now we're doing dynamics. A, A squared, A cubed, things are evolving in time. And the eigenvalues are the crucial, numbers.
47:44
OK. I guess to complete this, I better
47:52
write down the eigenvectors. So we should complete the, the whole process
47:59
by finding the eigenvectors. OK, well, I have to -- up in the corner, then, I have to look at A minus lambda I.
48:07
So A minus lambda I is this one minus lambda one one and minus
48:15
lambda. And now can we spot an eigenvector out of that? That's, that's, for these two lambdas,
48:23
this matrix is singular. I guess the eigenvector -- two by two ought to be, I mean,
48:30
easy. So if I know that this matrix is singular, then u- seems to me the eigenvector
48:37
has to be lambda and one, because that multiplication will give me the zero.
48:43
And this multiplication gives me -- better give me also zero. Do you see why it does?
48:48
This is the minus lambda squared plus lambda plus one. It's the thing that's zero because these lambdas are
48:56
special. There's the eigenvector. x1 is lambda one one, and x2 is lambda two one.
49:07
I did that as a little trick that was available in the two by two case.
49:14
So now I finally have to -- oh, I have to take the initial u0 now.
49:20
So to complete this example entirely, I have to say, OK, what was u0?
49:26
u0 was F1 F0. So u0, the starting vector is F1 F0, and those were one and
49:40
zero. So I have to use that vector.
49:47
So I have to look for, for a multiple of the first eigenvector and the second to produce u0,
49:56
the one zero vector. This is what will find c1 and c2, and then I'm done.
50:05
Do you -- so let me instead of, in the last five seconds,
50:10
grinding out a formula, let me repeat the idea. Because I'd really -- it's the idea that's central.
50:19
When things are evolving in time -- let me come back to this board, because the ideas are here.
50:25
When things are evolving in time by a first-order system, starting from an original u0, the key
50:34
is find the eigenvalues and eigenvectors of A.
50:39
That will tell -- those eigenvectors -- the eigenvalues will already tell you what's happening.
50:46
Is the solution blowing up, is it going to zero, what's it doing. And then to, to find out exactly a formula,
50:56
you have to take your u0 and write it as a combination of eigenvectors and then
51:03
follow each eigenvector separately. And that's really what this formula, the formula for, --
51:10
that's what the formula for A to the K is doing. So remember that formula for A to the K
51:17
is S lambda to the K S inverse. OK.
51:22
That's, that's difference equations. And you just have to -- so the, the homework will give some
51:33
examples, different from Fibonacci, to follow through.
51:41
And next time will be differential equations.
51:48
Thanks.