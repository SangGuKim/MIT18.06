https://www.youtube.com/watch?v=lGGDIGizcQ0

# Introduction
0:00
-- two, one and -- okay.
0:11
Here is a lecture on the applications of eigenvalues
0:18
and, if I can -- so that will be Markov matrices.
0:24
I'll tell you what a Markov matrix is, so this matrix A will be a Markov matrix
0:32
and I'll explain how they come in applications.
0:38
And -- and then if I have time, I would like to say a little bit about Fourier series, which is a fantastic application
0:46
of the projection chapter. Okay. What's a Markov matrix? Can I just write down a typical Markov matrix, say .1, .2, .7,
# Markov Matrix
1:01
.01, .99 0, let's say, .3, .3, .4.
1:08
Okay. There's a -- a totally just invented Markov matrix.
1:13
What makes it a Markov matrix?
1:19
Two properties that this -- this matrix has. So two properties are --
1:24
one, every entry is greater equal zero.
1:30
All entries greater than or equal to zero.
1:38
And, of course, when I square the matrix, the entries will still be greater/equal zero.
1:45
I'm going to be interested in the powers of this matrix. And this property, of course, is going to -- stay there.
1:54
It -- really Markov matrices you'll see are connected to probability ideas and probabilities are never
2:04
negative. The other property -- do you see the other property in there?
2:09
If I add down the columns, what answer do I get? One. So all columns add to one.
2:17
All columns add to one.
2:25
And actually when I square the matrix,
2:31
that will be true again. So that the powers of my matrix are all
2:38
Markov matrices, and I'm interested in, always, the eigenvalues and the eigenvectors.
2:46
And this question of steady state will come up. You remember we had steady state for differential equations
2:53
last time? When -- what was the steady state -- what was the eigenvalue?
2:59
What was the eigenvalue in the differential equation case that led to a steady state?
3:05
It was lambda equals zero. When -- you remember that we did an example and one
3:11
of the eigenvalues was lambda equals zero, and that -- so then we had an E to the zero T, a constant one --
3:20
as time went on, there that thing stayed steady. Now what -- in the powers case, it's not a zero eigenvalue.
3:30
Actually with powers of a matrix, a zero eigenvalue, that part is going to die right away.
3:36
It's an eigenvalue of one that's all important.
3:41
So this steady state will correspond -- will be totally connected with an eigenvalue of one and its
3:49
eigenvector. In fact, the steady state will be the eigenvector for that eigenvalue.
3:55
Okay. So that's what's coming. Now, for some reason then that we have to see,
4:04
this matrix has an eigenvalue of one. This property, that the columns all add to one --
4:12
turns out -- guarantees that one is an eigenvalue, so that you can actually find the eigenvalue --
4:19
find that eigenvalue of a Markov matrix without computing any determinants of A minus lambda I --
4:27
that matrix will have an eigenvalue of one, and we want to see why. And then the other thing is --
# Key Points
4:35
so the key points -- let me -- let me write these underneath. The key points are -- the key points are lambda equal one is
4:45
an eigenvalue.
4:53
I'll add in a little -- an additional -- well, a thing about eigenvalues --
4:59
key point two, the other eigenval- values -- all other eigenvalues are, in magnitude, smaller than one --
5:12
in absolute value, smaller than one. Well, there could be some exceptional case when --
5:18
when an eigen -- another eigenvalue might have magnitude equal one. It never has an eigenvalue larger than one.
5:26
So these two facts -- somehow we ought to -- linear algebra ought to tell us.
5:32
And then, of course, linear algebra is going to tell us what the -- what's -- what happens if I take -- if --
5:40
you remember when I solve -- when I multiply by A time after time the K-th thing is A
5:47
to the K u0 and I'm asking what's special about this --
5:54
these powers of A, and very likely the quiz will have
6:01
a problem to computer s- to computer some powers of A or --
6:06
or applied to an initial vector. So, you remember the general form? The general form is that there's some amount
# General Form
6:14
of the first eigenvalue to the K-th power times the first eigenvector, and another amount
6:22
of the second eigenvalue to the K-th power times the second eigenvector and so on. A -- just --
6:30
my conscience always makes me say at least once per lecture that this requires a complete set of eigenvectors,
6:40
otherwise we might not be able to expand u0 in the eigenvectors and we couldn't get started.
6:45
But once we're started with u0 when K is zero, then every A brings in these lambdas.
6:53
And now you can see what the steady state is going to be. If lambda one is one -- so lambda one equals one
7:01
to the K-th power and these other eigenvalues are smaller
7:07
than one -- so I've sort of scratched over the equation there to --
7:14
we had this term, but what happens to this term -- if the lambda's smaller than one, then the -- when --
7:21
as we take powers, as we iterate as we -- as we go forward in time, this goes to zero, Can I just --
7:30
having scratched over it, I might as well scratch right? further. That term and all the other terms are going to zero
7:36
because all the other eigenvalues are smaller than one and the steady state that we're approaching is just --
7:45
whatever there was -- this was -- this was the -- this is the x1 part of un- of the initial condition u0 --
7:56
is the steady state.
8:05
This much we know from general -- from -- you know, what we've already done.
8:11
So I want to see why -- let's at least see number one, why one is an eigenvalue.
8:18
And then there's actually -- in this chapter we're interested not only in eigenvalues,
8:23
but also eigenvectors. And there's something special about the eigenvector.
8:28
Let me write down what that is. The eigenvector x1 --
8:36
x1 is the eigenvector and all its components are positive,
8:41
so the steady state is positive, if the start was.
8:49
If the start was -- so -- well, actually, in general, I --
8:54
this might have a -- might have some component zero always,
9:00
but no negative components in that eigenvector. Okay. Can I come to that point?
9:08
How can I look at that matrix -- so that was just an example.
9:15
How could I be sure -- how can I see that a matrix --
9:21
if the columns add to zero -- add to one, sorry -- if the columns add to one, this property means that lambda
9:30
equal one is an eigenvalue. Okay. So let's just think that through.
9:37
What I saying about -- let me ca- let me look at A, and if I believe that one is an eigenvalue,
9:44
then I should be able to subtract off one times the identity and then I would get a matrix that's, what, -.9,
9:56
-.01 and -.6 -- wh- I took the ones away and the other parts, of course,
10:02
are still what they were, and this is still .2 and .7 and --
10:11
okay, what's -- what's up with this matrix now? I've shifted the matrix, this Markov matrix by one,
10:19
by the identity, and what do I want to prove? I -- what is it that I believe this matrix --
10:28
about this matrix? I believe it's singular. Singular will -- if A minus I is singular,
10:36
that tells me that one is an eigenvalue, right? The eigenvalues are the numbers that I subtract off --
10:43
the shifts -- the numbers that I subtract from the diagonal -- to make it singular.
10:48
Now why is that matrix singular? I -- we could compute its determinant,
10:55
but we want to see a reason that would work for every Markov matrix not just this particular random example.
11:05
So what is it about that matrix? Well, I guess you could look at its columns now --
11:11
what do they add up to? Zero. The columns add to zero, so all columns --
11:21
let me put all columns now of -- of -- of A minus I add to zero, and then I
11:32
want to realize that this means A minus I is singular.
11:40
11:45
Okay. Why? So I could I -- you know, that could be a quiz question,
11:52
a sort of theoretical quiz question. If I give you a matrix and I tell you all its columns add to zero, give me a reason,
12:00
because it is true, that the matrix is singular.
12:06
Okay. I guess actually -- now what -- I think of -- you know, I'm thinking of two or three ways
12:11
to see that.
12:17
How would you do it? We don't want to take its determinant somehow. For the matrix to be singular, well,
12:24
it means that these three columns are dependent, right?
12:31
The determinant will be zero when those three columns are dependent. You see, we're -- we're at a point in this course, now,
12:36
where we have several ways to look at an idea. We can take the determinant -- here we don't want to.
12:42
B- but we met singular before that -- those columns are dependent.
12:47
So how do I see that those columns are dependent? They all add to zero.
12:57
Let's see, whew -- well, oh, actually, what --
13:05
another thing I know is that the -- I would like to be able to show is that the rows are dependent.
13:11
Maybe that's easier. If I know that all the columns add to zero, that's my information, how
13:18
do I see that those three rows are linearly dependent?
13:25
What -- what combination of those rows gives the zero row?
13:30
How -- how could I combine those three rows -- those three row vectors to produce the zero row vector?
13:38
And that would tell me those rows are dependent, therefore the columns are dependent, the matrix is singular, the determinant is zero -- well,
13:46
you see it. I just add the rows. One times that row plus one times that row plus one times
13:52
that row -- it's the zero row. The rows are dependent.
13:58
In a way, that one one one, because it's multiplying the rows, is like an eigenvector in the --
14:07
it's in the left null space, right? One one one is in the left null space.
# Null Space
14:12
It's singular because the rows are dependent --
14:23
and can I just keep the reasoning going? Because this vector one one one is --
14:32
it's not in the null space of the matrix, but it's in the null space of the transpose -- is in the null space of the transpose.
14:43
And that's good enough. If we have a square matrix -- if we have a square matrix
14:48
and the rows are dependent, that matrix is singular. So it turned out that the immediate guy we could identify
14:58
was one one one. Of course, the --
15:04
there will be somebody in the null space, too. And actually, who will it be?
15:10
So what's -- so -- so now I want to ask about the null space of --
15:15
of the matrix itself. What combination of the columns gives zero?
15:20
I -- I don't want to compute it because I just made up this matrix and -- it will -- it would take me a while --
15:28
it looks sort of doable because it's three by three but wh- my point is, what --
15:34
what vector is it if we -- once we've found it, what have we got that's in the -- in the null space of A?
15:41
It's the eigenvector, right? That's where we find X one. Then X one, the eigenvector, is in the null space of A.
15:54
That's the eigenvector corresponding to the eigenvalue one. Right? That's how we find eigenvectors.
16:01
So those three columns must be dependent -- some combination of columns -- of those three columns is
16:08
the zero column and that -- the three components in that combination are the eigenvector.
16:14
And that guy is the steady state. Okay. So I'm happy about the --
16:21
the thinking here, but I haven't given -- I haven't completed it because I haven't found x1.
16:29
But it's there. Can I -- another thought came to me as I was doing this,
16:36
another little comment that -- you -- about eigenvalues and eigenvectors,
16:41
because of A and A transpose. What can you tell me about eigenvalues of A --
# Eigenvalues of transposes
16:50
of A and eigenvalues of A transpose?
16:56
Whoops.
17:01
They're the same. They're -- so this is a little comment -- we -- it's useful,
17:11
since eigenvalues are generally not easy to find -- it's always useful to know some cases where you've got them,
17:19
where -- and this is -- if you know the eigenvalues of A, then you know the eigenvalues of A transpose.
17:25
eigenvalues of A transpose are the same.
17:32
And can I just, like, review why that is?
17:37
So to find the eigenvalues of A, this would be determinate of A
17:44
minus lambda I equals zero, that gives me an eigenvalue of A --
17:53
now how can I get A transpose into the picture here?
17:58
I'll use the fact that the determinant of a matrix and the determinant of its transpose are the same.
18:06
The determinant of a matrix equals the determinant of a -- of the transpose. That was property ten, the very last guy
18:13
in our determinant list. So I'll transpose that matrix. This leads to --
18:21
I just take the matrix and transpose it, but now what do I get when I transpose lambda I?
18:29
I just get lambda I. So that's -- that's all there was to the reasoning.
18:37
The reasoning is that the eigenvalues of A solved that equation. The determinant of a matrix is the determinant
18:44
of its transpose, so that gives me this equation and that tells me that the same lambdas
18:50
are eigenvalues of A transpose. So that, backing up to the Markov case,
18:56
one is an eigenvalue of A transpose and we actually found its eigenvector, one one one, and that tell us that one is
19:05
also an eigenvalue of A -- but, of course, it has a different eigenvector, the -- the left null space isn't the same as the null space and we
19:13
would have to find it. So there's some vector here which is x1 that produces zero
19:20
zero zero. Actually, it wouldn't be that hard to find, you know, I -- as I'm talking I'm thinking, okay,
19:26
I going to follow through and actually find it? Well, I can tell from this one -- look,
19:32
if I put a point six there and a point seven there, that's what -- then I'll be okay in the last row, right?
19:43
Now I only -- remains to find one guy. And let me take the first row, then.
19:49
Minus point 54 plus point 21 -- there's some big number going in there, right?
19:56
So I have -- just to make the first row come out zero, I'm getting minus point 54 plus point 21,
20:03
so that was minus point 33 and what -- what do I want?
20:09
Like thirty three hundred? This is the first time in the history of linear algebra that an eigenvector has every had a component
20:16
thirty three hundred. But I guess it's true. Because then I multiply by minus one over a hundred -- oh no,
20:24
it was point 33. So is this just -- oh, shoot. Only 33.
20:30
Okay. Only 33. Okay, so there's the eigenvector. Oh, and notice that it -- that it turned -- did turn out,
20:39
at least, to be all positive. So that was, like, the theory -- predicts that part, too.
20:45
I won't give the proof of that part. So 30 -- 33 -- point six 33 point seven.
20:52
Okay. Now those are the ma- that's the linear algebra part.
20:58
Can I get to the applications? Where do these Markov matrices come from? Because that's -- that's part of this course and absolutely part
21:06
of this lecture. Okay. So where's -- what's an application of Markov matrices?
21:12
Okay.
# Example
21:17
Markov matrices -- so, my equation, then, that I'm solving and studying is this equation u(k+1)=Auk.
21:24
And now A is a Markov matrix.
21:31
A is Markov. And I want to give an example.
21:39
Can I just create an example? It'll be two by two.
21:44
And it's one I've used before because it seems to me to bring out the idea.
21:50
It's -- because we have two by two, we have two states, let's say California and Massachusetts.
22:01
And I'm looking at the populations in those two states, the people in those two states, California
22:08
and Massachusetts. And my matrix A is going to tell me in a -- in a year,
22:17
some movement has happened. Some people stayed in Massachusetts, some people moved to California, some smart people
22:24
moved from California to Massachusetts, some people stayed in California and made a billion.
22:29
Okay. So that -- there's a matrix there with four entries
22:36
and those tell me the fractions of my population -- so I'm making --
22:41
I'm going to use fractions, so they won't be negative, of course, because -- because only positive people are
22:48
in- involved here -- and they'll add up to one, because I'm accounting for all people.
22:54
So that's why I have these two key properties. The entries are greater equal zero
22:59
because I'm looking at probabilities. Do they move, do they stay?
23:06
Those probabilities are all between zero and one. And the probabilities add to one because everybody's
23:12
accounted for. I'm not losing anybody, gaining anybody in this Markov chain.
23:17
It's -- it conserves the total population. Okay.
23:22
So what would be a typical matrix, then? So this would be u, California and u Massachusetts at time t
23:34
equal k+1. And it's some matrix, which we'll
23:40
think of, times u California and u Massachusetts at time k.
23:48
And notice this matrix is going to stay the same,
23:54
you know, forever. So that's a severe limitation on the example.
24:01
The example has a -- the same Markov matrix, the same probabilities act at every time.
24:08
Okay. So what's a reasonable, say -- say point nine of the people in California at time k
24:16
stay there. And point one of the people in California
24:24
move to Massachusetts. Notice why that column added to one, because we've now accounted for all the people in California
24:32
at time k. Nine tenths of them are still in California, one tenth are here at time k+1.
24:41
Okay. What about the people who are in Massachusetts? This is going to multiply column two, right,
24:47
by our fundamental rule of multiplying matrix by vector, it's the -- it's the population in Massachusetts.
24:57
Shall we say that -- that after the Red Sox, fail again,
25:06
eight -- only 80 percent of the people in Massachusetts stay
25:11
and 20 percent move to California. Okay. So again, this adds to one, which
25:18
accounts for all people in Massachusetts where they are. So there is a Markov matrix.
25:26
Non-negative entries adding to one. What's the steady state? If everybody started in Massachusetts, say, at --
25:33
you know, when the Pilgrims showed up or something. Then where are they now?
25:40
Where are they at time 100, let's say, or maybe -- I don't know, how many years since the Pilgrims?
25:47
300 and something. Or -- and actually where will they be, like, way out a million years from now?
25:54
I -- I could multiply --
26:00
take the powers of this matrix. In fact, you'll -- you would -- ought to be able to figure out
26:07
what is the hundredth power of that matrix? Why don't we do that?
26:12
But let me follow the steady state. So what -- what's my starting --
26:17
my starting u Cal, u Mass at time zero is, shall we say --
26:28
shall we put anybody in California? Let's make -- let's make zero there, and say the population of Massachusetts is --
26:36
let's say a thousand just to -- okay.
26:43
So the population is -- so the populations are zero and a thousand at the start.
26:49
What can you tell me about this population after -- after k steps?
26:55
What will u Cal plus u Mass add to?
27:01
A thousand. Those thousand people are always accounted for. But -- so u Mass will start dropping from a thousand and u
27:10
Cal will start growing. Actually, we could see -- why don't we figure out what it is after one?
27:16
After one time step, what are the populations at time one?
27:22
So what happens in one step? You multiply once by that matrix and, let's see,
27:30
zero times this column -- so it's just a thousand times this column, so I think we're getting 200 and 800.
27:39
So after the first step, 200 people have -- are in California. Now at the following step, I'll multiply again by this matrix
27:49
-- more people will move to California. Some people will move back. Twenty people will come back and, the --
28:00
the net result will be that the California population will be above 200 and the Massachusetts below 800 and they'll still add
28:08
up to a thousand. Okay. I do that a few times.
28:14
I do that 100 times. What's the population? Well, okay, to answer any question like that,
28:20
I need the eigenvalues and eigenvectors, right? As soon as I've -- I've created an example, but as soon
28:26
as I want to solve anything, I have to find eigenvalues and eigenvectors of that matrix.
28:31
Okay. So let's do it. So there's the matrix .9, .2, .1, .8 and tell
28:40
me its eigenvalues. Lambda equals -- so tell me one eigenvalue?
28:46
One, thanks. And tell me the other one.
28:53
What's the other eigenvalue -- from the trace or the determinant -- from the --
28:59
I -- the trace is what -- is, like, easier. So the trace of that matrix is one point seven.
29:06
So the other eigenvalue is point seven. And it -- notice that it's less than one.
29:13
And notice that that determinant is point 72-.02, which is point seven.
29:18
Right. Okay. Now to find the eigenvectors. This is -- so that's lambda one and the eigenvector --
29:26
I'll subtract one from the diagonal, right? So can I do that in light let -- in light here?
29:33
Subtract one from the diagonal, I have minus point one and minus point two, and of course these are still
29:39
there. And I'm looking for its -- here's -- here's -- this is going to be x1.
29:47
It's the null space of A minus I. Okay, everybody sees that it's two and one.
29:56
Okay? And now how about -- so that -- and it -- notice that that eigenvector is positive.
30:03
And actually, we can jump to infinity right now.
30:08
What's the population at infinity?
30:14
It's a multiple -- this is -- this eigenvector is giving the steady state.
30:19
It's some multiple of this, and how is that multiple decided? By adding up to a thousand people.
30:26
So the steady state, the c1x1 -- this is the x1, but that adds up to three, so I really want two --
30:36
it's going to be two thirds of a thousand and one third of a thousand, making a total of the thousand
30:42
people. That'll be the steady state. That's really all I need to know at infinity.
30:48
But if I want to know what's happened after just a finite number like 100 steps, I'd better find this eigenvector.
30:55
So can I -- can I look at -- I'll subtract point seven time -- ti- from the diagonal and I'll get that and I'll look at the null
31:06
space of that one and I -- and this is going to give me x2,
31:11
now, and what is it? So what's in the null space of -- that's certainly singular,
31:16
so I know my calculation is right, and -- one and minus one.
31:24
One and minus one. So I'm prepared now to write down
31:30
the solution after 100 time steps. The -- the populations after 100 time steps,
31:35
right? Can -- can we remember the point one -- the -- the one with this two one eigenvector and the point seven
31:43
with the minus one one eigenvector. So I'll -- let me -- I'll just write it above here.
31:48
u after k steps is some multiple of one to the k times the two one eigenvector
31:57
and some multiple of point seven to the k times the minus one
32:04
one eigenvector. Right?
32:09
That's -- I -- this is how I take -- how powers of a matrix work.
32:15
When I apply those powers to a u0, what I -- so it's u0,
32:21
which was zero a thousand --
32:26
that has to be corrected k=0. So I'm plugging in k=0 and I get c1 times two one and c2 times
32:35
minus one one. Two equations, two constants, certainly
32:44
independent eigenvectors, so there's a solution and you see what it is?
32:51
Let's see, I guess we already figured that c1 was a thousand over three, I think -- did we think that had to be a thousand
32:59
over three? And maybe c2 would be --
33:05
excuse me, let -- get an eraser -- we can -- I just -- I think we've -- get it here.
33:11
c2, we want to get a zero here, so maybe we need plus two thousand over three.
33:21
I think that has to work. Two times a thousand over three minus two thousand over three, that'll give us the zero and a thousand
33:29
over three and the two thousand over three will give us three thousand over three, the thousand. So this is what we approach --
33:37
this part, with the point seven to the k-th power is the part that's disappearing.
33:45
That's -- that's Markov matrices. Okay. That's an example of where they come from,
33:52
from modeling movement of people with no gain or loss,
34:00
with total -- total count conserved. Okay. I -- just if I can add one more comment,
34:07
because you'll see Markov matrices in electrical engineering courses and often you'll see them -- sorry,
34:16
here's my little comment. Sometimes -- in a lot of applications they prefer
34:22
to work with row vectors. So they -- instead of -- this was natural for us, right?
34:29
For all the eigenvectors to be column vectors. So our columns added to one in the Markov matrix.
34:37
Just so you don't think, well, what -- what's going on? If we work with row vectors and we multiply vector times matrix
34:48
-- so we're multiplying from the left -- then it'll be the then we'll be using the transpose of --
34:55
of this matrix and it'll be the rows that add to one. So in other textbooks, you'll see -- instead of col-
35:05
columns adding to one, you'll see rows add to one. Okay. Fine.
35:10
Okay, that's what I wanted to say about Markov, now I want to say something about projections and even
35:17
leading in -- a little into Fourier series. Because -- but before any Fourier stuff,
# Projections
35:24
let me make a comment about projections. This -- so this is a comment about projections onto --
35:33
with an orthonormal basis.
35:42
So, of course, the basis vectors are q1 up to qn.
35:49
Okay. I have a vector b. Let -- let me imagine -- let me imagine this is a basis.
35:58
Let -- let's say I'm in n by n. I'm -- I've got, eh, n orthonormal vectors,
36:06
I'm in n dimensional space so they're a complete -- they're a basis -- any vector v could be expanded in this basis.
36:16
So any vector v is some combination, some amount of q1
36:22
plus some amount of q2 plus some amount of qn.
36:28
So -- so any v.
36:35
I just want you to tell me what those amounts are.
36:41
What are x1 -- what's x1, for example? So I'm looking for the expansion.
36:49
This is -- this is really our projection. I could -- I could really use the word expansion.
36:56
I'm expanding the vector in the basis.
37:01
And the special thing about the basis is that it's orthonormal. So that should give me a special formula for the answer,
37:10
for the coefficients. So how do I get x1? What -- what's a formula for x1?
37:15
I could -- I can go through the projection --
37:23
the Q transpose Q, all that -- normal equations, but --
37:31
and I'll get -- I'll come out with this nice answer that I think I can see right away. How can I pick --
37:38
get a hold of x1 and get these other x-s out of the equation? So how can I get a nice, simple formula for x1?
37:47
And then we want to see, sure, we knew that all the time. Okay. So what's x1?
37:52
The good way is take the inner product of everything with q1.
37:59
Take the inner product of that whole equation, every term, with q1. What will happen to that last term?
38:08
The inner product -- when -- if I take the dot product with q1 I get zero, right?
38:13
Because this basis was orthonormal. If I take the dot product with q2 I get zero.
38:20
If I take the dot product with q1 I get one. So that tells me what x1 is. q1 transpose
38:28
v, that's taking the dot product, is x1 times q1 transpose q1 plus a bunch of zeroes.
38:41
And this is a one, so I can forget that. I get x1 immediately.
38:47
So -- do you see what I'm saying -- is that I have an orthonormal basis,
38:53
then the coefficient that I need for each basis vector is
38:58
a cinch to find. Let me -- let me just -- I have to put this into matrix language, too,
39:05
so you'll see it there also. If I write that first equation in matrix language, what --
39:10
what is it? I'm writing -- in matrix language, this equation says I'm taking these columns -- are --
39:19
are you guys good at this now? I'm taking those columns times the Xs and getting V, right?
39:29
That's the matrix form. Okay, that's the matrix Q. Qx is v.
39:37
What's the solution to that equation? It's -- of course, it's x equal Q inverse v.
39:44
So x is Q inverse v, but what's the point?
39:51
Q inverse in this case is going to -- is simple. I don't have to work to invert this matrix Q,
39:58
because of the fact that the -- these columns are orthonormal, I know the inverse to that.
40:05
And it is Q transpose. When you see a Q, a square matrix with that letter Q,
40:15
the -- that just triggers -- Q inverse is the same as Q transpose. So the first component, then --
40:22
the first component of x is the first row times v, and what's that?
40:29
The first component of this answer is the first row of Q transpose.
40:36
That's just -- that's just q1 transpose times v.
40:42
So that's what we concluded here, too. Okay.
40:49
So -- so nothing Fourier here.
40:55
The -- the key ingredient here was that the q-s are orthonormal.
41:01
And now that's what Fourier series are built on. So now, in the remaining time, let
41:08
me say something about Fourier series. Okay. So Fourier series is --
# Fourier Series
41:20
well, we've got a function f of x.
41:25
And we want to write it as a combination of -- maybe it has a constant term.
41:30
And then it has some cos(x) in it. And it has some sin(x) in it.
41:38
And it has some cos(2x) in it. And a -- and some sin(2x), and forever.
41:45
41:50
So what's -- what's the difference between this type problem and the one above it?
41:56
This one's infinite, but the key property
42:02
of things being orthogonal is still true for sines and cosines, so it's the property that
42:09
makes Fourier series work. So that's called a Fourier series. Better write his name up. Fourier series.
42:15
42:22
So it was Joseph Fourier who realized that, hey, I could work in function space.
42:29
Instead of a vector v, I could have a function f of x. Instead of orthogonal vectors, q1, q2 , q3,
42:38
I could have orthogonal functions, the constant, the cos(x), the sin(x), the s- cos(2x),
42:45
but infinitely many of them. I need infinitely many, because my space is infinite dimensional.
42:52
So this is, like, the moment in which we leave finite dimensional vector spaces and go to infinite dimensional vector
43:00
spaces and our basis -- so the vectors are now functions --
43:07
and of course, there are so many functions that it's -- that we've got an infin- infinite dimensional space --
43:13
and the basis vectors are functions, too. a0, the constant function one -- so my basis is one cos(x),
43:25
sin(x), cos(2x), sin(2x) and so on. And the reason Fourier series is a success
43:32
is that those are orthogonal. Okay. Now what do I mean by orthogonal?
43:40
I know what it means for two vectors to be orthogonal -- y transpose x equals zero, right?
43:46
Dot product equals zero. But what's the dot product of functions?
43:52
I'm claiming that whatever it is, the dot product -- or we would more likely use the word inner product of, say,
43:59
cos(x) with sin(x) is zero. And cos(x) with cos(2x), also zero.
44:06
So I -- let me tell you what I mean by that, by that dot product. Well, how do I compute a dot product?
44:12
So, let's just remember for vectors v trans- v transpose w for vectors, so this was vectors,
44:23
v transpose w was v1w1 +...+vnwn.
44:30
Okay. Now functions.
44:40
Now I have two functions, let's call them f and g.
44:45
What's with them now? The vectors had n components, but the functions have a whole, like, continuum.
44:53
To graph the function, I just don't have n points, I've got this whole graph. So I have functions --
44:59
I'm really trying to ask you what's the inner product of this function f with another function
45:05
g? And I want to make it parallel to this the best I can.
45:11
So the best parallel is to multiply f (x) times g(x)
45:20
at every x -- and here I just had n multiplications,
45:25
but here I'm going to have a whole range of x-s, and here I added the results.
45:31
What do I do here? So what's the analog of addition when you have --
45:38
when you're in a continuum? It's integration. So that the -- the dot product of two functions will be
45:47
the integral of those functions, dx. Now I have to say -- say, well, what are the limits
45:55
of integration? And for this Fourier series, this function f(x) --
46:03
if I'm going to have -- if that right hand side is going to be f(x), that function that I'm seeing on the right,
46:11
all those sines and cosines, they're all periodic, with -- with period two pi.
46:18
So -- so that's what f(x) had better be. So I'll integrate from zero to two pi.
46:24
My -- all -- everything -- is on the interval zero two pi now, because if I'm going to use these sines and cosines,
46:33
then f(x) is equal to f(x+2pi).
46:39
This is periodic --
46:45
periodic functions. Okay. So now I know what --
46:52
I've got all the right words now. I've got a vector space, but the vectors are functions.
47:00
I've got inner products and -- and the inner product gives a number, all right.
47:07
It just happens to be an integral instead of a sum. I've got -- and that -- then I have the idea of orthogonality
47:15
-- because, actually, just -- let's just check. Orthogonality -- if I take the integral -- s- I --
47:21
let me do sin(x) times cos(x) -- sin(x) times cos(x) dx from zero to two pi --
47:31
I think we get zero.
47:36
That's the differential of that, so it would be one half sine x squared, was that right?
47:42
Between zero and two pi --
47:50
and, of course, we get zero. And the same would be true with a little more --
47:58
some trig identities to help us out -- of every other pair. So we have now an orthonormal infinite
48:05
basis for function space, and all we want to do is express a function in that
48:12
basis. And so I -- the end of my lecture is, okay, what is a1?
48:20
What's the coefficient -- how much cos(x) is there in a function compared to the other harmonics?
48:30
How much constant is in that function? That'll -- that would be an easy question.
48:35
The answer a0 will come out to be the average value of f. That's the amount of the constant
48:40
that's in there, its average value. But let's take a1 as more typical. How will I get -- here's the end of the lecture, then --
48:48
how do I get a1? The first Fourier coefficient.
48:54
Okay. I do just as I did in the vector case.
48:59
I take the inner product of everything with cos(x) Take the inner product of everything with cos(x).
49:07
Then on the left -- on the left I have -- the inner product is the integral of f(x)
49:13
times cos(x) cx. And on the right, what do I have?
49:22
When I -- so what I -- when I say take the inner product with cos(x), let me put it in ordinary calculus words.
49:28
Multiply by cos(x) and integrate. That's what inner products are.
49:34
So if I multiply that whole thing by cos(x) and I integrate, I get a whole lot of zeroes.
49:40
The only thing that survives is that term. All the others disappear.
49:47
So -- and that term is a1 times the integral of cos(x) squared
49:53
dx zero to 2pi equals -- so this was the left side and this is
50:01
all that's left on the right-hand side. And this is not zero of course, because it's the length
50:09
of the function squared, it's the inner product with itself, and -- and a simple calculation gives that answer to be pi.
50:18
So that's an easy integral and it turns out to be pi,
50:23
so that a1 is one over pi times there -- times this integral.
50:31
So there is, actually -- that's Euler's famous formula for the -- or maybe Fourier found it --
50:39
for the coefficients in a Fourier series. And you see that it's exactly an expansion
50:47
in an orthonormal basis. Okay, thanks. So I'll do a quiz review on Monday and then the quiz itself
50:56
in Walker on Wednesday. Okay, see you Monday. Thanks.
51:02
