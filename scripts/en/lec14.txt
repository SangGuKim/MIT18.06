https://www.youtube.com/watch?v=YzZUIYRCE38

0:00
0:11
OK. cameras are rolling. This is lecture fourteen, starting a new chapter.
0:21
Chapter about orthogonality. What it means for vectors to be orthogonal.
0:29
What it means for subspaces to be orthogonal. What it means for bases to be orthogonal.
0:37
So ninety degrees, this is a ninety-degree chapter.
0:44
So what does it mean -- let me jump to subspaces.
0:50
Because I've drawn here the big picture. This is the 18.06 picture here.
0:59
And hold it down, guys. So this is the picture and we know a lot about that picture
1:09
already. We know the dimension of every subspace.
1:14
We know that these dimensions are r and n-r.
1:22
We know that these dimensions are r and m-r.
1:29
What I want to show now is what this figure is saying, that the angle --
1:38
the figure is just my attempt to draw what I'm now going to say,
1:43
that the angle between these subspaces is ninety degrees.
1:50
And the angle between these subspaces is ninety degrees. Now I have to say what does that mean?
1:55
What does it mean for subspaces to be orthogonal?
2:01
But I hope you appreciate the beauty of this picture,
2:06
that that those subspaces are going to come out to be orthogonal.
2:14
Those two and also those two. So that's like one point, one important point
2:24
to step forward in understanding those subspaces. We knew what each subspace was like,
2:31
we could compute bases for them. Now we know more. Or we will in a few minutes.
2:38
OK. I have to say first of all what does it mean for two vectors to be orthogonal?
2:44
So let me start with that. Orthogonal vectors.
2:50
2:57
The word orthogonal is -- is just another word for perpendicular.
3:02
It means that in n-dimensional space the angle between those vectors is ninety degrees.
3:10
It means that they form a right triangle. It even means that the going way back to the Greeks that this
3:20
angle that this triangle a vector x, a vector x,
3:29
and a vector x+y -- of course that'll be the hypotenuse,
3:36
so what was it the Greeks figured out and it's neat.
3:42
It's the fact that the -- so these are orthogonal, this is a right angle, if --
3:50
so let me put the great name down, Pythagoras,
3:57
I'm looking for -- what I looking for? I'm looking for the condition if you give me two vectors, how do
4:05
I know if they're orthogonal? How can I tell two perpendicular vectors?
4:11
And actually you probably know the answer. Let me write the answer down. Orthogonal vectors, what's the test for orthogonality?
4:20
I take the dot product which I tend to write as x transpose y,
4:29
because that's a row times a column, and that matrix multiplication just gives me the right thing,
4:38
that x1y1+x2y2 and so on, so these vectors are orthogonal
4:45
if this result x transpose y is zero.
4:52
That's the test. OK. Can I connect that to other things?
5:00
I mean -- it's just beautiful that here we have we're in n
5:06
dimensions, we've got a couple of vectors, we want to know the angle between them,
5:11
and the right thing to look at is the simplest thing that you
5:16
could imagine, the dot product. OK. Now why? So I'm answering the question now why --
5:24
let's add some justification to this fact, that's the test.
5:33
OK, so Pythagoras would say we've got a right triangle, if that length squared plus that length squared equals
5:41
that length squared. OK, can I write it as x squared plus y squared equals
5:51
x plus y squared? Now don't, please don't think that this is always true.
5:59
This is only going to be true in this -- it's going to be equivalent to orthogonality.
6:07
For other triangles of course, it's not true. For other triangles it's not. But for a right triangle somehow that fact
6:16
should connect to that fact. Can we just make that connection?
6:23
What's the connection between this test for orthogonality and this statement of orthogonality?
6:31
Well, I guess I have to say what is the length squared? So let's continue on the board underneath with that equation.
6:42
Give me another way to express the length squared of a vector.
6:47
And let me just give you a vector. The vector one, two, three.
6:53
That's in three dimensions. What is the length squared of the vector x equal one, two,
7:00
three? So how do you find the length squared?
7:06
Well, really you just, you want the length of that vector that goes along one -- up two, and out three --
7:15
and we'll come back to that right triangle stuff. The length squared is exactly x transpose x.
7:25
Whenever I see x transpose x, I know
7:31
I've got a number that's positive. It's a length squared unless it --
7:37
unless x happens to be the zero vector, that's the one case where the length is zero.
7:44
So right -- this is just x1 squared plus x2 squared plus
7:49
so on, plus xn squared. So one -- in the example I gave you what was the length squared
7:55
of that vector one, two, three? So you square those, you get one, four and nine,
8:02
you add, you get fourteen. So the vector one, two, three has length fourteen.
8:08
So let me just put down a vector here. Let x be the vector one, two, three.
8:15
Let me cook up a vector that's orthogonal to it.
8:22
So what's the vector that's orthogonal?
8:27
So right down here, x squared is one plus four plus nine,
8:33
fourteen, let me cook up a vector that's orthogonal to it,
8:38
we'll get right that that -- those two vectors are
8:47
orthogonal, the length of y squared is five,
8:55
and x plus y is one and two making three,
9:02
two and minus one making one, three and zero making three, and the length of this squared is nine plus one plus nine,
9:13
nineteen. And sure enough, I haven't proved anything.
9:20
I've just like checked to see that my test x transpose
9:28
y equals zero, which is true, right? Everybody sees that x transpose y is zero here?
9:35
That's maybe the main point. That you should get really quick at doing x transpose y,
9:41
so it's just this plus this plus this and that's zero. And sure enough, that clicks with fourteen plus five
9:51
agreeing with nineteen. Now let me just do that in letters.
9:56
So that's y transpose y. And this is x plus y transpose x plus y.
10:08
OK. So I'm looking, again, this isn't always true. I repeat.
10:14
This is going to be true when we have a right angle. And let's just -- well, of course,
10:20
I'm just going to simplify this stuff here. There's an x transpose x there.
10:25
And there's a y transpose y there. And there's an x transpose y.
10:34
And there's a y transpose x.
10:41
I knew I could do that simplification because I'm just
10:46
doing matrix multiplication and I've just followed the rules. OK. So x transpose x-s cancel.
10:53
Y transpose y-s cancel. And what about these guys? What can you tell me about the inner product of x with y
11:03
and the inner product of y with x? Is there a difference?
11:09
I think if we -- while we're doing real vectors,
11:15
which is all we're doing now, there isn't a difference, there's no difference. If I take x transpose y, that'll give me zero,
11:23
if I took y transpose x I would have the same x1y1 and x2y2
11:28
and x3y3, it would be the same, so this is --
11:35
this is the same as that, this is really I'll knock that guy out and say two of these.
11:43
So actually that's the -- this equation boiled down to this thing being zero.
11:51
Right? Everything else canceled and this equation boiled down to that one.
11:57
So that's really all I wanted. I just wanted to check that Pythagoras for a right triangle
12:08
led me to this -- of course I cancel the two now.
12:13
No problem. To x transpose y equals zero as the test.
12:18
Fair enough. OK. You knew it was coming. The dot product of orthogonal vectors is zero.
12:27
It's just -- I just want to say that's really neat. That it comes out so well.
12:32
All right. Now what about -- so now I know if two --
12:38
when two vectors are orthogonal. By the way, what about if one of these guys is the zero vector?
12:45
Suppose x is the zero vector, and y is whatever.
12:51
Are they orthogonal? Sure.
12:57
In math the one thing about math is you're supposed to follow the rules. So you're supposed to -- if x is the zero vector,
13:05
you're supposed to take the zero vector dotted with y and of course you always get zero.
13:12
So just so we're all sure, the zero vector
13:17
is orthogonal to everybody. But what I want to --
13:24
what I now want to think about is subspaces.
13:32
What does it mean for me to say that some subspace is
13:37
orthogonal to some other subspace? So OK. Now I've got to write this down.
13:42
So because we're defining definition of subspace
13:47
S is orthogonal so to subspace let's say T,
14:02
so I've got a couple of subspaces. And what should it mean for those guys to be orthogonal?
14:11
It's just sort of what's the natural extension from orthogonal vectors to orthogonal subspaces?
14:19
Well, and in particular, let's think
14:26
of some orthogonal subspaces, like this wall.
14:34
Let's say in three dimensions. So the blackboard extended to infinity, right, is a --
14:41
is a subspace, a plane, a two-dimensional subspace.
14:47
It's a little bumpy but anyway, it's a -- think of it as a subspace, let me take the floor as another
14:57
subspace. Again, it's not a great subspace,
15:07
MIT only built it like so-so, but I'll
15:15
put the origin right here. So the origin of the world is right there.
15:23
OK. Thereby giving linear algebra its proper importance in this.
15:30
OK. So there is one subspace, there's another one. The floor.
15:35
And are they orthogonal?
15:42
What does it mean for two subspaces to be orthogonal and in that special case are they orthogonal?
15:47
All right. Let's finish this sentence. What does it mean means we have to know
15:58
what we're talking about here. So what would be a reasonable idea of orthogonal?
16:05
Well, let me put the right thing up. It means that every vector in S, every vector in S,
16:17
is orthogonal to --
16:23
what I going to say?
16:30
Every vector in T.
16:38
That's a reasonable and it's a good and it's the right definition for two subspaces
16:45
to be orthogonal. But I just want you to see, hey, what does that mean?
16:50
So answer the question about the -- the blackboard and the floor.
16:57
Are those two subspaces, they're two-dimensional, right, and we're in R^3.
17:03
It's like a xz plane or something and a xy plane.
17:09
17:15
Are they orthogonal? Is every vector in the blackboard orthogonal to every vector in the floor, starting from the origin
17:23
right there? Yes or no?
17:28
I could take a vote. Well we get some yeses and some noes.
17:35
No is the answer. They're not. You can tell me a vector in the blackboard
17:41
and a vector in the floor that are not orthogonal.
17:49
Well you can tell me quite a few, I guess.
17:54
Maybe like I could take some forty-five-degree guy in the blackboard, and something in the floor,
18:05
they're not at ninety degrees, right? In fact, even more, you could tell me
18:10
a vector that's in both the blackboard plane and the floor
18:17
plane, so it's certainly not orthogonal to itself. So for sure, those two planes aren't orthogonal.
18:25
What would that be? So what's a vector that's -- in both of those planes?
18:32
It's this guy running along the crack there, in the intersection, the intersection.
18:39
A vector, you know -- if two subspaces meet at some vector, well then for sure
18:45
they're not orthogonal, because that vector is in one
18:51
and it's in the other, and it's not orthogonal to itself unless it's zero. So the only I mean so orthogonal is
19:01
for me to say these two subspaces are orthogonal first of all I'm certainly saying that they don't intersect
19:09
in any nonzero vector.
19:14
But also I mean more than that just not intersecting isn't good enough. So give me an example, oh, let's say in the plane, oh well,
19:26
when do we have orthogonal subspaces in the plane? Yeah, tell me in the plane, so we don't --
19:32
there aren't that many different subspaces in the plane. What what have we got in the plane as possible subspaces?
19:38
The zero vector, real small.
19:44
A line through the origin. Or the whole plane.
19:49
OK. Now so when is a line through the origin orthogonal
19:55
to the whole plane? Never, right, never.
20:02
When is a line through the origin orthogonal to the zero subspace?
20:09
Always. Right. When is a line through the origin orthogonal to a different line through the origin?
20:15
Well, that's the case that we all have a clear picture of, they --
20:21
the two lines have to meet at ninety degrees. They have only the -- so that's like this simple case
20:28
I'm talking about. There's one subspace, there's the other subspace. They only meet at zero.
20:33
And they're orthogonal. OK. Now. So we now know what it means for two subspaces to be orthogonal.
20:43
And now I want to say that this is true for the row space and the null space.
20:49
OK. So that's the neat fact. So row space is orthogonal to the null space.
21:05
Now how did I come up with that?
21:12
But you see the rank it's great, that means that these -- that these subspaces are just the right things,
21:19
they're just cutting the whole space up into two perpendicular subspaces.
21:27
OK. So why?
21:33
Well, what have I got to work with? All I know is the null space.
21:41
The null space has vectors that solve Ax equals zero.
21:46
So this is a guy x. x is in the null space.
21:53
Then Ax is zero. So why is it orthogonal to the rows of A?
22:03
If I write down Ax equals zero, which is all I know about the null space, then I guess I want you to see that that's telling me,
22:13
just that equation right there is telling me that the rows of A, let me write it out.
22:19
There's row one of A. Row two.
22:27
Row m of A. that's A.
22:32
And it's multiplying X. And it's producing zero. OK.
22:41
Written out that way you'll see it.
22:47
So I'm saying that a vector in the row space is perpendicular to this guy X in the null space.
22:54
And you see why? Because this equation is telling you
22:59
that row one of A multiplying that's a dot product, right?
23:06
Row one of A dot product with this x is producing this zero.
23:11
So x is orthogonal to the first row. And to the second row.
23:17
Row two of A, x is giving that zero. Row m of A times x is giving that zero.
23:23
So x is -- the equation is telling me that x is orthogonal to all the rows.
23:31
Right, it's just sitting there. That's all we -- it had to be sitting there because we
23:36
didn't know anything more about the null space than this. And now I guess to be totally complete here
23:46
I'd now check that x is orthogonal to each separate row.
23:51
But what else strictly speaking do I have to do?
23:58
To show that those subspaces are orthogonal, I have to take this x in the null space and show that
24:05
it's orthogonal to every vector in the row space, every vector in the row space, so what --
24:12
what else is in the row space? This row is in the row space, that row is in the row space,
24:18
they're all there, but it's not the whole row space, right, we just have to like remember, what does it
24:24
mean, what does that word space telling us? And what else is in the row space?
24:34
Besides the rows? All their combinations.
24:41
So I really have to check that sure enough if x is perpendicular to row one, row two, all the different separate rows,
24:49
then also x is perpendicular to a combination of the rows. And that's just matrix multiplication again.
24:57
You know, I have row one transpose x is zero,
25:03
so on, row two transpose x is zero,
25:11
so I'm entitled to multiply that by some c1, this by some c2,
25:16
I still have zeroes, I'm entitled to add, so I have c1 row one so -- so all this when I put that
25:24
together that's big parentheses c1 row one plus c2 row two
25:33
and so on. Transpose x is zero. Right?
25:38
I just added the zeroes and got zero, and I just added these following the rule.
25:43
No big deal. The whole point was right sitting in that.
25:51
OK. So if I come back to this figure now I'm like a happier person.
26:02
Because I have this -- I now see how those subspaces are oriented.
26:10
And these subspaces are also oriented. Well, actually why is that orthogonality?
26:20
Well, it's the same statement for A transpose that that one was for A.
26:25
So I won't take time to prove it again because we've checked it for every matrix
26:32
and A transpose is just as good a matrix as A. So we're orthogonal over there.
26:39
So we really have carved up this --
26:46
this was like carving up m-dimensional space into two subspaces and this one was carving up
26:56
n-dimensional space into two subspaces.
27:01
And well, one more thing here. One more important thing.
27:07
Let me move into three dimensions.
27:13
Tell me a couple of orthogonal subspaces in three dimensions
27:22
that somehow don't carve up the whole space, there's stuff left there.
27:30
I'm thinking of a couple of orthogonal lines. If I -- suppose I'm in three dimensions, R^3.
27:38
And I have one line, one one-dimensional subspace, and a perpendicular one.
27:46
Could those be the row space and the null space?
27:51
Could those be the row space and the null space? Could I be in three dimensions and have
28:00
a row space that's a line and a null space that's a line?
28:05
No. Why not? Because the dimensions aren't right.
28:11
Right? The dimensions are no good. The dimensions here, r and n-r, they add up to three,
28:19
they add up to n. If I take -- just follow that example --
28:26
if the row space is one-dimensional, suppose A is what's a good in R^3,
28:36
I want a one-dimensional row space, let me take one, two, five, two, four, ten.
28:43
What's the dimension of that row space?
28:48
One. What's the dimension of the null space?
28:56
Tell what's the null space look like in that case? The row space is a line, right?
29:01
One-dimensional, it's just a line through one, two, five. Geometrically what's the row space look like?
29:08
29:13
What's its dimension? So here r here n is three, the rank
29:20
is one, so the dimension of the null space,
29:26
so I'm looking at this x, x1, x2, x3.
29:32
To give zero. So the dimension of the null space is we all know is two.
29:43
Right. It's a plane. And now actually we know, we see better, what plane is it?
29:49
What plane is it? It's the plane that's perpendicular to one,
29:57
two, five. Right? We now see. In fact the two, four, ten didn't actually
30:04
have any effect at all. I could have just ignored that.
30:09
That didn't change the row space or the null space. I'll just make that one equation.
30:17
Yeah. OK. Sure. That's the easiest to deal with. One equation.
30:22
Three unknowns. And I want to ask --
30:32
what would the equation give me the null space, and you would have said back in September
30:41
you would have said it gives you a plane, and we're completely right.
30:46
And the plane it gives you, the normal vector, you remember in calculus, there was this dumb normal vector
30:53
called N. Well there it is. One, two, five. OK. What is the what's the point I want to make here?
31:08
I want to make -- I want to emphasize that not only are the --
31:14
let me write it in words.
31:19
So I want to write the null space and the row space are
31:33
orthogonal, that's this neat fact, which we've --
31:40
we've just checked from Ax equals zero, but now I want to say more because there's a little more
31:48
that's true. Their dimensions add to the whole space.
31:54
So that's like a little extra information. That it's not like I could have -- I couldn't have a line and a line in three dimensions.
32:03
Those don't add up one and one don't add to three. So I used the word orthogonal complements in R^n.
32:17
And the idea of this word complement is that the orthogonal complement of a row space
32:28
contains not just some vectors that are orthogonal to it, but all.
32:34
So what does that mean? That means that the null space contains all, not just
32:42
some but all, vectors that are perpendicular to the row space.
32:51
OK. Really what I've done in this half of the lecture is just
33:04
notice some of the nice geometry that -- that we didn't pick up before because we didn't discuss
33:12
perpendicular vectors before. But it was all sitting there. And now we picked it up.
33:18
That these vectors are orthogonal complements. And I guess I even call this part
33:23
one of the fundamental theorem of linear algebra. The fundamental theorem of linear algebra
33:32
is about these four subspaces, so part one is about their dimension, maybe I should call it part two now.
33:41
Their dimensions we got. Now we're getting their orthogonality, that's part two.
33:49
And part three will be about bases for them.
33:54
Orthogonal bases. So that's coming up.
34:00
OK. So I'm happy with that geometry right now.
34:10
OK. OK. Now what's my next goal in this chapter?
34:16
Here's the main problem of the chapter. The main problem of the chapter is -- so this is coming.
34:21
It's coming attraction. This is the very last chapter that's about Ax=b.
34:37
34:44
I would like to solve that system of equations when there is no solution.
34:50
You may say what a ridiculous thing to do.
34:56
But I have to say it's done all the time. In fact it has to be done.
35:02
You get -- so the problem is solve --
35:07
the best possible solve I'll put quote Ax=b when there is no
35:20
solution. And of course what does that mean?
35:25
b isn't in the column space. And it's quite typical if this matrix A is rectangular if I --
35:35
maybe I have m equations and that's bigger than the number of unknowns, then for sure the rank is not m,
35:47
the rank couldn't be m now, so there'll be a lot of right-hand sides with no solution, but here's an example.
36:00
Some satellite is buzzing along. You measure its position.
36:06
You make a thousand measurements. So that gives you a thousand equations for the --
36:13
for the parameters that -- that give the position. But there aren't a thousand parameters,
36:20
there's just maybe six or something. Or you're measuring the -- you're doing questionnaires.
36:26
You're measuring resistances.
36:36
You're taking pulses. You're measuring somebody's pulse rate. There's just one unknown.
36:42
OK. The pulse rate. So you measure it once, OK, fine, but if you really want to know it,
36:49
you measure it multiple times, but then the measurements have
36:54
noise in them, so there's -- the problem is that in many many problems
36:59
we've got too many equations and they've got noise in the right-hand side.
37:05
So Ax=b I can't expect to solve it exactly right,
37:11
because I don't even know what -- there's a measurement mistake in b.
37:18
But there's information too. There's a lot of information about x in there.
37:25
And what I want to do is like separate the noise, the junk, from the information.
37:33
And so this is a straightforward linear algebra problem. How do I solve, what's the best solution?
37:41
OK. Now. I want to say so that's like describes the problem
37:52
in an algebraic way. I got some equations, I'm looking for the best solution.
37:58
Well, one way to find it is -- one way to start, one way to find a solution is throw away equations until
38:09
you've got a nice, square invertible system and solve that.
38:14
That's not satisfactory. There's no reason in these measurements
38:21
to say these measurements are perfect and these measurements are useless. We want to use all the measurements
38:27
to get the best information, to get the maximum information. But how?
38:33
OK. Let me anticipate a matrix that's going to show up.
38:40
This A is typically rectangular. But a matrix that shows up whenever you have --
38:47
and we chapter three was all about rectangular matrices. And we know when this is solvable,
38:54
you could do elimination on it, right? But I'm thinking hey, you do elimination
39:00
and you get equation zero equal other non-zeroes. I'm thinking we really -- elimination is going to fail. So that's our question.
39:05
Elimination will get us down to --
39:16
will tell us if there is a solution or not. But I'm now thinking not.
39:22
So what are we going to do? OK. All right. I want to tell you to jump ahead to the matrix that
39:30
will play a key role. So this is the matrix that you want to understand for this chapter four.
39:38
And it's the matrix A transpose A.
39:47
What's -- tell me some things about that matrix.
39:53
So A is this m by n matrix, rectangular, but now
39:58
I'm saying that the good matrix that shows up in the end is A transpose A.
40:05
So tell me something about that. Is it -- what's the first thing you know about A transpose A.
40:14
It's square. Right? Square because this is m by n and this is n by m.
40:21
So this is the result is n by n. Good. Square. What else?
40:27
It's symmetric. Good. It's symmetric.
40:35
Because you remember how to do that. If we transpose that matrix let's transpose it,
40:44
A transpose A, if I transpose it, then that comes first transposed, this comes second,
40:55
transposed, and then transposing twice is leaves it --
41:01
brings it back to the same so it's symmetric. Good. Now we now know how to ask more about a matrix.
41:11
I'm interested in is it invertible?
41:20
If not, what's its null space? So I want to know about -- because you're going to see,
41:26
well, let me -- let me even, well I shouldn't do this,
41:32
but I will. Let me tell you what equation to solve
41:37
when you can't solve that one. The good equation comes from multiplying both sides
41:47
by A transpose, so the good equation that you get to
41:53
is this one. A transpose Ax equals A transpose b.
42:00
That will be the central equation in the chapter.
42:08
So I think why not tell it to you. Why not admit it right away. OK.
42:14
I have to -- I should really give x. I want to sort of indicate that this x isn't I mean this x was
42:27
the solution to that equation if it existed, but probably didn't.
42:33
Now let me give this a different name, x hat.
42:39
Because I'm hoping this one will have a solution.
42:45
And I'm saying that it's my best solution. I'll have to say what does best mean.
42:52
But that's going to be my -- my plan. I'm going to say that the best solution solves this equation.
42:59
So you see right away why I'm so interested in this matrix A transpose A.
43:05
And in its invertibility. OK. Now, when is it invertible?
43:11
OK. Let me take a case, let me just do an example and then --
43:22
I'll just pick a matrix here. Just so we see what A transpose A looks like.
43:28
So let me take a matrix A one, one, one, one, two, five.
43:35
Just to invent a matrix. So there's a matrix A.
43:40
Notice that it has M equal three rows and N equal two columns.
43:48
Its rank is -- the rank of that matrix is two.
43:55
Right, yeah, the columns are independent. Does Ax equal b?
44:01
If I look at Ax=b, so x is just x1 x2, and b is b1 b2 b3.
44:09
Do I expect to solve Ax=b?
44:15
What's -- no way, right? I mean linear algebra's great, but solving
44:21
three equations with only two unknowns usually we can't do it. We can only solve it if this vector is b is what?
44:30
I can solve that equation if that vector b1 b2 b3
44:37
is in the column space. If it's a combination of those columns then fine.
44:44
But usually it won't be. The combinations just fill up a plane and most vectors aren't on that plane.
44:52
So what I'm saying is that I'm going to work with the matrix A transpose A.
44:59
And I just want to figure out in this example what A transpose A is.
45:07
So it's two by two. The first entry is a three, the next entry is an eight,
45:13
this entry is -- what's that entry?
45:20
Eight, for sure. We knew it had to be, and this entry is, what's that now, getting out my trusty calculator, thirty,
45:32
is that right? And is that matrix invertible? Thirty.
45:38
There's an A transpose A. And it is invertible, right? Three, eight is not a multiple of eight, thirty --
45:46
and it's invertible. And that's the normal, that's what I expect.
45:51
So this is I want to show.
45:56
So here's the final -- here's the key point. The null space of A transpose A --
46:04
it's not going to be always invertible. Tell me a matrix --
46:11
I have to say that I can't say A transpose A is always invertible.
46:16
Because that's asking too much. I mean what could the matrix A be, for example,
46:22
so that A transpose A was not invertible? Well, it even could be the zero matrix.
46:29
I mean that's like extreme case. Suppose I make this rank --
46:38
suppose I change to that A.
46:46
Now I figure out A transpose A again and I get -- what do I get?
46:53
I get nine, I get nine of course and here I get what's that entry?
47:02
Twenty-seven. And is that matrix invertible?
47:09
No. And why do I -- I knew it wouldn't be invertible anyway.
47:16
Because this matrix only has rank one.
47:23
And if I have a product of matrices of rank one, the product is not going to have a rank bigger than one.
47:30
So I'm not surprised that the answer only has rank one. And that's what I --
47:36
always happens, that the rank of A transpose A comes out to equal the rank of A.
47:44
So, yes, so the null space of A transpose A
47:49
equals the null space of A, the rank of A transpose A
47:57
equals the rank of A. So let's -- as soon as I can why that's true.
48:11
But let's draw from that what the fact that I want.
48:18
This tells me that this square symmetric matrix is invertible
48:24
if -- so here's my conclusion.
48:30
A transpose A is invertible if exactly when --
48:40
exactly if this null space is only got the zero vector.
48:46
Which means the columns of A are independent.
48:51
So A transpose A is invertible exactly if A has independent columns. That's the fact that I need about A transpose A.
49:12
And then you'll see next time how A transpose A enters everything.
49:18
Next lecture is actually a crucial one. Here I'm preparing for it by getting us
49:26
thinking about A transpose A. And its rank is the same as the rank of A,
49:31
and we can decide when it's invertible. OK. So I'll see you Friday. Thanks.
49:37
