https://www.youtube.com/watch?v=0h43aV4aH7I

0:00
0:07
OK. So, coming nearer the end of the course, this lecture will be a mixture of the linear algebra that
0:20
comes with a change of basis. And a change of basis from one basis to another basis
0:26
is something you really do in applications. And, I would like to talk about those applications.
0:33
I got a little bit involved with compression. Compressing a signal, compressing an image.
0:39
And that's exactly change-of-basis. And then, the main theme in this chapter
0:48
is th- the connection between a linear transformation, which
0:54
doesn't have to have coordinates, and the matrix that tells us that transformation
1:01
with respect to coordinates. So the matrix is the coordinate-based description
1:08
of the linear transformation. Let me start out with the nice part, which
1:14
is just to tell you something about image compression. Those of you -- well, everybody's going to meet
1:21
compression, because you know that the amount of data that we're getting --
1:27
well, these lectures are compressed. So that, actually, probably you see my motion as
1:37
jerky? Shall I use that word? Have you looked on the web? I should like to find a better word.
1:46
Compressed, let's say. So the complete signal is, of course, in those video cameras,
1:53
and in the videotape, but that goes to the bottom of building nine, and out of that
1:59
comes a jumpy motion because it uses a standard system
2:06
for compressing images. And, you'll notice that the stuff that sits on the board
2:13
comes very clearly, but it's my motion that needs
2:20
a whole lot of bits, right? So, and if I were to run up and back up there and back,
2:26
that would need too many bits, and I'd be compressed even more.
2:32
So, what does compression mean? Let me just think of a still image.
2:39
And of course, satellites, and computations
2:44
of the climate, computations of combustion, the computers and sensors of all kinds
2:52
are just giving us overwhelming amounts of data. The Web is, too.
2:58
Now, some compression can be done with no loss. Lossless compression is possible just using, sort of, the fact
3:07
that there are redundancies. But I'm talking here about lossy compression.
3:13
So I'm talking about -- here's an image.
3:18
And what does an image consist of? It consists of a lot of little pixels, right?
3:24
Maybe five hundred and twelve by five hundred and twelve. Two to the ninth by two to the ninth pixels,
3:31
and so this is pixel number one, one, so that's a pixel.
3:40
And if we're in black and white, the typical pixel
3:45
would tell us a gray-scale, from zero to two fifty five.
3:52
So a pixel is usually a value of one of the xi,
3:59
so this would be the i-th pixel, is -- it's usually a real number on a scale
4:05
from zero to two fifty five. In other words, two to the eighth possibilities. So usually, that's the standard, so that's eight -- eight bits.
4:19
But then we have that for every pixel,
4:26
so we have five hundred and twelve squared pixels, we're really operating x is a vector in R^n, but what is n?
4:36
n is five hundred and twelve squared. That's our problem, right there.
4:45
A pixel is a vector that gives us the information about the image.
4:53
I'm sorry. The image that comes through is a vector of that length that --
5:02
that's the information that we have about the image, if it's a color image, we would have three times that length,
5:08
because we'd need three coordinates to get color. So it would be three times five hundred and twelve squared.
5:16
It's an enormous amount of information, and we couldn't send out the image for these lectures
5:22
without compressing it. It would overload the system.
5:29
So it has to be compressed. The standard compression, and still used with lectures
5:37
is, called JPEG.
5:42
I think that stands for Joint Photographic Experts Group. They established a system of compression.
5:50
And I just want to tell you what it's about. It's a change-of-basis. What basis do we have?
5:58
The current basis we have is, you could say, the standard basis is, every pixel, give a value.
6:07
So that's like we have a vector x which is five hundred
6:12
and twelve squared long and, in the i-th position, we get a number like one twenty one or something.
6:21
The pixel next to it might be one twenty four, maybe
6:27
where my tie begins to enter, so if it was mostly blue shirt,
6:34
this would be a slight difference in shading, but pretty close, then the tie would be a different color,
6:41
so we might have quite a few pixels for the blue shirt, and a whole lot
6:47
more for the blackboard, that are very close. And that's what are very correlated.
6:55
And that's what gives us the possibility of compression. For example, before the lecture starts,
7:02
if we had a blank blackboard, then there's an image, but it would make no sense to take that image
7:12
and tell you what it is pixel by pixel. I mean, there's a case in which all pixel values,
7:21
all gray levels are the same -- or practically the same, depending on the erasing
7:26
of the board, but extremely close -- and, so that's an image where the standard basis is lousy.
7:35
That's the basic fact, that the standard basis which gives
7:40
the value of every pixel makes no use of the fact that we're getting a whole lot of pixels whose gray levels --
7:49
the neighboring pixels tend to have the same gray level
7:54
as their neighbors. So how do we take advantage of that fact?
7:59
Well, one basis vector that would be extremely nice to include in the basis
8:07
would be a vector of all ones.
8:12
That's not in our standard basis, so let me just write again, the standard basis is our one,
8:23
and all the rest zeroes, zero, one, and all the rest, zeroes,
8:29
everybody knows what these standard basis is. Now, any other basis for R -- so this is --
8:39
for this very high-dimensional space -- now I'm going to speak about a better basis.
8:46
Better basis -- and let me just emphasize,
8:51
one vector that would be extremely nice to have in that basis is the vector of all ones.
8:59
Why is that? Let me just say again, because that vector of all ones, by itself, one vector is able to completely give
9:11
the information on a solid image. Of course, our image won't be solid,
9:17
it will have a mix of solid and signal.
9:26
So having that one vector in the basis is going to save us a whole lot.
9:33
Now, the question is, what other vectors should be in the basis? The extreme vector in the basis might
9:41
be a vector of one minus one, one minus one, one minus one.
9:47
That would be a vector that shows -- I mean, that's like a checkerboard vector, right?
9:52
That's a vector that would, if the image was
9:58
like a huge checkerboard of plus, minus, plus, minus, plus, minus, that vector
10:03
would carry the whole signal. But much more common would be maybe
10:08
to have half the image, darker and the other half lighter.
10:14
So another vector that might be quite useful in here would be half ones and half minus ones.
10:21
I'm just trying to get across the idea of that a basis could
10:31
be where, that first of all, we've got the bases at our disposal.
10:38
Like, we're free to choose that. And it's a billion-dollar decision what we choose.
10:45
So, and TV people would rather pre- would prefer one basis based on the way the signal is scanned,
10:55
and movie people would prefer another, I mean, there's giant politics in this question that
11:02
really reduces to a linear algebra problem, what basis to choose.
11:07
I'll just mention the best known basis, which JPEG uses, --
11:15
let me put that here -- is the Fourier basis.
11:21
So when you use the Fourier basis, that includes --
11:27
this is the constant vector, the D C vector if we're electrical engineers, the l- vector of all ones, so it would include one,
11:37
one, one, one. Often eight by eight is a good choice.
11:44
Eight by eight is a good choice. So, what do I mean by this eight by eight?
11:50
I mean that the big signal, which is five twelve by five
11:55
twelve, gets broken down, and JPEG does this, into eight
12:00
by eight blocks. And we -- sort of, this is too much to deal with at once.
12:06
So what JPEG does is take this eight by eight block, which is sixty four coefficients, sixty four,
12:16
pixels, and changes the basis on that piece.
12:22
And then, now, let's see, I was going to write down Fourier, so you remember Fourier as this vector of all ones, and then,
12:31
the vector -- oh, well, actually, I gave a lecture earlier about the Fourier matrix,
12:39
this matrix whose columns are powers of a complex number w.
12:50
I won't repeat that, because I don't want to go into the details of the Fourier basis,
12:56
just to tell you how compression works. So what happens in JPEG?
13:02
What happens to the video, to each image, of these lectures?
13:11
It gets broken into eight by eight blocks. OK.
13:16
Within each block, we have sixty four coefficients, sixty four basis vectors, sixty four pixels,
13:24
and we change basis in sixty four dimensional space using these Fourier vectors.
13:31
Just note, that was a lossless step.
13:38
Let me emphasize. In comes the signal x.
13:46
We change basis. This is the basis change. Change basis. Choose a better basis.
13:54
So it produces, the coefficients c.
14:03
So sixty four pixels come in, sixty four coefficients come out.
14:08
Now comes the compression. Now come -- this was lossless.
14:16
It's just -- we know that R -- R sixty four has plenty of bases, and we've chosen one.
14:25
Now, in that basis, we write the signal in that basis,
14:32
and that's what my lecture -- that's the math part of my lecture. Now here's the application part.
14:37
The next part is going to be the compression step.
14:43
And that's lossy. We're going to lose information. And what will actually happen at that step?
14:52
Well, one thing we could do is just throw away the small coefficients.
14:58
So that's called thresholding, we set some threshold. Every coefficient, every basis vector that's
15:07
not in there more than the threshold value, and we set them threshold so that our eye can't
15:14
see the difference, or can hardly see the difference,
15:19
whether we throw away that little bit of that basis vector or keep it. So this compression step produces a compressed set of
15:28
I'll just keep going here. coefficients. So it keeps going, this compression step
15:36
produces some coefficient c hat. And with many zeroes.
15:42
So that's where the compression came.
15:48
Probably, there is enough of this vector of all ones -- we very seldom throw that away.
15:55
Usually, its coefficient will be large. But the coefficient of something like this,
16:01
that quickly alternative vector, there's probably very little of that in any smooth signal.
16:07
That's high-frequency -- this is low-frequency, zero frequency. This stuff is the highest frequency we could have,
16:15
and if the noise, the jitter is producing that sort of output,
16:23
but a smooth lecture like this one is, has very little of that highest frequency,
16:30
very little noise in this lecture. OK, so we throw away whatever there is,
16:37
and we're left with just a few coefficients, and then we reconstruct a signal using those coefficients.
16:48
We take those coefficients, times their basis vectors,
16:53
but this sum doesn't have sixty four terms any more.
16:59
Probably, it has about two or three terms. So that would -- say it has three terms.
17:05
From sixty four down to three, that's compression of twenty one to one. That's the kind of compression you're looking for.
17:12
And everybody is looking for that sort of compression. Let's see, I guess I met the problem
17:19
with the FBI and fingerprints. So there's a whole lot of still images.
17:26
You know, with your thumb, you make these inky marks which go somewhere.
17:34
it used to go to Washington and get stored in a big file. So Washington had a file of thirty million murderers,
17:44
cheaters on quizzes, other stuff,
17:50
and actually, there was no way to retrieve them in time.
17:56
So suppose you're at the police station, they say, OK, this person may have done this, check with Washington,
18:03
have they got -- are his or her fingerprints on file? Well, Washington won't know the answer within a week
18:12
if it's got filing cabinets full of fingerprints. So of course, the natural step is digitizing.
18:21
So all fingerprints are now digitized, so now it's at least electronic, but still there's too much
18:30
information in each one. I mean, you can't search through that many, fingerprints
18:39
if the digital image is five twelve squared by five twelve
18:45
squared, if it's that many pixels. So you get compressed. So the FBI had to decide what basis to choose for compression
18:54
of fingerprints. And then they built a big new facility in West Virginia, and that's where fingerprints now are sent.
19:03
So I think, if you get your fingerprints done now at the police station, if it's an up-to-date police station,
19:09
it happens digitally, and the signal is sent digitally, and then in West Virginia, it's compressed and indexed.
19:18
And then, if they want to find you, they can do it within minutes instead of within a week.
19:26
OK. So this compression comes up for signals, for images,
19:31
for video -- which is, like these lectures -- there's another aspect.
19:38
You could treat the video as one still image after another one,
19:43
and compress each one, and then run them and make a video.
19:50
But that misses -- well, you can see why that's not optimal.
19:55
In a video thing, you have a sequence of images,
20:04
so video is really a sequence of images but what about one
20:14
image to the next image? They're extremely correlated. I mean that I'm getting an image every split-second,
20:21
and also, I'm moving slightly. That's what's producing the, jumpy motion on the video.
20:29
But I'm not, like, you know --
20:35
each image in the sequence is pretty close to the one before. So you have to use, like, prediction and correction.
20:44
I mean, the image of me one instant -- one time-step later,
20:49
you would assume would be the same, and then plus a small correction. And you would only code and digitize the correction,
20:58
and compress the correction. So a sequence of images that's highly correlated
21:05
and the problem in compression is always
21:11
to use this correlation, this fact that, in time, or in space, things
21:17
don't change instantly, they're very often smooth changes,
21:23
and, you can predict one value from the previous value.
21:29
OK. So those are applications which are pure linear algebra.
21:36
I could, well, maybe you'll allow me to tell you,
21:42
and the book describes, the new basis that's the competition for Fourier.
21:50
So the competition for Fourier is called wavelets, and I can describe what that basis is like,
21:58
say, in the eight by eight case. So the eight by eight wavelet basis is the vector of all ones, eight ones, then
22:08
the vector of four ones and four minus ones, then the vector of two ones, and two minus ones,
22:18
and four zeroes. And also the vector of four zeroes
22:24
and two ones and two minus ones. So now I'm up to four, and I need four more, right?
22:31
For R^8? The next basis vector will be one minus one and six zeroes,
22:39
and then three more like that, with the one minus one
22:44
there, and there, and there. So those are eight vectors in eight-dimensional space,
22:52
those are called wavelets, and it's a very simple wavelet
22:59
choice, it's a more sophisticated choice. This is a little jumpy, to jump between one and minus one.
23:09
And, actually, you can see, now, suppose you compare the wavelet basis with the Fourier basis above.
23:16
How could I write this guy, which is in the Fourier basis,
23:22
it's an eight -- it's a vector in R^8. How would I write that as a combination
23:28
of the wavelet basis? Have I told you enough about the wavelet basis that you can see,
23:34
how does this very fast guy -- what combination of the wavelet basis is that very fast guy?
23:41
It would be this one -- it would be the sum of these four, right?
23:47
That very fast guy will be that one minus one, and the next one, and the next one, and the next one.
23:53
So this is the sum of those last four wavelets.
23:59
This one, we've kept, and so on. So, each -- well, every -- well, that's what a basis does.
24:06
Every vector in R^8 is some combination of those, and for the linear algebra -- so the linear algebra is this
24:15
step, find the coefficient. That's the step we want to take.
24:21
What if I give you the basis, like this wavelet basis,
24:28
and I give you the pixel -- so here are the pixel values, P1,
24:34
P2, down to P8 -- what's the job?
24:39
What's the linear algebra here? So these are the values, this is in the standard basis, right?
24:48
Those are just the values at eight successive points.
24:53
I guess I'm dropping down to one dimension, instead of eight by eight, I'm just going to take eight pixel values
25:01
along that first top row. So what do I want to do? In standard basis, here are the pixel values.
25:10
I want to write that as a combination of c1 times this
25:15
guy, plus c2 times this guy, plus c3,
25:21
these are the coefficients, plus c4 times this one -- do you see what I'm doing?
25:26
I want to write this vector P as a combination of c1 times
25:32
the first wavelet plus c8 times the eighth wavelet.
25:39
That's the transform step. That's the lossless step. That's the step from P -- oh, I'm calling it P here,
25:47
and I called it x there, so let me -- at the risk of moving, and therefore making this jumpy --
25:54
suppose the signal I'm now calling P, that a pixel values,
26:00
and I'm looking for the coefficients. OK, tell me how to do it.
26:08
If I give you eight basis vectors, and I give you the input signal, and I ask for the coefficients,
26:17
what do I do? What's the step? I'm trying to solve this, I want to know the eight coefficients,
26:28
so I'm changing from the standard basis, which is just the eight gray-scale values to the wavelet basis, where
26:37
the same vector is represented by eight numbers. It's got to take eight numbers to tell you a vector in R^8,
26:44
and those eight numbers are the coefficients of the basis. Look, we've done this thing before.
26:54
There is the equation in vector notation, we want to see it as a matrix.
27:01
This is a combination of columns of the wavelet matrix,
27:06
right? This is P equals c1, c2, down to c8,
27:14
and these guys are the columns. I mean, this is the step that we're constantly
27:19
taking in this course, the first basis vector goes in the first column, the second basis vector
27:25
goes in the second column, and so on, the eight columns of this wavelet matrix
27:32
are the eight basis vectors. This is a wavelet matrix W.
27:40
So, the step to change basis -- so now I'm finally coming
27:46
to this change-of-basis, so the change of basis that,
27:51
let me stay with this board, but -- well, let me just go above it, here.
27:56
So the standard basis, we know, the wavelet basis we have here,
28:07
and the transform is simply, solve the equations,
28:14
P=W C. So the coefficients are W inverse P. Right.
28:23
This shows a critical point. A good basis has a nice, fast, inverse.
28:31
So good basis means what?
28:38
So this is like the billion-dollar competition, Eh? and it's not over yet. People are going to come up with better bases than these.
28:46
So a good basis will be, first good thing would be fast.
28:53
I have to be able to multiply by W fast, and multiply by W --
28:58
by its inverse fast. That's -- if a basis doesn't allow you to do that fast,
29:04
then it's going to take so much time that you can't afford it.
29:09
So these bases -- the Fourier basis, everybody said, OK, I
29:15
know how to deal quickly with the Fourier basis, because we have something called the Fast Fourier Transform.
29:23
So there's a FFT that came in my earlier lecture, and comes in the last chapter of the book,
29:30
so change-of-basis is done -- if, for the Fourier basis, it's done fast by the FFT and there's a fast wavelet
29:39
transform. I can change, for this wavelet example,
29:47
this matrix is easy to invert. It's just somebody had a smart idea
29:52
in choosing that wavelet basis and inverting it, it has a nice inverse.
29:58
Actually, you can see why it has a nice inverse. Do you see any property of these eight basis vectors?
30:06
Well, I've only written five of them, but if you see that property for those five, you'll see it for the three
30:13
remaining. Well, if I give you those eight vectors and ask, what's a nice property?
30:20
Well, you would say, first, they're all ones and minus ones and zeroes. So every multiplication is very fast using -- just in binary.
30:32
But what's the other great property of those vectors? Anybody see it?
30:37
So, of course, when I think about a basis, one nice property -- I don't have to have it, but I'm happy if it's there --
30:45
is that they're orthogonal. If the basis vectors are orthogonal,
30:51
then I'm in good shape. And these are... do you see? Take the dot product of that with that,
30:56
you get four plus ones and four minus ones, you get zero. Take the dot product of that with that.
31:03
You get two plus ones and two minus ones. Or the dot product of that with that. Two plus ones and two minus ones.
31:10
You can easily check that that's an orthogonal basis. It's not orthonormal.
31:15
To fix it up, I should divide by the length,
31:22
to make them unit vectors. Let's suppose I do that. So somewhere in here, I've got to account for the fact
31:29
that this has length square root of eight, that has length square root of four, that
31:34
has length square root of two. But that's just a constant factor that's easy to --
31:40
so suppose we've done that. Then, tell me what's W inverse?
31:47
That's what chapter four, section four point four was about.
31:53
If we have orthonormal columns then the inverse is the same as the transpose.
32:02
So if we have a fast way to multiply by W, which we do, the inverse is going to look just the same,
32:09
and we'll have a fast way to do W inverse. So that's the wavelet basis passes this requirement
32:17
for fast. We can use it fast. But there's a second requirement, is it any good?
32:24
Because the the very fastest thing we could do is not to change basis at all.
32:30
Right? The fastest thing would be, OK, stay with the standard basis, stay with eight pixel values.
32:36
But that was poor from compression point of view, right? Those eight pixel values, if I just took those eight numbers,
32:45
I can't throw some of those away. If I throw away ninety percent --
32:52
if I compress ten to one, and throw away ninety percent of my pixel values,
32:57
well, my picture's just gone dark. Whereas, the basis that was good,
33:03
the wavelet basis or the Fourier basis, if I throw away c5, c6, c7, and c8, all I'm throwing away
33:12
is little blips that are probably there in very small amounts.
33:18
So the second property that we need is good compression. So first, it has to be fast, and secondly, a few basis vectors
33:32
should come close to the signal. So a few is enough.
33:40
Can I write it that way? A few basis vectors are enough to reproduce the image just
33:50
exactly as on a video of these 18.06 lectures. Uh, I don't know what the compression rate is, I'll ask,
33:58
David, who does the compression -- and, by the way, I'll try to get the lectures, that are relevant for the quiz
34:09
up onto the Web in time. So I'll send them a message today.
34:15
So, he's using the Fourier basis because the JPEG --
34:21
so JPEG two thousand, which will be the next standard for image compression, will include wavelets.
34:29
So, I mean, you're actually getting a kind of up-to-date, picture of where this big world of signal and image processing
34:40
is. That Fourier is what everybody knew,
34:45
and what people automatically used, and the new one is wavelets, where this is
34:53
the simplest set of wavelets. And this isn't the one that the FBI uses, by the way,
34:59
the FBI uses a smoother wavelet, instead of jumping from one to minus one, it's a smooth, Cutoff.
35:08
and, that's what we'll be in in JPEG two thousand.
35:15
OK, so that's that application. Now, let me come to the math, the linear algebra
35:23
part of the lecture. Well, we've actually seen a change-of-basis.
35:29
So let -- let me just review that eh-eh change-of-basis idea, and then the i- and then the transformation to a matrix.
35:37
OK. So this, I hope you see that these applications are
35:43
really big. Now, I have to talk a little about change-of-basis,
35:49
and a little about that. The matrix. OK. OK.
35:55
OK. So change-of-basis.
36:01
Basically, forgive that put, OK, I have,
36:10
I have my vector in one basis, and I want to change to a different one.
36:16
Actually, you saw it for the wavelet case. So I need the --
36:22
let the matrix W, and the columns of W
36:31
be the new basis vectors.
36:40
Then the change-of-basis involves, just as it did there, W inverse.
36:46
So we have the vector, say, x, in the old basis,
36:52
and that converts to a vector, let's say, c, in the new basis,
37:04
and the relation is exactly what we had there, that x is W c.
37:16
That's the step we have to take.
37:22
There's a matrix W that gives us a change-of-basis. OK.
37:28
What I want to do is think about transformations on matrices.
37:38
So here's the question to complete this lecture.
37:43
Suppose I have a linear transformation T.
37:50
So we would think of it as an eight -- as a n by n matrix.
37:57
And it's computed with respect to a certain basis.
38:02
So T -- no, I'm sorry. I've got the transformation T, period.
38:07
That's taking eight-dimensional space to eight-dimensional space. Now, let's get matrices in there.
38:15
OK. So, with respect to a first basis, say v1 up to v8,
38:27
it has a matrix A.
38:35
I'm just setting up letters here. With respect to a second basis, say, I'll make it u1 up to --
38:48
or w1, since I've used (w)s, w1 up to w8, it has a matrix B.
39:00
And my question is, what's the connection between A
39:05
How is the matrix -- the transformation T is settled. and B?
39:11
We could say, it's a rotation, for example.
39:16
So that would be one transformation of eight-dimensional space, just spin it a little.
39:24
Or project it. Or whatever linear transformation we've got.
39:32
Now, we have to remember -- my first step is to remind you how you create that matrix A.
39:41
Then my second step is, we would use the same method to create B, but because it came from the same transformation,
39:49
there's got to be a relation between A and B. What's the relation between A and B?
39:55
And let me jump to the answer on that one.
40:02
That if I have the same transformation, and I'm compute on its matrix in one basis, and then I computer
40:10
it in another basis, those two matrices are similar.
40:16
So these two matrices are similar. Now, do you remember what similar matrices meant?
40:23
Similar. A is similar to -- the two matrices are similar. Similar.
40:28
And what do I mean by that?
40:33
I mean that I take the matrix B, and I can compute it
40:40
from the matrix A using some similarity, some matrix
40:47
M on one side, and M inverse on the other. And this M will be the change-of-basis matrix.
40:55
This part of the lecture is, admittedly, compressed.
41:03
What I wanted you to -- it's really the conclusion that I want you to spot.
41:12
Now, I have to go back and say, what does it mean for A
41:17
to be the matrix of this transformation T. So I have to remind you what that meant,
41:23
that was in the last lecture. Then this is the conclusion that if I change to a different
41:32
basis, we now know -- see, if I change to a different basis, two things happen.
41:38
Every vector has new coordinates. There, the rule is this one, between the old coordinates
41:45
and the new ones. Every matrix changes, every transformation has a new
41:50
matrix. And the new matrix is related this way, the M could be the same as the W.
41:57
The M there would be the W here. OK. So, can I, in the remaining minutes,
42:04
recapture my lecture -- the end of my lecture that was just before Thanksgiving, about the matrix?
42:13
OK. What's the matrix?
42:19
And I'll just take one basis. So now this part is going to go onto this board here.
42:26
What is the matrix? What is A?
42:32
OK. Using a basis v1 up to v8.
42:42
Mm. OK. What's the point? The point is, if I know what the transformation does
42:51
to those eight basis vectors, I know it completely.
42:56
I know T, I know everything about T, I know T completely from knowing T of V -- what T does to v1,
43:11
what T does to v2, what T does to v8.
43:18
Why is that? It's because T is a linear transformation.
43:23
So that if I know what these outputs are -- so these are the inputs v1 up to v8,
43:29
these are the outputs from the transformation, like everyone rotated, everyone projected,
43:35
whatever transformation I've done, then why is it that I know everything?
43:42
How does linearity work? Why? This is because every x is some combination of these basis
43:56
vectors, right? c1v1, c2v2, c8v8, they were a basis.
44:05
That's the whole point of a basis, that every vector is a combination of the basis vectors in exactly one way.
44:12
And then, what is T of x?
44:18
The point is, I claim that we know T of x completely for every x, because every x is a combination of those --
44:27
and now we use the linear transformation part to say that the output from x has to be c1 times the output from v1 plus
44:37
v2 times the output from v2, and so on.
44:43
Up through c8 times the output from v8. So this is like just saying, OK.
44:49
We know everything when we know what
44:54
T does to each basis vector. OK. So those are the eight things we need.
45:02
Now -- but we need these answers in this basis.
45:08
So this first output is some combination
45:13
of the eight basis vectors. So write T acting on the first input -- in other words,
45:24
write the first output as a combination of the basis vectors, say a11 v1 + a21 v2 and so on a81 v8.
45:38
Write T of v2 as some combination a12 of v1,
45:50
a22 of v2 and so on. I'm creating the matrix A, column by column.
45:57
Those numbers go in the first column, these numbers go in the second column, the matrix A that thi-
46:06
this -- this is our matrix that represents T in this basis is
46:11
these numbers. a11 down to a18, a21 down to a28, and so on.
46:22
OK. That's the recipe. In other words, if I give you a transformation, and a basis.
46:31
So that's what I have to give you. The inputs are the basis and to tell you
46:37
what the transformation is. And then, you tell me --
46:42
you compute T for each basis, expand
46:47
that result in the basis, and that gives you the sixty four numbers that go into the matrix A.
46:55
Let me suppose -- let's close with the best example of all.
47:01
Suppose v1 to v8, this basis, is the eigenvectors.
47:08
Suppose we have an eigenvector basis so that T(vi)
47:22
is in the same direction of vi. Now, my question is, what is A?
47:29
47:35
Can you carry through the steps? Let's do them together, because we can do it in one minute.
47:43
So, we've chosen this perfect basis. And, actually, with signal image processing,
47:52
they might look for the eigenvectors. But that would take more calculation
47:57
time that just saying, OK, we'll use the wavelet basis. Or, OK, we'll use the Fourier basis.
48:04
But the very best basis is the eigenvector basis. OK, what's the matrix?
48:10
So, what's the first column of the matrix? How do I get the first column?
48:17
I take the first basis vector v1. I opt -- I look to see, what does the transformation do
48:25
to it? The output is lambda one v1. I express that output as a combination so the first input
48:36
is v1. Its output is lambda one v1.
48:47
Now write lambda one v1 as a combination of the basis vectors, well, it's already done.
48:54
It's just lambda one times the first basis vector and zero times the others. So this first column will have lambda one and zeroes.
49:03
OK. Second input is v2.
49:10
Output is lambda two v2.
49:15
OK, write that output as a combination of the (v)s. It's already done.
49:20
It's just lambda two times the second v. So we need, in the second column, we have lambda two times the second v.
49:28
Well, you see what's coming, that in that basis, in the eigenvector basis, the matrix is diagonal.
49:37
So that's the perfect basis, that's the basis we'd love to have for image processing,
49:43
but to find the eigenvectors of our pixel matrix would be too expensive.
49:49
So we do something cheaper and close, which is to choose a good basis like wavelets.
49:58
OK, thanks. So I'll -- quiz review on Wednesday, all day.
50:04
Thanks. 