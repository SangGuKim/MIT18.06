https://www.youtube.com/watch?v=nHlE7EgJFds

0:00
0:09
OK, here is lecture ten in linear algebra. Two important things to do in this lecture.
0:17
One is to correct an error from lecture nine. So the blackboard with that awful error is still with us.
0:27
And the second, the big thing to do is to tell you about the four subspaces that
0:34
come with a matrix. We've seen two subspaces, the column space and the null space.
0:39
There's two to go. First of all, and this is a great way to
0:45
OK. recap and correct the previous lecture -- so you remember I was just doing R^3.
0:54
I couldn't have taken a simpler example than R^3. And I wrote down the standard basis.
1:02
That's the standard basis. The basis -- the obvious basis for the whole three dimensional
1:11
space. And then I wanted to make the point that there was nothing special, nothing about that basis
1:22
that another basis couldn't have. It could have linear independence, it could span a space.
1:28
There's lots of other bases. So I started with these vectors, one one two and two two five,
1:34
and those were independent. And then I said three three seven wouldn't do, because three three seven is the sum of those.
1:42
So in my innocence, I put in three three eight. I figured probably if three three seven is on the plane,
1:53
is -- which I know, it's in the plane with these two, then probably three three eight sticks a little bit out
1:59
of the plane and it's independent and it gives a basis. But after class, to my sorrow, a student tells me,
2:09
"Wait a minute, that ba- that third vector, three three eight, is not independent." And why did she say that?
2:17
She didn't actually take the time, didn't have to, to find w- w- what combination of this one and this one
2:27
gives three three eight. She did something else.
2:33
In other words, she looked ahead, because she said, wait a minute, if I look at that matrix,
2:41
it's not invertible. That third column can't be independent of the first two,
2:47
because when I look at that matrix, it's got two identical rows.
2:53
I have a square matrix. Its rows are obviously dependent.
3:01
And that makes the columns dependent.
3:06
So there's my error. When I look at the matrix A that has those three columns,
3:13
those three columns can't be independent because that matrix is not invertible because it's got two equal rows.
3:21
And today's lecture will reach the conclusion,
3:27
the great conclusion, that connects the column
3:33
space with the row space. So those are -- the row space is now going to be another one
3:43
of my fundamental subspaces. The row space of this matrix, or of this one -- well,
3:49
the row space of this one is OK, but the row space of this one,
3:54
I'm looking at the rows of the matrix -- oh, anyway, I'll have two equal rows and the row space will be only two
4:02
dimensional. The rank of the matrix with these columns will only be two.
4:08
So only two of those columns, columns can be independent too.
4:13
The rows tell me something about the columns, in other words, something that I should have noticed and I didn't.
4:20
OK. So now let me pin down these four fundamental subspaces.
4:27
So here are the four fundamental subspaces.
4:36
This is really the heart of this approach to linear algebra, to see these four subspaces, how they're related.
4:44
So what are they? The column space, C of A.
4:53
The null space, N of A.
5:00
And now comes the row space, something new. The row space, what's in that?
5:08
It's all combinations of the rows. That's natural. We want a space, so we have to take all combinations,
5:16
and we start with the rows. So the rows span the row space.
5:22
Are the rows a basis for the row space? Maybe so, maybe no.
5:29
The rows are a basis for the row space when they're independent, but if they're dependent, as in this example,
5:39
my error from last time, they're not -- those three rows are not a basis.
5:45
The row space wouldn't -- would only be two dimensional. I only need two rows for a basis. So the row space, now what's in it?
5:52
It's all combinations of the rows of A. All combinations of the rows of A.
6:01
But I don't like working with row vectors. All my vectors have been column vectors.
6:06
I'd like to stay with column vectors. How can I get to column vectors out of these rows?
6:14
I transpose the matrix. So if that's OK with you, I'm going to transpose the
6:20
matrix. I'm, I'm going to say all combinations of the columns of A transpose.
6:31
And that allows me to use the convenient notation, the column
6:40
space of A transpose.
6:47
Nothing, no mathematics went on there. We just got some vectors that were lying down to stand up.
6:55
But it means that we can use this column space of A transpose, that's telling me in a nice matrix
7:04
notation what the row space is. OK.
7:09
And finally is another null space.
7:15
The fourth fundamental space will be the null space of A transpose.
7:22
The fourth guy is the null space of A transpose.
7:30
And of course my notation is N of A transpose.
7:36
That's the null space of A transpose. Eh, we don't have a perfect name for this space as a --
7:46
connecting with A, but our usual name is the left null space,
7:51
and I'll show you why in a moment. So often I call this the --
7:57
just to write that word -- the left null space of A.
8:10
So just the way we have the row space of A and we switch it to the column space of A transpose,
8:16
so we have this space of guys l- that I call the left null space of A, but the good notation
8:25
is it's the null space of A transpose. OK. Those are four spaces.
8:32
Where are those spaces? What, what big space are they in for -- when A is m by n?
8:40
8:47
In that case, the null space of A, what's in the null space of A?
8:54
Vectors with n components, solutions to A x equals zero. So the null space of A is in R^n.
9:01
What's in the column space of A? Well, columns.
9:08
How many components dothose columns have? m. So this column space is in R^m.
9:15
What about the column space of A transpose,
9:22
which are just a disguised way of saying the rows of A? The rows of A, in this three by six matrix,
9:31
have six components, n components.
9:36
The column space is in R^n.
9:42
And the null space of A transpose, I see that this fourth space is already getting second,
9:51
you know, second class citizen treatment and it doesn't deserve it.
9:57
It's, it should be there, it is there, and shouldn't be squeezed.
10:03
The null space of A transpose -- well, if the null space of A had vectors with n components,
10:10
the null space of A transpose will be in R^m. I want to draw a picture of the four spaces.
10:19
OK.
10:24
OK. Here are the four spaces.
10:32
OK, Let me put n dimensional space over on this side.
10:38
Then which were the subspaces in R^n? The null space was and the row space was.
10:47
So here we have the -- can I make that picture of the row
10:53
space? And can I make this kind of picture of the null space?
11:00
11:06
That's just meant to be a sketch, to remind you that they're in this -- which you know, how --
11:13
what type of vectors are in it? Vectors with n components. Over here, inside, consisting of vectors with m components,
11:25
is the column space and what I'm calling
11:33
the null space of A transpose.
11:39
Those are the ones with m components. OK.
11:44
To understand these spaces is our, is our job now. Because by understanding those spaces,
11:50
we know everything about this half of linear algebra.
11:56
What do I mean by understanding those spaces? I would like to know a basis for those spaces.
12:03
For each one of those spaces, how would I create --
12:10
construct a basis? What systematic way would produce a basis?
12:15
And what's their dimension?
12:24
OK. So for each of the four spaces, I
12:30
have to answer those questions. How do I produce a basis? And then -- which has a somewhat long answer.
12:38
And what's the dimension, which is just a number, so it has a real short answer.
12:44
Can I give you the short answer first? I shouldn't do it, but here it is.
12:50
I can tell you the dimension of the column space.
12:56
Let me start with this guy. What's its dimension? I have an m by n matrix.
13:01
The dimension of the column space is the rank,
13:09
r. We actually got to that at the end of the last lecture,
13:17
but only for an example. So I really have to say, OK, what's going on there.
13:24
I should produce a basis and then I just look to see how many vectors I needed in that basis,
13:31
and the answer will be r. Actually, I'll do that, before I get on to the others.
13:38
What's a basis for the columns space? We've done all the work of row reduction,
13:46
identifying the pivot columns, the ones that have pivots, the ones that end up with pivots.
13:53
But now I -- the pivot columns I'm interested in are columns of A, the original A.
14:00
And those pivot columns, there are r of them. The rank r counts those.
14:05
Those are a basis. So if I answer this question for the column space,
14:12
the answer will be a basis is the pivot columns
14:18
and the dimension is the rank r, and there are r pivot columns
14:23
and everything great. OK. So that space we pretty well understand.
14:29
I probably have a little going back to see that --
14:36
to prove that this is a right answer, but you know it's the right answer.
14:42
Now let me look at the row space.
14:49
OK. Shall I tell you the dimension of the row space?
14:55
Yes. Before we do even an example, let me tell you the dimension of the row space.
15:00
Its dimension is also r. The row space and the column space have the same dimension.
15:06
That's a wonderful fact. The dimension of the column space of A transpose --
15:12
that's the row space -- is r. That, that space is r dimensional.
15:19
Snd so is this one.
15:24
OK. That's the sort of insight that got used in this example.
15:35
If those -- are the three columns of a matrix --
15:40
let me make them the three columns of a matrix by just erasing some brackets.
15:47
OK, those are the three columns of a matrix. The rank of that matrix, if I look at the columns,
15:54
it wasn't obvious to me anyway. But if I look at the rows, now it's obvious.
16:01
The row space of that matrix obviously is two dimensional, because I see a basis for the row
16:07
space, this row and that row. And of course, strictly speaking, I'm supposed to transpose those guys, make them stand up.
16:16
But the rank is two, and therefore the column space is two dimensional by this wonderful fact
16:21
that the row space and column space have the same dimension. And therefore there are only two pivot columns, not three,
16:29
and, those, the three columns are dependent.
16:34
OK. Now let me bury that error and talk about the row space.
16:45
Well, I'm going to give you the dimensions of all the spaces. Because that's such a nice answer.
16:52
OK. So let me come back here. So we have this great fact to establish,
16:59
that the row space, its dimension is also the rank.
17:06
What about the null space? OK. What's a basis for the null space?
17:13
What's the dimension of the null space? Let me, I'll put that answer up here for the null space.
17:20
Well, how have we constructed the null space?
17:27
We took the matrix A, we did those row operations to get it into a form U or, or even further.
17:36
We got it into the reduced form R. And then we read off special solutions.
17:43
Special solutions. And every special solution came from a free variable. And those special solutions are in the null space,
17:50
and the great thing is they're a basis for it. So for the null space, a basis will be the special solutions.
17:59
And there's one for every free variable, right?
18:07
For each free variable, we give that variable the value one, the other free variables zero.
18:13
We get the pivot variables, we get a vector in the -- we get a special solution.
18:20
So we get altogether n-r of them, because that's the number of free variables.
18:30
If we have r -- this is the dimension is r, is the number of pivot variables.
18:38
This is the number of free variables. So the beauty is that those special solutions do form
18:44
a basis and tell us immediately that the dimension of the null
18:51
space is n -- I better write this well, because it's so nice -- n-r.
19:00
And do you see the nice thing? That the two dimensions in this n dimensional space,
19:08
one subspace is r dimensional -- to be proved, that's the row space.
19:15
The other subspace is n-r dimensional, that's the null space.
19:21
And the two dimensions like together give n. The sum of r and n-R is n.
19:28
And that's just great. It's really copying the fact that we have n variables,
19:35
r of them are pivot variables and n-r are free variables, and n altogether.
19:41
OK. And now what's the dimension of this poor misbegotten fourth
19:47
subspace? It's got to be m-r.
19:53
The dimension of this left null space, left out practically,
20:00
is m-r. Well, that's really just saying that this -- again,
20:08
the sum of that plus that is m, and m is correct,
20:15
it's the number of columns in A transpose.
20:21
A transpose is just as good a matrix as A. It just happens to be n by m.
20:29
It happens to have m columns, so it will have m variables
20:38
when I go to A x equals 0 and m of them, and r of them will be pivot variables and m-r will
20:46
be free variables. A transpose is as good a matrix as A.
20:52
It follows the same rule that the this plus the dimension -- this dimension plus this dimension adds up to the number
21:00
of columns. And over here, A transpose has m columns.
21:06
OK. OK. So I gave you the easy answer, the dimensions.
21:13
Now can I go back to check on a basis?
21:21
We would like to think that -- say the row space, because we've got a basis for the column space.
21:29
The pivot columns give a basis for the column space. Now I'm asking you to look at the row space.
21:36
And I -- you could say, OK, I can produce a basis for the row space by transposing my matrix, making those columns,
21:45
then doing elimination, row reduction, and checking out the pivot columns in this transposed
21:54
matrix. But that means you had to do all that row reduction on A transpose.
22:00
It ought to be possible, if we take a matrix A -- let me take the matrix -- maybe we had this matrix in the last
22:08
lecture. 1 1 1, 2 1 2, 3 2 3, 1 1 1.
22:15
22:21
OK. That, that matrix was so easy. We spotted its pivot columns, one and two, without actually
22:29
doing row reduction. But now let's do the job properly.
22:35
So I subtract this away from this to produce a zero. So one 2 3 1 is fine.
22:42
Subtracting that away leaves me minus 1 -1 0, right? And subtracting that from the last row, oh, well that's easy.
22:49
OK? I'm doing row reduction.
22:56
Now I've -- the first column is all set. The second column I now see the pivot.
23:04
And I can clean up, if I -- actually, OK.
23:09
Why don't I make the pivot into a 1. I'll multiply that row through by by -1, and then I have 1 1.
23:18
That was an elementary operation I'm allowed,
23:24
multiply a row by a number. And now I'll do elimination. Two of those away from that will knock this guy out
23:31
and make this into a 1. So that's now a 0 and that's a
23:36
OK. Done. That's R.
23:42
I'm seeing the identity matrix here. I'm seeing zeros below.
23:48
And I'm seeing F there. OK.
23:53
What about its row space?
24:00
What happened to its row space -- well, what happened -- let me first ask, just because this is, is -- sometimes something does happen.
24:06
Its column space changed. The column space of R is not the column space of A, right?
24:18
Because 1 1 1 is certainly in the column space of A and certainly not in the column space of R.
24:26
I did row operations. Those row operations preserve the row space.
24:33
So the row, so the column spaces are different. Different column spaces, different column spaces.
24:39
24:45
But I believe that they have the same row space.
24:55
Same row space. I believe that the row space of that matrix and the row space
25:04
of this matrix are identical. They have exactly the same vectors in them.
25:09
Those vectors are vectors with four components, right? They're all combinations of those rows.
25:17
Or I believe you get the same thing by taking all combinations of these rows.
25:24
And if true, what's a basis?
25:29
What's a basis for the row space of R, and it'll be a basis for the row space of the original A,
25:42
but it's obviously a basis for the row space of R. What's a basis for the row space of that matrix?
25:47
The first two rows. So a basis for the row -- so a basis is,
25:57
for the row space of A or of R, is, is the first R rows of R.
26:15
Not of A. Sometimes it's true for A, but not necessarily.
26:21
But R, we definitely have a matrix here whose row space we
26:29
can, we can identify. The row space is spanned by the three rows,
26:36
but if we want a basis we want independence. So out goes row three.
26:43
The row space is also spanned by the first two rows. This guy didn't contribute anything.
26:48
And of course over here this 1 2 3 1 in the bottom didn't contribute anything. We had it already.
26:56
So this, here is a basis. 1 0 1 1 and 0 1 1 0.
27:04
I believe those are in the row space. I know they're independent. Why are they in the row space?
27:10
Why are those two vectors in the row space? Because all those operations we did,
27:17
which started with these rows and took combinations of them
27:22
-- I took this row minus this row, that gave me something
27:28
that's still in the row space. That's the point. When I took a row minus a multiple of another row,
27:36
I'm staying in the row space. The row space is not changing. My little basis for it is changing,
27:43
and I've ended up with, sort of the best basis.
27:49
If the columns of the identity matrix are the best basis for R^3 or R^n, the rows of this matrix are the best basis
28:00
for the row space. Best in the sense of being as clean as I can make it.
28:06
Starting off with the identity and then finishing up with whatever has to be in there.
28:11
OK. Do you see then that the dimension is r?
28:16
For sure, because we've got r pivots, r non-zero rows.
28:23
We've got the right number of vectors, r. They're in the row space, they're independent.
28:30
That's it. They are a basis for the row space. And we can even pin that down further.
28:36
How do I know that every row of A is a combination?
28:41
How do I know they span the row space? Well, somebody says, I've got the right number of them,
28:48
so they must. But -- and that's true. But let me just say, how do I know that this row is
28:54
a combination of these? By just reversing the steps of row reduction.
29:01
If I just reverse the steps and go from A -- from R back to A,
29:07
then what do I, what I doing? I'm starting with these rows, I'm taking combinations of them.
29:15
After a couple of steps, undoing the subtractions that I did before, I'm back to these rows.
29:22
So these rows are combinations of those rows. Those rows are combinations of those rows.
29:28
The two row spaces are the same. The bases are the same.
29:34
And the natural basis is this guy. Is that all right for the row space?
29:41
The row space is sitting there in R in its cleanest possible form.
29:47
OK. Now what about the fourth guy, the null space of A transpose?
29:56
First of all, why do I call that the left null space?
30:03
So let me save that and bring that down.
30:11
OK. So the fourth space is the null space of A transpose.
30:20
So it has in it vectors, let me call them y,
30:27
so that A transpose y equals 0. If A transpose y equals 0, then y
30:35
is in the null space of A transpose, of course. So this is a matrix times a column equaling zero.
30:47
And now, because I want y to sit on the left
30:56
and I want A instead of A transpose, I'll just transpose that equation.
31:03
Can I just transpose that? On the right, it makes the zero vector lie down.
31:10
And on the left, it's a product, A, A transpose times y.
31:21
If I take the transpose, then they come in opposite order, right? So that's y transpose times A transpose transpose.
31:30
But nobody's going to leave it like that.
31:35
That's -- A transpose transpose is just A, of course. When I transposed A transpose I got back to A.
31:43
Now do you see what I have now? I have a row vector, y transpose,
31:51
multiplying A, and multiplying from the left.
31:58
That's why I call it the left null space. But by making it -- putting it on the left,
32:05
I had to make it into a row instead of a column vector, and so my convention is I usually don't do that.
32:15
I usually stay with A transpose y equals 0. OK.
32:20
And you might ask, how do we get a basis -- or I might ask,
32:27
how do we get a basis for this fourth space, this left null space?
32:36
OK. I'll do it in the example. As always -- not that one.
32:43
32:49
The left null space is not jumping out at me here.
32:57
I know which are the free variables -- the special solutions, but those are special solutions to A x
33:03
equals zero, and now I'm looking at A transpose, and I'm not seeing it here. So -- but somehow you feel that the work that you did which
33:12
simplified A to R should have revealed the left null space
33:19
too. And it's slightly less immediate, but it's there.
33:25
So from A to R, I took some steps,
33:31
and I guess I'm interested in what were those steps, or what were all of them together.
33:36
I don't -- I'm not interested in what particular ones they were.
33:43
I'm interested in what was the whole matrix that took me from A to R.
33:51
How would you find that? Do you remember Gauss-Jordan, where you
33:58
tack on the identity matrix? Let's do that again. So I, I'll do it above, here.
34:06
So this is now, this is now the idea of --
34:13
I take the matrix A, which is m by n.
34:20
In Gauss-Jordan, when we saw him before -- A was a square invertible matrix and we were finding its inverse.
34:27
Now the matrix isn't square. It's probably rectangular. But I'll still tack on the identity matrix, and of course
34:36
since these have length m it better be m by m.
34:42
And now I'll do the reduced row echelon form of this matrix.
34:49
And what do I get?
34:56
35:01
The reduced row echelon form starts with these columns, starts with the first columns, works like mad, and produces R.
35:11
Of course, still that same size, m by n. And we did it before. And then whatever it did to get R,
35:19
something else is going to show up here. Let me call it E, m by m.
35:26
It's whatever -- do you see that E is just going to contain a record of what we did?
35:32
We did whatever it took to get A to become R.
35:38
And at the same time, we were doing it to the identity matrix.
35:44
So we started with the identity matrix, we buzzed along. So we took some --
35:51
all this row reduction amounted to multiplying on the left by some matrix, some series of elementary matrices
36:00
that altogether gave us one matrix, and that matrix is E.
36:05
So all this row reduction stuff amounted to multiplying by E.
36:11
How do I know that? It certainly amounted to multiply it by something.
36:16
And that something took I to E, so that something was E. So now look at the first part, E A is R.
36:29
No big deal. All I've said is that the row reduction steps that we all
36:38
know -- well, taking A to R, are in a, in some matrix,
36:45
and I can find out what that matrix is by just tacking I on and seeing what comes out.
36:51
What comes out is E. Let's just review the invertible square case.
36:58
What happened then? Because I was interested in it in chapter two also.
37:04
When A was square and invertible, I took A I, I did row, row elimination.
37:10
And what was the R that came out? It was I. So in chapter two, in chapter two, R was I.
37:24
The row the, the reduced row echelon form of a nice invertible square matrix is the identity.
37:31
So if R was I in that case, then E was -- then E was A inverse,
37:41
because E A is I. Good. That's, that was good and easy.
37:48
Now what I'm saying is that there still is an E. It's not A inverse any more, because A is rectangular.
37:55
It hasn't got an inverse. But there is still some matrix E that connected this to this --
38:05
oh, I should have figured out in advanced what it was. Shoot.
38:11
I didn't -- I did those steps and sort of erased as I went -- and, I should have done them to the identity too.
38:20
Can I do that? Can I do that? I'll keep the identity matrix, like I'm supposed to do,
38:26
and I'll do the same operations on it, and see what I end up with. OK.
38:31
So I'm starting with the identity -- which I'll write in light, light enough, but --
38:40
OK. What did I do? I subtracted that row from that one and that row from that one.
38:45
OK, I'll do that to the identity. So I subtract that first row from row two and row three.
38:52
Good. Then I think I multiplied -- Do you remember? I multiplied row two by minus one.
39:01
Let me just do that. Then what did I do? I subtracted two of row two away from row one.
39:14
I better do that. Subtract two of this away from this. That's minus one, two of these away leaves a plus 2 and 0.
39:24
I believe that's E. The way to check is to see, multiply that E by this A,
39:35
just to see did I do it right. So I believe E was -1 2 0, 1 -1 0, and -1 0 1.
39:49
OK. That's my E, that's my A, and that's R.
39:58
All right. All I'm struggling to do is right, the reason I wanted this blasted E was so that I could figure
40:09
out the left null space, not only its dimension, which I know --
40:17
actually, what is the dimension of the left null space? So here's my matrix.
40:23
What's the rank of the matrix? And the dimension of the null -- of the left null space is
40:30
supposed to be m-r. It's 3 -2, 1. I believe that the left null space is one dimensional.
40:39
There is one combination of those three rows that produces the zero row.
40:46
There's a basis -- a basis for the left null space has only
40:52
got one vector in it. And what is that vector? It's here in the last row of E.
40:58
But we could have seen it earlier. What combination of those rows gives the zero row?
41:05
-1 of that plus one of that. So a basis for the left null space of this matrix --
41:14
I'm looking for combinations of rows that give the zero row if I'm looking at the left null space.
41:22
For the null space, I'm looking at combinations of columns to get the zero column. Now I'm looking at combinations of these three rows
41:29
to get the zero row, and of course there is my zero row, and here is my vector that produced it.
41:37
-1 of that row and one of that row. Obvious. OK.
41:42
So in that example -- and actually in all examples, we have seen how to produce a basis for the left null space.
41:51
I won't ask you that all the time, because -- it didn't come out immediately from R.
41:58
We had to keep track of E for that left null space.
42:03
But at least it didn't require us to transpose the matrix and start all over again.
42:10
OK, those are the four subspaces. Can I review them?
42:15
The row space and the null space are in R^n. Their dimensions add to n.
42:22
The column space and the left null space are in R^m,
42:27
and their dimensions add to m. OK.
42:33
So let me close these last minutes
42:39
by pushing you a little bit more to a new type of vector space.
42:49
All our vector spaces, all the ones that we took seriously, have been subspaces of some real three or n dimensional space.
43:01
Now I'm going to write down another vector space, a new vector space.
43:06
43:14
Say all three by three matrices.
43:26
My matrices are the vectors.
43:31
Is that all right? I'm just naming them. You can put quotes around vectors. Every three by three matrix is one of my vectors.
43:40
Now how I entitled to call those things vectors? I mean, they look very much like matrices.
43:46
But they are vectors in my vector space because they obey the rules. All I'm supposed to be able to do with vectors is add them --
43:55
I can add matrices -- I'm supposed to be able to multiply them by scalar numbers
44:01
like seven -- well, I can multiply a matrix by And that
44:09
-- and I can take combinations of matrices, I can take three of one matrix minus five of another
44:15
matrix. And those combinations, there's a zero matrix, the matrix
44:21
that has all zeros in it. If I add that to another matrix, it doesn't change it. All the good stuff.
44:26
If I multiply a matrix by one it doesn't change it. All those eight rules for a vector space
44:32
that we never wrote down, all easily satisfied. So now we have a different --
44:41
now of course you can say you can multiply those matrices.
44:46
I don't care. For the moment, I'm only thinking of these matrices as forming a vector space -- so I only doing A plus B and c
44:57
times A. I'm not interested in A B for now.
45:03
The fact that I can multiply is not
45:09
relevant to th- to a vector space. OK. So I have three by three matrices.
45:15
And how about subspaces?
45:21
What's -- tell me a subspace of this matrix space. Let me call this matrix space M.
45:30
That's my matrix space, my space of all three by three matrices. Tell me a subspace of it.
45:37
What about the upper triangular matrices? OK. So subspaces.
45:43
Subspaces of M.
45:50
All, all upper triangular matrices.
46:00
Another subspace. All symmetric matrices.
46:11
The intersection of two subspaces is supposed to be a subspace. We gave a little effort to the proof of that fact.
46:20
If I look at the matrices that are in this subspace -- they're symmetric, and they're also in this subspace,
46:26
they're upper triangular, what do they look like? Well, if they're symmetric but they
46:33
have zeros below the diagonal, they better have zeros above the diagonal, so the intersection
46:38
would be diagonal matrices.
46:44
That's another subspace, smaller than those.
46:50
How can I use the word smaller? Well, I'm now entitled to use the word smaller.
46:56
I mean, well, one way to say is, OK, these are contained in those.
47:02
These are contained in those. But more precisely, I could give the dimension of these spaces.
47:09
So I could -- we can compute -- let's compute it next time,
47:14
the dimension of all upper -- of the subspace of upper triangular three by three matrices.
47:20
The dimension of symmetric three by three matrices. The dimension of diagonal three by three matrices.
47:27
Well, to produce dimension, that means I'm supposed to produce a basis, and then
47:33
I just count how many vecto- how many I needed in the basis. Let me give you the answer for this one.
47:39
What's the dimension? The dimension of this -- say, this subspace, let me call it D, all diagonal matrices.
47:47
The dimension of this subspace is --
47:54
as I write you're working it out -- three. Because here's a matrix in this -- it's a diagonal matrix.
48:09
Here's another one.
48:15
Here's another one. Better make it diagonal, let me put a seven there.
48:22
That was not a very great choice,
48:28
but it's three diagonal matrices, and I believe that they're a basis.
48:33
I believe that those three matrices are independent and I believe that any diagonal matrix is
48:40
a combination of those three. So they span the subspace of diagonal matrices.
48:47
Do you see that idea? It's like stretching the idea from R^n to R^(n by n),
48:55
three by three. But the -- we can still add, we can still multiply by numbers,
49:02
and we just ignore the fact that we can multiply two matrices together.
49:07
OK, thank you. That's lecture ten. 