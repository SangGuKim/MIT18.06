https://www.youtube.com/watch?v=6-wh6yvk6uc

0:00
0:12
This is lecture twelve. OK. We've reached twelve lectures. And this one is more than the others about applications
0:20
of linear algebra. And I'll confess. When I'm giving you examples of the null space and the row
0:29
space, I create a little matrix. You probably see that I just invent that matrix
0:35
as I'm going. And I feel a little guilty about it,
0:41
because the truth is that real linear algebra uses matrices
0:49
that come from somewhere. They're not just, like, randomly invented by the instructor.
0:55
They come from applications. They have a definite structure.
1:00
And anybody who works with them gets, uses that structure.
1:07
I'll just report, like, this weekend I was at an event with chemistry professors.
1:15
OK, those guys are row reducing matrices, and what matrices are they working with?
1:21
Well, their little matrices tell them how much of each element
1:26
goes into the -- or each molecule, how many molecules of each go into a reaction and what comes out.
1:34
And by row reduction they get a clearer picture of a complicated reaction.
1:40
And this weekend I'm going to -- to a sort of birthday party at Mathworks.
1:49
So Mathworks is out Route 9 in Natick.
1:55
That's where Matlab is created. It's a very, very successful, software,
2:01
tremendously successful. And the conference will be about how linear algebra is used.
2:08
And so I feel better today to talk about what I think is the most important model
2:16
in applied math. And the discrete version is a graph.
2:24
So can I draw a graph? Write down the matrix that's associated with it,
2:29
and that's a great source of matrices. You'll see. So a graph is just, so a graph --
2:38
to repeat -- has nodes and edges.
2:47
OK. And I'm going to write down the graph, a graph, so I'm just
2:53
creating a small graph here. As I mentioned last time, we would be very interested
2:59
in the graph of all, websites.
3:04
Or the graph of all telephones. I mean -- or the graph of all people in the world.
3:09
Here let me take just, maybe nodes one two three --
3:18
well, I better put in an -- I'll put in that edge and maybe an edge to, to a node four,
3:25
and another edge to node four. How's that? So there's a graph with four nodes.
3:34
So n will be four in my -- n equal four nodes.
3:41
And the matrix will have m equal the number -- there'll be a row for every edge,
3:47
so I've got one two three four five edges. So that will be the number of rows.
3:56
And I have to to write down the matrix that I want to, I want to study, I need to give a direction to every edge,
4:05
so I know a plus and a minus direction. So I'll just do that with an arrow.
4:10
Say from one to two, one to three, two to three, one to four, three to four.
4:15
That just tells me, if I have current flowing on these edges
4:24
then I know whether it's -- to count it as positive or negative according as whether it's with the arrow or against
4:32
the arrow. But I just drew those arrows arbitrarily. OK. Because I -- my example is going to come -- the example I'll --
4:41
the words that I will use will be words like potential, potential difference, currents.
4:50
In other words, I'm thinking of an electrical network.
4:55
But that's just one possibility. My applied math class builds on this example.
5:03
It could be a hydraulic network, so we could be doing, flow of water, flow of oil.
5:11
Other examples, this could be a structure.
5:18
Like the -- a design for a bridge or a design for a Buckminster Fuller dome.
5:27
Or many other possibilities, so many. So l- but let's take potentials and currents
5:34
as, as a basic example, and let me create the matrix that tells you exactly what the graph tells
5:42
you. That's --, that's it. So now I'll call it the incidence matrix, incidence
5:49
matrix. So let me write it down, and you'll see,
5:56
OK. what its properties are. So every row corresponds to an edge.
6:04
I have five rows from five edges, and let me write down again what this graph looks like.
6:12
OK, the first edge, edge one, goes from node one to two.
6:19
So I'm going to put in a minus one and a plus one in th- this
6:25
corresponds to node one two three and four,
6:30
That's a basis for the null space. the four columns. The five rows correspond -- the first row corresponds to edge
6:37
one. Edge one leaves node one and goes into node two, and that --
6:42
and it doesn't touch three and four. Edge two, edge two goes -- oh, I haven't numbered these edges.
6:50
I just figured that was probably edge one, but I didn't say so. Let me take that to be edge one.
6:56
Let me take this to be edge two. Let me take this to be edge three. This is edge four.
7:02
Ho, I'm discovering -- no, wait a minute. Did I number that twice? Here's edge four.
7:09
And here's edge five. All right. OK?
7:15
So, so edge one, as I said, goes from node one to two.
7:20
Edge two goes from two to three, node two to three, so minus one and one in the second and third columns.
7:30
Edge three goes from one to three.
7:36
I'm, I'm tempted to stop for a moment with those three edges. The null space is actually one dimensional, and it's the line Edges one two three, those form what would we, A basis for the null space will be
7:45
just that.1 what do you call the, the little, the little,
7:50
the subgraph formed by edges one, two, and three? That's a loop.
7:56
And the number of loops and the position of the loops will be crucial. OK.
8:01
Actually, here's a interesting point about loops. If I look at those rows, corresponding
8:09
to edges one two three, and these guys made a loop.
8:18
You want to tell me -- if I just looked at that much of the matrix it would be natural for me to ask,
8:25
are those rows independent? Are the rows independent?
8:31
And can you tell from looking at that if they are or are not independent? Do you see a, a relation between those three rows?
8:41
Yes. If I add that row to that row, I get this row.
8:47
So, so that's like a hint here that loops correspond to dependent, linearly dependent column --
8:54
linearly dependent give me a basis for the null space. rows.
9:00
OK, let me complete the incidence matrix. Number four, edge four is going from node one to node four.
9:09
And the fifth edge is going from node three to node four.
9:16
OK. There's my matrix. It came from the five edges and the four nodes.
9:24
And if I had a big graph, I'd have a big matrix.
9:31
And what questions do I ask about matrices?
9:37
Can I ask -- here's the review now. There's a matrix that comes from somewhere. of all vectors
9:44
through that one. If, if it was a big graph, it would be a large matrix, but a lot of zeros, right?
9:50
Because every row only has two non-zeros. So the number of -- it's a very sparse matrix.
9:57
The number of non-zeros is exactly two times
10:03
five, it's two m. Every row only has two non-zeros. And that's with a lot of structure.
10:10
And -- that was the point I wanted to begin with, that graphs, that real graphs from real --
10:18
real matrices from genuine problems have structure.
10:24
OK. We can ask, and because of the structure, we can answer, If it -- yeah, let me ask you just always,
10:31
the, the main questions about matrices. So first question, what about the null space?
10:38
So what I asking if I ask you for the null space of that matrix? I'm asking you if I'm looking at the columns of the matrix,
10:47
four columns, and I'm asking you, So there's a basis for it, and here is the whole null are those columns independent? If the columns are independent, then what's in the null space?
11:02
Only the zero vector, right? The null space contains -- tells us what combinations
11:10
of the columns -- it tells us how to combine columns to get zero.
11:17
Can -- and is there anything in the null space of this matrix other than just the zero vector?
11:23
In other words, are those four columns independent or dependent? What else is in the null space?
11:30
OK. That's our question. Let me, I don't know if you see the answer.
11:40
Whether there's -- so let's see. I guess we could do it properly. space.
11:45
We could solve Ax=0. So let me solve Ax=0 to find the null space.
11:54
OK. What's Ax? Can I put x in here in, in little letters?
12:01
x1, x2, x3, x4, that's -- it's got four columns.
12:08
Ax now is that matrix times x. And what do I get for Ax?
12:17
If the camera can keep that matrix multiplication there,
12:22
I'll put the answer here. Ax equal -- what's the first component of Ax?
12:30
Can you take that first row, minus one one zero zero,
12:35
and multiply by the x, and of course you get x2-x1. space.
12:41
The second row, I get x3-x2. From the third row, I get x3-x1. Any multiple of one one one one, it's the whole line in four From the fourth row, I get x4-x1.
12:56
And from the fifth row, I get x4-x3. And I want to know when is the thing zero.
13:05
This is my equation, Ax=0.
13:12
Notice what that matrix A is doing, what we've created a matrix that computes
13:19
the differences across every edge, the differences in potential. differences are zero, and that x is in the null
13:27
Let me even begin to give this interpretation. I'm going to think of this vector x, which is x1 x2 x3 x4,
13:38
as the potentials at the nodes. So I'm introducing a word, potentials at the nodes.
13:45
And now if I multiply by A, I get these --
13:57
I get these five components, x2-x1, et cetera.
14:04
And what are they? They're potential differences. That's what A computes.
14:11
If I have potentials at the nodes and I multiply by A, it gives me the potential differences, the differences
14:18
in potential, across the edges.
14:25
OK. When are those differences all zero? So I'm looking for the null space.
14:33
Of course, if all the (x)s are zero then I get zero. That, that just tells me, of course, the zero vector
14:40
is in the null space. But w- there's more in the null space.
14:47
Those columns are -- of A are dependent, right -- because I can find solutions to that equation.
14:54
dimensional space. Tell me -- the null space.
15:00
Tell me one vector in the null space, so tell me an x, it's got four components, and it makes that thing zero.
15:10
So what's a good x to do that? One one one one, constant potential.
15:22
If the potentials are constant, then all the potential
15:52
Do you see that that's the null space?
16:03
So the, so the dimension of the null space of A is one.
16:12
And there's a basis for it, and there's everything that's in it. Good. And what does that mean physically?
16:23
I mean, what does that mean in the application? That guy in the null space. It means that the potentials can only
16:33
be determined up to a constant. Potential differences are what make current flow.
16:39
That's what makes things happen.
16:45
It's these potential differences that will make something move in the, in our network, between x2- between node two and node one.
16:53
Nothing will move if all potentials are the same. If all potentials are c, c, c, and c, then nothing will move.
17:01
So we're, we have this one parameter, this arbitrary constant that raises or drops
17:10
all the potentials. It's like ranking football teams, whatever. We have a, there's a, there's a constant --
17:16
or looking at temperatures, you know, there's a flow of heat from higher temperature to lower
17:23
temperature. If temperatures are equal there's no flow,
17:28
and therefore we can measure -- we can measure temperatures by, Celsius or we
17:36
can start at absolute zero. And that arbitrary -- it's the same arbitrary constant that,
17:44
that was there in calculus. In calculus, right, when you took
17:49
the integral, the indefinite integral, there was a plus c, and you had to set a starting point to know what that c was.
17:58
So here what often happens is we fix one of the potentials, like the last one.
18:06
So a typical thing would be to ground that node.
18:12
To set its potential at zero. And if we do that, if we fix that potential
18:19
so it's not unknown anymore, then that column disappears
18:25
and we have three columns, and those three columns are independent. So I'll leave the column in there,
18:33
but we'll remember that grounding a node is the way to get it out. And grounding a node is the way to -- setting a node --
18:42
setting a potential to zero tells us the, the base for all
18:47
potentials. Then we can compute the others. But what's the -- now I've talked enough to ask
18:55
OK. what the rank of the matrix is? What's the rank then?
19:01
The rank of the matrix. So we have a five by four matrix. We've located its null space, one dimensional.
19:11
How many independent columns do we have? What's the rank? It's three.
19:17
And the first three columns, or actually any three columns, will be independent.
19:22
Any three potentials are independent, good variables.
19:28
The fourth potential is not, we need to set,
19:35
and typically we ground that node. OK. Rank is three. Rank equals three.
19:43
OK. Let's see, do I want to ask you about the column space? The column space is all combinations of those columns.
19:52
I could say more about it and I will. Let me go to the null space of A transpose,
20:01
because the equation A transpose y equals zero
20:07
is probably the most fundamental equation of applied mathematics. All right, let's talk about that.
20:14
That deserves our attention. A transpose y equals zero.
20:21
Let's -- let me put it on here.
20:29
So A transpose y equals zero. OK. So now I'm finding the null space of A transpose.
20:38
Oh, and if I ask you its dimension, you could tell me what it is.
20:43
What's the dimension of the null space of A transpose?
20:49
We now know enough to answer that question. What's the general formula for the dimension of the null space
20:55
of A transpose? A transpose, let me even write out A transpose.
21:02
This A transpose will be n by m, right?
21:10
In this case, it'll be four by five. n by m. Those columns will become rows.
21:16
Minus one zero minus one minus one zero is now the first row.
21:25
The second row of the matrix, one minus one and three zeros.
21:31
The third column now becomes the third row, zero one one zero minus one.
21:38
And the fourth column becomes the fourth row.
21:45
OK, good. There's A transpose. That multiplies y, y1 y2 y3 y4 and y5.
21:55
OK.
22:01
Now you've had time to think about this question. What's the dimension of the null space, if I set all those
22:09
-- wow. Usually -- sometime during this semester,
22:15
I'll drop one of these erasers behind there. That's a great moment. There's no recovery.
22:22
There's -- centuries of erasers are back there.
22:29
OK.
22:35
OK, what's the dimension of the null space?
22:40
Give me the general formula first in terms of r and m and n.
22:46
This is like crucial, you -- we struggled to, to decide what dimension meant,
22:54
and then we figured out what it equaled for an m
22:59
by n matrix of rank r, and the answer was m-r, right?
23:05
There are m=5 components, m=5 columns of A transpose.
23:14
And r of those columns are pivot columns, because it'll have r pivots.
23:19
It has rank r. And m-r are the free ones now for A transpose,
23:28
so that's five minus three, so that's two.
23:35
And I would like to find this null space. I know its dimension.
23:41
Now I want to find out a basis for it. And I want to understand what this equation is.
23:48
So let me say what A transpose y actually represents, why I'm interested in that equation.
23:57
I'll put it down with those old erasers and continue this.
24:04
Here's the great picture of applied mathematics. So let me complete that.
24:09
There's a matrix that I'll call C that connects potential differences to currents.
24:17
So I'll call these -- these are currents on the edges, y1 y2 y3 y4 and y5.
24:27
Those are currents on the edges.
24:34
And this relation between current and potential difference is Ohm's Law.
24:41
This here is Ohm's Law.
24:47
Ohm's Law says that the current on an edge is some number times the potential drop.
24:56
That's -- and that number is the conductance of the edge, one over the resistance.
25:01
This is the old current is, is, the relation
25:08
of current, resistance, and change in potential.
25:13
So it's a change in potential that makes some current happen, and it's Ohm's Law that says how much current happens.
25:22
OK. And then the final step of this framework is the equation A transpose y equals zero.
25:31
And that's -- what is that saying?
25:38
It has a famous name. It's Kirchoff's Current Law, KCL, Kirchoff's Current Law,
25:50
A transpose y equals zero. So that when I'm solving, and when I go back up with this
25:55
blackboard and solve A transpose y equals zero,
26:03
it's this pattern of -- that I want you to see. That we had rectangular matrices, but --
26:11
and real applications, but in those real applications comes A
26:17
and A transpose. So our four subspaces are exactly the right things to know about.
26:23
All right. Let's know about that null space of A transpose. Wait a minute, where'd it go?
26:31
There it is. OK. OK. Null space of A transpose.
26:38
We know what its dimension should be.
26:43
Let's find out -- tell me a vector in it. Tell me -- now, so what I asking you?
26:50
I'm asking you for five currents that satisfy Kirchoff's Current Law.
26:57
So we better understand what that law says. That, that law, A transpose y equals zero, what does that say, say in the first row of A transpose?
27:09
That says -- the so the first row of A transpose says minus y1 minus y3 minus y4 is zero.
27:19
Where did that equation come from?
27:25
Let me -- I'll redraw the graph. Can I redraw the graph here, so that we -- maybe here,
27:31
so that we see again -- there was node one, node two, node three,
27:39
node four was off here. That was, that was our graph.
27:45
We had currents on those. We had a current y1 going there.
27:50
We had a current y -- what were the other, what are those edge numbers? y4 here and y3 here.
27:58
And then a y2 and a y5.
28:04
I'm, I'm just copying what was on the other board so it's ea- convenient to see it.
28:10
What is this equation telling me, this first equation of Kirchoff's Current Law?
28:19
What does that mean for that graph? Well, I see y1, y3, and y4 as the currents leaving node one.
28:29
So sure enough, the first equation refers to node one, and what does it say?
28:34
It says that the net flow is zero. That, that equation A transpose y, Kirchoff's Current Law,
28:43
is a balance equation, a conservation law. Physicists, be overjoyed, right, by this stuff.
28:51
It, it says that in equals out. And in this case, the three arrows are all going out,
29:01
so it says y1, y3, and y4 add to zero. Let's take the next one.
29:07
The second row is y1-y2, and that's all that's in that row.
29:16
And that must have something to do with node two. And sure enough, it says y1=y2, current in equals current out.
29:25
The third one, y2 plus y3 minus y5 equals
29:33
zero. That certainly will be what's up at the third node.
29:38
y2 coming in, y3 coming in, y5 going out has to balance.
29:43
And finally, y4 plus y5 equals zero
29:48
says that at this node, y4 plus y5, the total flow,
29:57
We don't -- you know, charge doesn't accumulate at is zero. the nodes.
30:03
It travels around. OK. Now give me --
30:09
I come back now to the linear algebra question. What's a vector y that solves these equations?
30:17
Can I figure out what the null space is for this matrix, A transpose, by looking at the graph?
30:28
I'm happy if I don't have to do elimination. I can do elimination, we know how to do,
30:35
we know how to find the null space basis. We can do elimination on this matrix,
30:42
and we'll get it into a good reduced row echelon form,
30:48
and the special solutions will pop right out. But I would like to -- even to do it without that.
30:56
Let me just ask you first, if I did elimination on that, on that, matrix, what would the last row become?
31:06
What would the last row -- if I do elimination on that matrix, the last row of R will be all zeros, right?
31:16
Why? Because the rank is three. We only going to have three pivots.
31:22
And the fourth row will be all zeros when we eliminate. So elimination will tell us what, what we spotted earlier,
31:32
what's the null space -- all the, all the information, what are the dependencies.
31:38
We'll find those by elimination, but here in a real example, we can find them by thinking.
31:44
OK. Again, my question is, what is a solution y?
31:52
How could current travel around this network without collecting any charge at the nodes?
32:02
Tell me a y. OK. So a basis for the null space of A transpose.
32:12
How many vectors I looking for? Two. It's a two dimensional space.
32:18
My basis should have two vectors in it. Give me one. One set of currents.
32:24
Suppose, let me start it. Let me start with y1 as one.
32:31
OK. So one unit of -- one amp travels on edge one with
32:38
the arrow. OK, then what? What is y2?
32:44
It's one also, right? And of course what you did was solve Kirchoff's Current Law
32:50
quickly in the second equation. OK. Now we've got one amp leaving node one, coming around to node
32:57
three. What shall we do now? Well, what shall I take for y3 in other words?
33:05
Oh, I've got a choice, but why not make it what you said, negative one.
33:11
So I have just sent current, one amp, around that loop.
33:19
What shall y4 and y5 be in this case? We could take them to be zero.
33:25
This satisfies Kirchoff's Current Law.
33:31
We could check it patiently, that minus y1 minus y3 gives zero.
33:37
We know y1 is y2. The others, y4 plus y5 is certainly zero.
33:42
Any current around a loop satisfies -- satisfies the Current Law.
33:49
Now you know how to get another one. OK. Take current around this loop.
33:55
So now let y3 be one, y5 be one, and y4 be minus one.
34:04
And so, so we have the first basis vector sent current
34:10
around that loop, the second basis vector sends current around that loop.
34:15
And I've -- and those are independent, and I've got two solutions -- two vectors in the null space
34:23
of A transpose, two solutions to Kirchoff's Current Law. Of course you would say what about sending
34:31
current around the big loop. What about that vector?
34:36
One for y1, one for y2, nothing f- on y3, one for y5,
34:44
and minus one for y4. What about that? Is that, is that in the null space of A transpose?
34:52
Sure. So why don't we now have a third vector in the basis?
35:01
Because it's not independent, right? It's not independent.
35:07
This vector is the sum of those two. If I send current around that and around that --
35:14
then on this edge y3 it's going to cancel out and I'll have altogether current around the whole, the outside loop.
35:23
That's what this one is, but it's a combination of those two.
35:28
Do you see that I've now, I've identified the null space of A
35:33
transpose -- but more than that, we've solved Kirchoff's Current Law.
35:40
And understood it in terms of the network.
35:48
OK. So that's the null space of A transpose. I guess I -- there's always one more space to ask you about.
35:58
Let's see, I guess I need the row space of A, the column
36:04
space of A transpose.
36:10
So what's N, what's its dimension? Yup?
36:15
What's the dimension of the row space of A? If I look at the original A, it had five rows.
36:21
How many were independent?
36:27
Oh, I guess I'm asking you the rank again, right? And the answer is three, right?
36:33
Three independent rows. When I transpose it, there's three independent columns.
36:38
Are those columns independent, those three? The first three columns, are they the pivot columns
36:45
of the matrix? No. Those three columns are not independent.
36:51
There's a in fact, this tells me a relation between them. There's a vector in the null space that
36:57
says the first column plus the second column equals the third column.
37:03
They're not independent because they come from a loop. So the pivot columns, the pivot columns of this matrix
37:11
will be the first, the second, not the third, but the fourth.
37:18
One, columns one, two, and four are OK.
37:24
Where are they -- those are the columns of A transpose, those correspond to edges.
37:30
So there's edge one, there's edge two, and there's edge four.
37:37
So there's a -- that's like --
37:46
is a, smaller graph. If I just look at the part of the graph that I've, that
37:52
I've, thick -- used with thick edges, it has the same four nodes.
38:00
It only has three edges. And the, those edges correspond to the independent guys.
38:08
And in the graph there -- those three edges have no loop,
38:14
right? The independent ones are the ones that don't have a loop. All the -- dependencies came from loops.
38:22
They were the things in the null space of A transpose. If I take three pivot columns, there
38:28
are no dependencies among them, and they form a graph without a loop, and I just want to ask you
38:34
what's the name for a graph without a loop? So a graph without a loop is -- has got not very many edges,
38:43
right? I've got four nodes and it only has three edges, and if I put another edge in, I would have a loop.
38:53
So it's this graph with no loops, and it's the one where the rows of A are independent.
39:01
And what's a graph called that has no loops? It's called a tree.
39:06
So a tree is the name for a graph with no loops.
39:11
39:17
And just to take one last step here.
39:24
Using our formula for dimension. Using our formula for dimension, let's look --
39:33
once at this formula.
39:41
The dimension of the null space of A transpose is m-r.
39:49
OK. This is the number of loops, number of independent loops.
39:58
m is the number of edges.
40:04
And what is r? What is r for our -- we'll have to remember way back.
40:12
The rank came -- from looking at the columns of our matrix.
40:17
So what's the rank? Let's just remember. Rank was -- you remember there was one --
40:26
we had a one dimensional -- rank was n minus one, that's what I'm struggling to say.
40:33
Because there were n columns coming from the n nodes, so it's minus, the number of nodes minus one,
40:45
because of that C, that one one one one vector
40:50
in the null space. The columns were not independent. There was one dependency, so we needed n minus one.
40:58
This is a great formula. This is like the first shall I, --
41:06
write it slightly differently? The number of edges -- let me put things --
41:14
have I got it right? Number of edges is m, the number -- r- is m-r, OK.
41:22
So, so I'm getting -- let me put the number of nodes on the other side. So I -- the number of nodes --
41:31
I'll move that to the other side -- minus the number of edges plus the number of loops is --
41:46
I have minus, minus one is one. The number of nodes minus the number
41:52
of edges plus the number of loops is one. These are like zero dimensional guys.
41:58
They're the points on the graph. The edges are like one dimensional things,
42:03
they're, they connect nodes. The loops are like two dimensional things.
42:09
They have, like, an area. And this count works for every graph.
42:16
And it's known as Euler's Formula.
42:22
We see Euler again, that guy never stopped. OK.
42:28
And can we just check -- so what I saying? I'm saying that linear algebra proves Euler's Formula.
42:37
Euler's Formula is this great topology fact about any graph.
42:43
I'll draw, let me draw another graph, let me draw a graph with more edges and loops.
42:51
Let me put in lots of -- OK. I just drew a graph there. So what are the, what are the quantities in that formula?
42:58
How many nodes have I got? Looks like five.
43:04
How many edges have I got? One two three four five six seven.
43:10
How many loops have I got? One two three. And Euler's right, I always get one.
43:19
That, this formula, is extremely useful in understanding
43:27
the relation of these quantities -- the number of nodes, the number of edges, and the number of loops.
43:34
OK. Just complete this lecture by completing
43:39
this picture, this cycle. So let me come to the --
43:45
43:50
so this expresses the equations of applied math.
43:57
This, let me call these potential differences, say, E. So E is A x.
44:04
That's the equation for this step. The currents come from the potential differences.
44:11
y is C E. The potential -- the currents satisfy Kirchoff's Current Law.
44:20
Those are the equations of -- with no source terms.
44:25
Those are the equations of electrical circuits of many --
44:32
those are like the, the most basic three equations.
44:37
Applied math comes in this structure. The only thing I haven't got yet in the picture
44:43
is an outside source to make something happen.
44:49
I could add a current source here, I could, I could add external currents
44:55
going in and out of nodes. I could add batteries in the edges. Those are two ways.
45:01
If I add batteries in the edges, they, they come into here. Let me add current sources.
45:07
If I add current sources, those come in here.
45:13
So there's a, there's where current sources go, because the F is a like a current coming from outside.
45:22
So we have our edges, we have our graph, and then I send one amp into this node and out of this node
45:33
-- and that gives me, a right-hand side in Kirchoff's Current Law.
45:38
And can I -- to complete the lecture, I'm just going to put these three equations together.
45:45
So I start with x, my unknown. I multiply by A.
45:51
That gives me the potential differences. That was our matrix A that the whole thing started with.
45:57
I multiply by C. Those are the physical constants in Ohm's Law.
46:03
Now I have y. I multiply y by A transpose, and now I have F.
46:12
So there is the whole thing. There's the basic equation of applied math.
46:22
Coming from these three steps, in which the last step is
46:28
this balance equation. There's always a balance equation to look for. These are the --
46:34
when I say the most basic equations of applied mathematics -- I should say, in equilibrium.
46:41
Time isn't in this problem. I'm not -- and Newton's Law isn't acting here.
46:47
I'm, I'm looking at the -- equations when everything has settled down, how do the currents distribute
46:54
in the network. And of course there are big codes to solve the --
47:00
this is the basic problem of numerical linear algebra for systems of equations, because that's how they come.
47:09
And my final question. What can you tell me about this matrix A transpose C A?
47:18
Or even A transpose A? I'll just close with that question.
47:24
What do you know about the matrix A transpose A? It is always symmetric, right.
47:32
OK, thank. So I'll see you Wednesday for a full review of these chapters,
47:38
and Friday you get to tell me. Thanks. 