https://www.youtube.com/watch?v=IZqwi0wJovM

# Intro
0:00
-- and lift-off on differential equations.
0:11
So, this section is about how to solve
0:16
a system of first order, first derivative, constant
0:22
coefficient linear equations. And if we do it right, it turns directly into linear algebra.
# Linear Algebra
0:32
The key idea is the solutions to constant coefficient linear equations are exponentials.
0:41
So if you look for an exponential, then all you have to find is what's up there in the exponent
0:48
and what multiplies the exponential and that's the linear algebra. So -- and the result -- one thing we will fine --
0:56
it's completely parallel to powers of a matrix. So the last lecture was about how would you compute
1:06
A to the K or A to the 100? How do you compute high powers of a matrix?
1:12
Now it's not powers anymore, but it's exponentials.
1:18
That's the natural thing for differential equation. Okay. But can I begin with an example?
1:26
And I'll just go through the mechanics. How would I solve the differential -- two differential equations?
1:33
So I'm going to make it -- I'll have a two by two matrix and the coefficients are minus one two, one minus two and I'd better give you
1:42
some initial condition. So suppose it starts u at times zero -- this is u1, u2 --
1:50
let it -- let it -- suppose everything is in u1 at times zero.
1:57
So -- at -- at the start, it's all in u1. But what happens as time goes on, du2/dt will --
2:07
will be positive, because of that u1 term, so flow will move into the u2 component and it will go out
2:16
of the u1 component. So we'll just follow that movement as time
2:22
goes forward by looking at the eigenvalues and eigenvectors
2:28
of that matrix. That's a first job. Before you do anything else, find the --
2:34
find the matrix and its eigenvalues and eigenvectors.
2:39
So let me do that. Okay. So here's our matrix.
2:45
Maybe you can tell me right away what -- what are the eigenvalues and -- eigenvalues anyway.
2:53
And then we can check. But can you spot any of the eigenvalues of that matrix? We're looking for two eigenvalues.
3:00
Do you see -- I mean, if I just wrote that matrix down, what -- what do you notice about it?
3:07
It's singular, right. That -- that's a singular matrix. That tells me right away that one of the eigenvalues
3:14
is lambda equals zero. I can -- that's a singular matrix,
3:20
the second column is minus two times the first column, the determinant is zero, it's -- it's singular,
3:28
so zero is an eigenvalue and the other eigenvalue will be --
3:33
from the trace. I look at the trace, the sum down the diagonal is minus three.
3:39
That has to agree with the sum of the eigenvalue, so that second eigenvalue better be minus three.
3:46
I could, of course -- I could compute -- why don't I over here -- compute the determinant of A minus lambda I,
3:54
the determinant of this minus one minus lambda two one minus
4:01
two minus lambda matrix. But we know what's coming. When I do that multiplication, I get a lambda squared.
4:09
I get a two lambda and a one lambda, that's a three lambda. And then -- now I'm going to get the determinant,
4:16
which is two minus two which is zero. So there's my characteristic polynomial, this determinant.
4:26
And of course I factor that into lambda times lambda plus three
4:31
and I get the two eigenvalues that we saw coming.
4:37
What else do I need? The eigenvectors. So before I even think about the differential equation or what
4:44
-- how to solve it, let me find the eigenvectors for this matrix. Okay.
4:49
So take lambda equals zero -- so that -- that's the first eigenvalue.
4:55
Lambda one equals zero and the second eigenvalue will be lambda two equals minus three.
5:03
By the way, I -- I already know something important about this.
5:09
The eigenvalues are telling me something. You'll see how it comes out, but let me point to --
5:15
these numbers are -- this eigenvalue, a negative eigenvalue, is going to disappear.
5:23
There's going to be an e to the minus three t in the answer. That e to the minus three t as times goes on
5:31
is going to be very, very small. The other part of the answer will involve an e
5:38
to the zero t. But e to the zero t is one and that's a constant.
5:44
So I'm expecting that this solution'll have two parts, an e to the zero t part and an e to the minus three t part,
5:53
and that -- and as time goes on, the second part'll disappear and the first part will be a steady
6:00
It won't move. state. It will be -- at the end of -- as t approaches infinity,
6:06
this part disappears and this is the -- the e to the zero t part is what I get.
6:13
And I'm very interested in these steady states, so that's -- I get a steady state when I have a zero
6:19
eigenvalue. Okay. What about those eigenvectors? So what's the eigenvector that goes with eigenvalue zero?
6:26
Okay. The matrix is singular as it is, the eigenvector is --
6:31
is the guy in the null space, so what vector is in the null space of that matrix?
6:37
Let's see. I guess I probably give the free variable the value one
6:42
and I realize that if I want to get zero I need a two up here.
6:48
Okay? So Ax1 is zero x1. A x1 is zero x1.
6:55
Fine. Okay. What about the other eigenvalue? Lambda two is minus three.
7:01
Okay. How do I get the other eigenvalue, then? For the moment -- can I mentally do it?
7:08
I subtract minus three along the diagonal, which means I add three --
7:15
can I -- I'll just do it with an erase -- erase for the moment. So I'm going to add three to the diagonal.
7:20
So this minus one will become a two and --
7:26
I'll make it in big loopy letters -- and when I add three to this guy, the minus two becomes --
7:32
well, I can't make one very loopy, but how's that? Okay.
7:37
Now that's A minus three I -- A plus three I, sorry. That's A plus three I.
7:43
It's supposed to be singular, right? I-- if things -- if I did it right, this matrix should be singular and the x2,
7:51
the eigenvector should be in its null space. Okay. What do I get for the null space of this?
7:58
Maybe minus one one, or one minus one. Doesn't matter. Those are both perfectly good.
8:05
Right? Because that's in the null space of this. Now I'll -- because A times that vector is three times that
8:14
vector. Ax2 is minus three x2.
8:20
Good. Okay. Can I get A again so we see that correctly?
8:26
That was a minus one and that was a minus two. Good.
8:31
Okay. That -- that's the first job. eigenvalues and eigenvectors.
8:37
And already the eigenvalues are telling me the most important information about the answer.
8:44
But now, what is the answer? The answer is -- the solution will be U of T --
8:54
okay. Now, wh- now I use those eigenvalues and eigenvectors.
8:59
The solution is some -- there are two eigenvalues. So I -- it -- so there're going to be two special solutions
9:08
here. Two pure exponential solutions. The first one is going to be either the lambda one tx1
9:17
and the -- so that solves the equation, and so does this one.
9:23
They both are solutions to the differential equation.
9:29
That's the general solution. The general solution is a combination of that pure exponential solution
9:37
and that pure exponential solution. Can I just see that those guys do solve the equation?
9:43
So let me just check -- check on this one, for example. I -- I want to check that the -- my equation --
9:53
let's Check. remember, the equation -- du/dt is Au. I plug in e to the lambda one t x1
10:04
and let's just see that the equation's okay. I believe this is a solution to that equation.
10:12
So just plug it in. On the left-hand side, I take the time derivative --
10:17
so the left-hand side will be lambda one, e to the lambda one t x1, right?
10:25
The time derivative -- this is the term that depends on time, it's just ordinary exponential, its derivative brings down
10:33
a lambda one. On the other side of the equation it's A times this thing.
10:38
A times either the lambda one t x one, and does that check out?
10:44
Do we have equality there? Yes, because either the lambda one t appears on both sides
10:52
and the other one is Ax1 equal lambda one x1 -- check.
10:58
Do you -- so, the -- we've come to the first point to remember. These pure solutions.
11:05
Those pure solutions are the -- those pure exponentials are
11:10
the differential equations analogue of -- last time we had pure powers.
11:17
Last time -- so -- so last time, the analog was lambda --
11:24
lambda one to the K-th power x1, some amount of that, plus some amount of lambda two to the K-th power x2.
11:35
That was our formula from last time. I put it up just to -- so your eye compares those two
11:41
formulas. Powers of lambda in the -- in the difference equation -- that -- this was in the --
11:48
this was for the equation uk plus one equals A uk.
11:55
That was for the finite step -- stepping by one. And we got powers, now this is the one
12:02
we're interested in, the exponentials. So -- so that's -- that's the solution --
12:08
what are c1 and c2? Then we're through. What are c1 and c2?
12:14
Well, of course we know these actual things. Let me just -- let me come back to this.
12:22
c1 is -- we haven't figured out yet, but e to the lambda one t, the lambda one is zero so that's just a one times x1 which is
12:32
two one. So it's c1 times this one that's not moving times the vector,
12:39
the eigenvector two one and c2 times --
12:44
what's e to the lambda two t? Lambda two is minus three.
12:51
So this is the term that has the minus three t and its eigenvector is this one minus one.
12:58
So this vector solves the equation
13:06
and any multiple of it. This vector solves the equation if it's got that factor
13:12
e to the minus three t. We've got the answer except for c1 and c2.
13:17
So -- so everything I've done is immediate as soon as you know the eigenvalues and eigenvectors.
13:25
So how do we get c1 and c2? That has to come from the initial condition.
13:30
So now I -- now I use -- u of zero is given as one zero.
13:38
So this is the initial condition that will find c1 and c2.
13:46
So let me do that on the board underneath. At t equals zero, then --
13:52
I get c1 times this guy plus c2 and now I'm at times zero.
14:05
So that's a one and this is a one minus one and that's supposed to agree with u of zero one zero.
14:12
14:19
Okay. That should be two equations. That should give me c1 and c2 and then I'm through.
14:26
So what are c1 and c2? Let's see. I guess we could actually spot them by eye
14:33
or we could solve two equations in two unknowns. Let's see.
14:38
If these were both ones -- so I'm just adding -- then I would get three zero.
14:43
So what's the -- what's the solution, then?
14:49
If -- if c1 and c2 are both ones, I get three zero, so I want, like, one third of that,
14:55
because I want to get one zero. So I think it's c1 equals a third, c2 equals a third.
15:02
So finally I have the answer.
15:08
Let me keep it in the -- in this board here. Finally the answer is one third of this plus one third of this.
15:20
Do you see what -- what's actually happening with this
15:27
flow? This flow started out at -- the solution started out at one zero.
15:34
Started at one zero. Then as time went on, people moved, essentially.
15:41
Some fraction of this one moved here. And -- and in the limit, there's -- there's the limit, as --
15:52
right? As t goes to infinity, as t gets very large, this disappears and this is the steady state.
15:59
So the steady state is -- so the steady state --
16:04
u -- we could call it u at infinity is one third of two
16:14
and one. It's -- it's two thirds of one third.
16:19
So that's the -- we really -- I mean, you're getting, like, total,
16:25
insight into the behavior of the solution, what the differential equation does.
16:32
Of course, we don't -- wouldn't always have a steady state.
16:37
Sometimes we would approach zero. Sometimes we would blow up. Can we straighten out those cases?
16:45
The eigenvalue should tell us. So when do we get -- so -- so let me ask first, when do we get stability?
16:54
That's u of t going to zero.
17:00
When would the solution go to zero no matter
17:05
what the initial condition is? Negative eigenvalues, right.
17:11
Negative eigenvalues. But now I have to -- I have to ask you for one more step.
17:16
Suppose the eigenvalues are complex numbers? Because we know they could be.
17:22
Then we want stability -- this -- this -- we want --
17:27
we need all these e to the lambda t-s all going to zero
17:35
and somehow that asks us to have lambda negative.
17:40
But suppose lambda is a complex number? Then what's the test? What -- if lambda's a complex number like, oh,
17:50
suppose lambda is negative plus an imaginary part? Say lambda is minus three plus six i?
17:59
What -- what happens then? Can we just, like, do a -- a case here? If -- if this lambda is minus three plus six it,
18:11
how big is that number? Does this -- does this imaginary part play a -- play a --
18:18
play a role here or not? Or how big is -- what's the absolute value of that -- of that quantity?
18:25
It's just e to the minus three t, right?
18:32
Because this other part, this -- the -- the magnitude -- the -- this -- e to the six it -- what -- that has absolute value one.
18:41
Right? That's just this cosine of six t plus i, sine of six t.
18:50
And the absolute value squared will be cos squared plus sine squared will be one.
18:56
This is -- this complex number is running around the unit circle. This com- this -- the -- it's the real part that matters.
19:04
This is what I'm trying to do. Real part of lambda has to be negative.
19:10
If lambda's a complex number, it's the real part, the minus three, that either makes us go to zero
19:19
or doesn't, or let -- or blows up.
19:24
The imaginary part won't -- will just, like, oscillate between the two components.
19:30
Okay. So that's stability. And what about --
19:36
what about a steady state?
19:42
When would we have, a steady state, always in the same direction?
19:47
So let me -- I'll take this part away -- when -- so that was, like, checking that it's --
19:54
that it's the real part that we care about. Now, we have a steady state when --
20:01
when lambda one is zero and the other eigenvalues have what?
20:12
So I'm looking -- like, that example was, like, perfect for a steady state.
20:18
We have a zero eigenvalue and the other eigenvalues, we want those to disappear.
20:25
So the other eigenvalues have real part negative.
20:31
And we blow up, for sure -- we blow up if any real part of lambda is positive.
20:45
So if I -- if I reverse the sign of A -- of the matrix A --
20:54
suppose instead of the matrix I had, the A that I had, I changed it -- I changed all its sines.
21:00
What would that do to the eigenvalues and eigenvectors? If I -- if -- if I know the eigenvalues and eigenvectors
21:08
of A, tell me about minus A. What happens to the eigenvalues?
21:14
For minus A, they'll all change sine. So I'll have blow up.
21:20
This -- instead of the e to the minus three t, if I change that to minus -- if I change the sines in that
21:26
matrix, I would change the trace to plus three, I would have an e to the plus three t and I would have blow
21:34
up. Of course the zero eigenvalue would stay at zero,
21:39
but the other guy is taking off in -- if I reversed all the sines.
21:45
Okay. So this is -- this is the stability picture.
21:51
And for a two by two matrix, we can actually
21:56
pin down even more closely what that means. Can I -- let -- can I do that?
22:02
Let me do that -- I want to -- for a two by two matrix, I can tell whether the real part
22:11
of the eigenvalues is negative, I -- well, let me -- let me tell you what I have in mind for that.
22:18
So two by two stability -- let me -- this is just a little comment.
22:25
Two by two stability.
22:31
So my matrix, now, is just a b c d and I'm looking for the real parts of both eigenvalues
22:41
to be negative.
22:47
Okay. What -- how can I tell from looking at the matrix,
22:55
without computing its eigenvalues, whether the two eigenvalues are negative,
23:02
or at least their real parts are negative? What would that tell me about the trace?
23:07
So -- so the trace -- that's this a plus d --
23:14
what can you tell me about the trace in the case of a two by two stable matrix?
23:21
That means the eigenvalues have -- are negative, or at least the real parts of those eigenvalues are negative
23:28
-- then, when I take the -- when I look at the matrix and find its trace, what -- what do I know about that?
23:36
It's negative, right. This is the sum of the -- this equals -- this equals lambda one plus lambda two, so it's negative.
23:47
The two eigenvalues, by the way, will have -- if they're complex -- will have plus six i and minus six i.
23:54
The complex parts will -- will be conjugates of each other and disappear when we add and the trace will be negative.
24:04
Okay, the trace has to be negative. Is that enough -- is a negative trace enough to make the matrix stable?
24:14
Shouldn't be enough, right? Can I -- can you make -- what's a matrix that has a negative trace but still it's not stable?
24:24
So it -- it has a blow -- it still has a blow-up factor and a -- and a -- and a decaying one.
24:30
So what would be a -- so just -- just to see -- maybe I just put that here.
24:35
This -- now I'm looking for an example where the trace could be negative but still blow up.
24:48
Of course -- yeah, let's just take one. Oh, look, let me -- let me make it minus two zero zero one.
24:57
Okay. There's a case where that matrix has negative trace --
25:04
I know its eigenvalues of course. They're minus two and one and it blows up. It's got -- it's got a plus one eigenvalue here,
25:12
so there would be an e to the plus t in the solution and it'll blow up if it has any second component at all.
25:21
I need another condition. And it's a condition on the determinant.
25:28
And what's that condition? If I know that the two eigenvalues -- suppose I know they're negative, both negative.
25:36
What does that tell me about the determinant? Let me ask again.
25:41
If I know both the eigenvalues are negative, then I know the trace is negative
25:47
but the determinant is positive, because it's
25:53
the product of the two eigenvalues. So this determinant is lambda one times lambda two.
26:00
This is -- this is lambda one times lambda two and if they're both negative, the product is positive.
26:08
So positive determinant, negative trace. I can easily track down the -- this condition also for the --
26:17
if -- if there's an imaginary part hanging around. Okay. So that's a -- like a small but quite useful,
26:25
because two by two is always important -- comment on stability.
26:33
Okay. So let's just look at the picture again.
26:40
Okay. The main part of my lecture, the one thing you want to be able to, like, just do automatically
26:46
is this step of solving the system.
26:51
Find the eigenvalues, find the eigenvectors, find the coefficients. And notice -- what's the matrix -- in this linear system here,
27:01
I can't help -- if I -- if I have equations like that -- that's my equations column at a time --
27:08
what's the matrix form of that equation? So -- so this -- this system of equations is --
27:18
is some matrix multiplying c1, c2 to give u of zero.
27:26
One zero. What's the matrix? Well, it's obviously two one, one minus one,
27:35
but we have a name, or at least a letter -- actually a name for that matrix. Wh- what matrix are we s- are we --
27:42
are we dealing with here in this step of finding the c-s? Its letter is S --
27:50
it's the eigenvector matrix. Of course. These are the eigenvectors, there in the columns of our matrix.
27:57
So this is S c equals u of zero --
28:02
is the -- that step where you find the actual coefficients,
28:09
you find out how much of each pure exponential is
28:14
in the solution. By getting it right at the start, then it stays right
28:20
forever. I -- you're seeing this picture that each -- each pure exponential goes on its own way once you start it
28:29
from u of zero. So you start it by figuring out how much each one is present in u of zero and then off they go.
28:37
Okay. So -- so that's a system of two equations in two unknowns
28:45
coupled -- the matrix sort of couples u1 and u2 and the eigenvalues
28:53
and eigenvectors uncouple it, diagonalize it. Actually -- okay, now can I --
29:00
can I think in terms of S and lambda? So I want to write the solution down,
29:07
again in terms of S and lambda. Okay. I'll do that on this far board.
# Uncoupling
29:14
Okay. So I'm coming back to --
29:20
I'm coming back to our equation du/dt equals Au.
29:26
Now this matrix A couples them.
29:35
The whole point of eigenvectors is to uncouple. So one way to see that is introduce set u equal A --
29:46
not A. It's S, the eigenvector matrix that uncouples it.
29:54
So I'm -- I'm taking this equation as I'm given, coupled with -- with A has -- is probably full of non-zeroes,
30:04
but I'm -- by uncoupling it, I mean I'm diagonalizing it. If I can get a diagonal matrix, I'm --
30:11
I'm in. Okay. So I plug that in. This is A S v.
30:18
And this is S dv/dt.
30:25
S is a constant. It's -- this it the eigenvector matrix. This is the eigenvector matrix.
30:31
Okay. Now I'm going to bring S inverse over here.
30:37
And what have I got?
30:45
That combination S inverse A S is lambda, the diagonal matrix.
30:53
That's -- that's the point, that in -- if I'm using the eigenvectors as my basis,
31:01
then my system of equations is just diagonal.
31:06
I -- each -- there's no coupling anymore -- dv1/dt is lambda one v1.
31:13
So let's just write that down. dv1/ dt is lambda one v1
31:20
and so on for all n of the equations.
31:26
It's a system of equations but they're not connected, so they're easy to solve and why don't I just
31:32
write down the solution? v -- well, v is now some e to the lambda one t --
31:47
well, okay -- I guess my idea here now is to use, the natural notation --
31:56
v of T is e to the lambda tv of zero.
32:04
And u of t will be Se to the lambda t S inverse, u of zero.
32:16
This is the -- this is the, formula I'm headed for.
# Exponential
32:25
This -- this matrix, S e to the lambda t S inverse, that's my exponential.
32:32
That's my e to the A t, is this S e to the lambda t S inverse.
32:40
So my -- my job really now is to explain what's going on with
32:47
this matrix up in the exponential. What does that mean? What does it mean to say e to a matrix?
32:54
This ought to be easier because this is e to a diagonal matrix,
33:01
but still it's a matrix. What do we mean by e to the A t?
33:07
Because really e to the A t is my answer here.
33:13
My -- my answer to this equation is --
33:18
this u of t, this is my -- this is my e to the A t u of zero.
33:26
So it's -- my job is really now to say what's -- what does that mean?
33:31
What's the exponential of a matrix and why is that formula correct?
33:38
Okay. So I'll put that on the board underneath. What's the exponential of a matrix?
33:45
Let me come back here. So I'm talking about matrix exponentials.
33:55
e to the At. Okay. How are we going to define the exponential of a --
34:01
of something? The trick -- the idea is -- the thing to go back to is
34:09
exponential -- there's a power series for exponentials.
34:15
That's how you -- you -- the good way to define e to the x is the power series one plus x plus one half x squared,
34:25
one six x cubed and we'll do it now when the -- when we have a matrix.
34:30
So the one becomes the identity, the x is At, the x squared is At squared and I divide by two.
34:42
The cube, the x cube is At cubed over six, and what's the general term in here?
34:50
At to the n-th power divided by --
34:55
and it goes on. But what do I divide by?
35:01
So, you see the pattern -- here I divided by one, here I divided by one by two by six, those are the factorials.
35:10
It's n factorial. That was, like, the one beautiful Taylor series.
35:17
The one beautiful Taylor series -- well, there are two --
# Taylor Series
35:23
there are two beautiful Taylor series in this world. The Taylor series for e to the x is
35:29
the s with x to the n-th over n factorial.
35:35
And all I'm doing is doing the same thing for matrixes. The other beautiful Taylor series
35:40
is just the sum of x to the n-th not divided by n factorial.
35:47
Can you -- do you know what function that one is? So if I take -- this is the series,
35:53
all these sums are going from zero to infinity. What's -- what function have I got --
35:59
this is like a side comment -- this is one plus x plus x squared plus x cubed plus x
36:06
to the fourth not divided by anything, what's -- what's that function?
36:11
One plus x plus x squared plus x cubed plus x fourth forever is one over one minus x.
36:18
It's the geometric series, the nicest power series of all.
36:24
So, actually, of course, there would be a similar thing here. If -- if I wanted, I minus A t inverse would be --
36:36
now I've got matrixes. I've got matrixes everywhere, but it's just like that series
36:43
with -- and just like this one without the divisions. It's I plus At plus At squared plus At cubed and forever.
36:56
So that's actually a -- a reasonable way to find
37:02
the inverse of a matrix. If I chop it off --
37:07
well, it's reasonable if t is small. If t is a small number, then --
37:13
then t squared is extremely small, t cubed is even smaller, so approximately
37:19
that inverse is I plus At. I can keep more terms if I like.
37:24
Do you see what I'm doing? I'm just saying we can do the same thing for matrixes that we
37:31
do for ordinary functions and the good thing about the exponential series -- so in a way,
37:38
this series is better than this one. Why? Because this one always converges.
37:45
I'm dividing by these bigger and bigger numbers, so whatever matrix A and however large t is, that series --
37:54
these terms go to zero. The series adds up to a finite sum, e to the At is a -- is --
38:01
is completely defined. Whereas this second guy could fail, right?
38:08
If At is big -- somehow if At has an eigenvalue larger than one,
38:15
then when I square it it'll have that eigenvalue squared, when I cube it the eigenvalue will be cubed --
38:21
that series will blow up unless the eigenvalues of At are smaller than one.
38:28
So when the eigenvalues of At are smaller than one -- so I'd better put that in.
38:33
The -- all eigenvalues of At below one -- then that series converges and gives me the inverse.
38:42
Okay. So this is the guy I'm chiefly interested in, and I wanted
38:47
to connect it to -- oh, okay. What's -- how do I -- how do I get -- this is my, like,
38:55
main thing now to do -- how do I get e to the At --
39:02
how do I see that e to the At is the same as this?
39:10
In other words, I can find e to the At by finding S and lambda,
39:16
because now e to the lambda t is easy. Lambda's a diagonal matrix and we can write down either
39:24
the lambda t -- and will right -- in a minute. But how -- do you see what -- do you see that we're hoping for a --
39:33
we're hoping that we can compute either the A T from S
39:38
and lambda -- and I have to look at that definition and say, okay,
39:44
how do -- how do I get an S and the lambda to come out of that? Okay, can -- do you see how I --
39:50
I want to connect that to that, from that definition. So let me erase this -- the geometric series,
39:59
which isn't part of the differential equations case
40:08
and get the S and the lambda into this picture.
40:14
Oh, okay. Here we go.
40:19
So identity is fine. Now -- all right, you -- you -- you'll see how I'm --
40:26
how I'm -- how I going to get A replaced by S and S is
40:31
in lambda's? Well I use the fundamental formula of this whole chapter. A is S lambda S inverse and then times t.
40:43
That's At. Okay. What's A squared t?
40:48
I can -- I've got the divide by two, I've got the t squared and I've got an A squared.
40:56
All right, I -- so I've got a -- there's A -- there's A.
41:02
Now square it. So what happens when I square it? We've seen that before.
41:08
When I square it, I get S lambda squared S inverse, right?
41:16
When I square that thing, the -- there's an S and a -- an S cancels out an S inverse.
41:23
I'm left with the S on the left, the S inverse on the right and lambda squared in the middle.
41:29
And so on. The next one'll be S lambda cubed, S inverse --
41:36
times t cubed over three factorial. And now -- what do I do now?
41:45
I want to pull an S out from everything. I want an S out of the whole thing.
41:53
Well, look, I'd better write the identity how? I -- I want to be able to pull an S out and an S inverse out
42:01
from the other side, so I just write the identity as S times S inverse. So I have an S there and an S inverse from this side
42:11
and what have I got in the middle?
42:16
If I pull out an S and an S inverse, what have I got in the middle? I've got the identity, a lambda t,
42:23
a lambda squared t squared over two -- I've got e to the lambda t.
42:30
That's what's in the middle. That's my formula for e to the At.
42:36
Oh, now I have to ask you. Does this formula always work?
42:42
This formula always works -- well, except it's an infinite series.
42:48
But what do I mean by always work? And this one doesn't always work and I just
42:55
have to remind you of what assumption is built into this formula that's
43:00
not built into the original. The assumption that A can be diagonalized.
43:07
You'll remember that there are some small -- sm- some subset of matrixes that don't
43:14
have n independent eigenvectors, so we don't have an S inverse for those matrixes
43:20
and the whole diagonalization breaks down. We could still make it triangular.
43:26
I'll tell you about that. But diagonal we can't do for those particular degenerate
43:32
matrixes that don't have enough independent eigenvectors. But otherwise, this is golden.
43:40
Okay. So that's the formula -- that's the matrix exponential. Now it just remains for me to say what is e to the lambda t?
43:48
Can I just do that? Let me just put that in the corner here.
43:55
What is the exponential of a diagonal matrix?
44:02
Remember lambda is diagonal, lambda one down to lambda n.
44:10
What's the exponential of that diagonal matrix? Because our whole point is that this ought to be simple.
44:19
Our whole point is that to take the exponential of a diagonal matrix ought to be completely decoupled --
44:27
it ought to be diagonal, in other words, and it is. It's just e to the lambda one t, e to the lambda two t,
44:37
e to the lambda n t, all zeroes. So -- so if we have a diagonal matrix and I plug it into this
44:47
exponential formula, everything's diagonal and the diagonal terms are just the ordinary scaler
44:55
exponentials e to the lambda one t. Okay, so that's -- that's --
45:01
in a sense, I'm doing here, on this board, with -- with, like, formulas what I did on the --
45:11
in the first half of the lecture with specific matrix A and specific eigenvalues and eigenvectors.
45:19
The -- so let me show you the formulas again. But the -- so you --
45:24
I guess -- what should you know from this lecture? You should know what this matrix exponential is and, like,
45:34
when does it go to zero? Tell me again, now, the answer to that. When does e to the At approach --
45:41
get smaller and smaller as t increases? Well, the S and the S inverse aren't moving.
45:49
It's this one that has to get smaller and smaller and that one has this simple diagonal form.
45:57
And it goes to zero provided every one of these lambdas -- I -- I need to have each one of these guys go to zero,
46:04
so I need every real part of every eigenvalue negative.
46:09
Right? If the real part is negative, that's --
46:15
that takes the exponential -- that forces -- the exponential goes to zero.
46:21
Okay, so that -- that's really the difference. If I can just draw the -- here's a picture of the -- of the --
46:33
this is the complex plain. Here's the real axis and here's the imaginary axis.
46:42
And where do the eigenvalues have to be for stability in differential equations?
46:47
They have to be over here, in the left half plain. So the left half plain is this plain, real part of lambda,
46:55
less than zero. Where do the ma- where do the eigenvalues have
47:01
to be for powers of the matrix to go to zero?
47:06
Powers of the matrix go to zero if the eigenvalues are in here.
47:11
So this is the stability region for powers --
47:17
this is the region -- absolute value of lambda, less than one. That's the stability for -- that tells us that the powers of A
47:26
go to zero, this tells us that the exponential of A goes to zero. Okay.
47:31
One final example. Let me just write down how to deal with a final example.
47:38
Let's see.
47:44
So my final example will be a single equation, u''+bu'+Ku=0.
47:57
One equation, second order. How do I --
48:03
and maybe I should have used -- I'll use -- I prefer to use y here, because that's what you see in differential equations.
48:12
And I want u to be a vector. So how do I change one second order equation into a two
48:23
by two first order system? Just the way I did for Fibonacci.
48:30
I'll let u be y prime and y.
48:38
What I'm going to do is I'm going to add an extra equation,
48:43
y prime equals y prime. So I take this -- so by --
48:50
so using this as the vector unknown, now my equation is u prime.
48:58
My first order system is u prime, that'll be y double prime y prime, the derivative of u,
49:06
okay, now the differential equation is telling me that y double prime is m- so I'm just looking for --
49:14
what's this matrix? y prime y. I'm looking for the matrix A.
49:23
What's the matrix in case I have a single first order equation
49:28
and I want to make it into a two by two system? Okay, simple. The first row of the matrix is given by the equation.
49:35
So y''-by'-ky -- no problem.
49:43
And what's the second row on the matrix? Then we're done. The second row of the matrix I want just
49:50
to be the trivial equation y prime equals y prime, so I need a one and a zero there.
49:56
So matrixes like these, the gen- the general case,
50:03
if I had a five by five -- if I had a fifth order equation
50:09
and I wanted a five by five matrix, I would see the coefficients of the equation up there and then
50:15
my four trivial equations would put ones here.
50:21
This is the kind of matrix that goes from a fifth order
50:27
to a five by five first order.
50:35
So the -- and the eigenvalues will come out in a natural way connected to the differential
50:41
equation. Okay, that's differential equations. The -- a parallel lecture compared to powers of a matrix
50:49
we can now do exponentials. Thanks. 