https://www.youtube.com/watch?v=UCc9q_cAhho

0:00
-- one and --
0:11
the lecture on symmetric matrixes. So that's the most important class
0:17
of matrixes, symmetric matrixes. A equals A transpose.
0:22
So the first points, the main points of the lecture I'll tell you right away.
0:31
What's special about the eigenvalues? What's special about the eigenvectors? This is -- the way we now look at a matrix.
0:42
We want to know about its eigenvalues and eigenvectors and if we have a special type of matrix,
0:48
that should tell us something about eigenvalues and eigenvectors. Like Markov matrixes, they have an eigenvalue equal
0:56
one. Now symmetric matrixes, can I just tell you right off what
1:03
the main facts -- the two main facts are? The eigenvalues of a symmetric matrix, real --
1:09
this is a real symmetric matrix, we -- talking mostly about real matrixes.
1:15
The eigenvalues are also real.
1:21
So our examples of rotation matrixes, where -- where we got E- eigenvalues that were complex,
1:29
that won't happen now. For symmetric matrixes, the eigenvalues are real
1:34
and the eigenvectors are also very special. The eigenvectors are perpendicular, orthogonal,
1:41
so which do you prefer? I'll say perpendicular. Perp- well, they're both long words.
1:46
1:52
Okay, right. So -- I have a -- you should say "why?"
2:01
and I'll at least answer why for case one, maybe case two,
2:06
the checking the Eigen -- that the eigenvectors are perpendicular, I'll leave to, the -- to the book.
2:16
But let's just realize what -- well, first I have to say, it --
2:24
it could happen, like for the identity matrix -- there's a symmetric matrix.
2:31
Its eigenvalues are certainly all real, they're all one for the identity matrix.
2:36
What about the eigenvectors? Well, for the identity, every vector is an eigenvector.
2:44
So how can I say they're perpendicular? What I really mean is the -- they --
2:49
this word are should really be written can be chosen
2:57
perpendicular. That is, if we have -- it's the usual case.
3:02
If the eigenvalues are all different, then each eigenvalue has one line of eigenvectors
3:09
and those lines are perpendicular here. But if an eigenvalue's repeated, then there's
3:16
a whole plane of eigenvectors and all I'm saying is that in that plain, we can choose perpendicular ones.
3:25
So that's why it's a can be chosen part, is -- this is in the case of a repeated eigenvalue where
3:31
there's some real, substantial freedom. But the typical case is different eigenvalues,
3:39
all real, one dimensional eigenvector space, Eigen spaces, and all perpendicular.
3:46
So, just -- let's just see the conclusion. If we accept those as correct, what happens --
3:56
and I also mean that there's a full set of them. so forgive me for doing such a thing, but, I'll look at the
4:01
I -- so that's part of this picture here, that there -- there's a complete set of eigenvectors,
4:08
perpendicular ones. So, having a complete set of eigenvectors means --
4:14
so normal -- so the usual -- maybe I put the -- usually -- usual --
4:20
usual case is that the matrix A we can write in terms of its
4:26
eigenvalue matrix and its eigenvector matrix this way,
4:31
right? We can do that in the usual case, but now what's special when the matrix is symmetric?
4:42
So this is the usual case, and now let me go to the symmetric case.
4:48
So in the symmetric case, A, this --
4:55
this should become somehow a little special. Well, the lambdas on the diagonal are still on the
5:03
diagonal. They're -- they're real, but that's where they are. What about the eigenvector matrix?
5:12
So what can I do now special about the eigenvector matrix when -- when the A itself is symmetric,
5:18
that says something good about the eigenvector matrix, so what is this -- what does this lead to?
5:25
This -- these perpendicular eigenvectors, I can not only --
5:31
I can not only guarantee they're perpendicular, I could also make them unit vectors, no problem,
5:36
just s- scale their length to one. So what do I have? I have orthonormal eigenvectors.
5:45
And what does that tell me about the eigenvector matrix?
5:55
What -- what letter should I now use in place of S -- I've got -- those two equations are identical,1 remember S has
6:02
the eigenvectors in its columns, but now those columns are orthonormal, so the right letter to use is Q.
6:11
So that's where -- so we've got the letter all set up, book. so this should be Q lambda Q inverse.
6:19
Q standing in our minds always for this matrix -- in this case
6:25
it's square, it's -- so these are the Okay. columns of Q, of course.
6:35
And one more thing. What's Q inverse?
6:43
For a matrix that has these orthonormal columns, So I took the dot product -- ye, somehow, it didn't -- I we know that the inverse is the same as the transpose.
6:52
So here is the beautiful -- there is the -- the great haven't learned anything.
6:59
description, the factorization of a symmetric matrix. And this is, like, one of the famous theorems
7:06
of linear algebra, that if I have a symmetric matrix, it can be factored in this form.
7:14
An orthogonal matrix times diagonal times the transpose of that orthogonal matrix.
7:20
And, of course, everybody immediately says yes, and if this is possible, then that's
7:28
clearly symmetric, right? That -- take -- we've looked at products of three guys like
7:33
that and taken their transpose and we got it back again. So do you -- do you see the beauty of this --
7:42
of this factorization, then? It -- it completely displays the eigenvalues and eigenvectors
7:49
the symmetry of the -- of the whole thing, because -- that product, Q times lambda times Q transpose,
7:56
if I transpose it, it -- this comes in this position and we
8:02
get that matrix back again. So that's -- in mathematics, that's called the spectral
8:08
Spectrum is the set of eigenvalues of a matrix. theorem. Spec- it somehow comes from the idea of the spectrum of light
8:17
as a combination of pure things -- where our matrix is broken down into pure eigenvalues
8:28
and eigenvectors -- in mechanics it's often called the principle axis theorem.
8:35
It's very useful. It means that if you have -- we'll see it geometrically.
8:42
It means that if I have some material -- if I look at the right axis, it becomes diagonal, it becomes --
8:49
the -- the I- I've done something dumb, because I've got the -- I should've taken the dot product of this guy here with -- that's directions don't couple together.
8:55
Okay. So that's -- that -- that's what to remember from -- from this lecture.
9:00
Now, I would like to say why are the eigenvalues real?
9:06
Can I do that? So -- so -- because that -- something useful comes out. So I'll just come back -- come to that question why real
9:16
eigenvalues?
9:22
Okay. So I have to start from the only thing we know, Ax equal lambda x.
9:28
9:33
Okay. But as far as I know at this moment, lambda could be complex.
9:40
I'm going to prove it's not -- and x could be complex. In fact, for the moment, even A could be --
9:48
we could even think, well, what happens if A is complex? Well, one thing we can always do -- this is --
9:53
this is like always -- always okay -- I can -- if I have an equation, I can take the complex
10:03
conjugate of everything. That's -- no -- no -- so A conjugate x conjugate equal
10:09
lambda conjugate x conjugate, it just means that everywhere over
10:16
here that there was a -- an equals x bar transpose lambda bar x bar. i, then here I changed it to a-i.
10:21
That's -- that -- you know that that step -- that conjugate business, that a+ib,
10:28
if I conjugate it it's a-ib. That's the meaning of conjugate -- and products behave right,
10:36
I can conjugate every factor. So I haven't done anything yet except to say what would be
10:43
true if, x -- in any case, even if x and lambda were complex.
10:53
Of course, our -- we're speaking about real matrixes A, so I can take that out.
10:58
Actually, this already tells me something about real matrixes.
11:04
I haven't used any assumption of A -- A transpose yet. Symmetry is waiting in the wings to be used.
11:12
This tells me that if a real matrix has an eigenvalue lambda
11:18
what I was going to do. and an eigenvector x, it also has -- another of its eigenvalues is lambda bar
11:25
with eigenvector x bar. Real matrixes, the eigenvalues come in lambda, lambda bar --
11:32
the complex eigenvalues come in lambda and lambda bar pairs. But, of course, I'm aiming to show
11:39
that they're not complex at all, here, by getting symmetry in. So how I going to use symmetry?
11:46
I'm going to transpose this equation to x bar
11:51
transpose A transpose equals x bar transpose lambda bar.
11:59
That's just a number, so I don't mind wear I put that number. This is -- this is --
12:05
this is a -- then again okay. Ax equals lambda x bar transpose x, right? But now I'm ready to use symmetry.
12:14
I'm ready -- so this was all just mechanics. Now -- now comes the moment to say, okay,
12:22
if the matrix is this from the right with x bar, I get x bar transpose Ax bar symmetric, then this A transpose is the same as A.
12:29
You see, at that moment I used the assumption. Now let me finish the discussion.
12:34
Here -- here's the way I finish. I look at this original equation and I take the inner product.
12:42
I multiply both sides by -- oh, maybe I'll do it with this one.
12:49
I take -- I multiply both sides by x bar transpose.
12:55
x bar transpose Ax bar equals lambda bar x bar transpose x bar.
13:03
Okay, fine. All right, now what's the other one?
13:12
Oh, for the other one I'll probably use this guy. A- I happy about this?
13:20
No. For some reason I'm not. I'm -- I want to --
13:26
if I take the inner product of Okay.
14:04
14:24
So that -- that was -- that's fine. That comes directly from that, multiplying both sides by x bar
14:30
transpose, but now I'd like to get -- why do I have x bars over there?
14:38
Oh, yes. Forget this. Okay.
14:43
On this one -- right. On this one, I took it like that, I multiply on the right
14:49
by x. That's the idea.
14:54
Okay. Now why I happier with this situation now?
15:02
A proof is coming here. Because I compare this guy with this one.
15:10
And they have the same left hand side. So they have the same right hand side. So comparing those two, can --
15:16
I'll raise the board to do this comparison -- this thing, lambda x bar transpose x
15:25
is equal to lambda bar x bar transpose x.
15:32
Okay. And the conclusion I'm going to reach --
15:38
I -- I on the right track here?
15:43
The conclusion I'm going to reach is lambda equal lambda bar.
15:52
I would have to track down the other possibility that this -- this thing is zero, but let me --
15:58
oh -- oh, yes, that's important. It's not zero. So once I know that this isn't zero, I just cancel it
16:08
and I learn that lambda equals lambda bar. And so what can you -- do you --
16:14
have you got the reasoning altogether?
16:20
What does this tell us? Lambda's an eigenvalue of this symmetric matrix.
16:27
We've just proved that it equaled lambda bar, so we have just proved that lambda is real,
16:34
right? If, if a number is equal to its own complex conjugate,
16:39
then there's no imaginary part at all. The number is real. So lambda is real.
16:45
Good. Good.
16:51
Now, what -- but it depended on this little expression, on knowing that that wasn't zero,
16:59
so that I could cancel it out -- so can we just take a second on that one?
17:05
Because it's an important quantity. x bar transpose x.
17:11
Okay, now remember, as far as we know, x is complex.
17:18
So this is -- here -- x is complex, x has these components, x1,
17:25
x2 down to xn. And x bar transpose, well, it's transposed and it's conjugated,
17:35
so that's x1 conjugated x2 conjugated up to xn conjugated.
17:42
I'm -- I'm -- I'm really reminding you of crucial facts about complex numbers that are going
17:48
to come into the next lecture as well as this one. So w- what can you tell me about that product --
17:57
I -- I guess what I'm trying to say is, if I had a complex vector, this would be the quantity I would
18:05
-- I would like. This is the quantity I like. I would take the vector times its transpose -- now what --
18:13
what happens usually if I take a vector -- a -- a -- x transpose x? I mean, that's a quantity we see all the time, x transpose x.
18:22
That's the length of x squared, right? That's this positive length squared, it's Pythagoras,
18:28
it's x1 squared plus x2 squared and so on. Now our vector's complex, and you see the effect?
18:36
I'm conjugating one of these guys. So now when I do this multiplication,
18:41
I have x1 bar times x1 and x2 bar times x2 and so on.
18:49
So this is an -- this is sum a+ib. And this is sum a-ib.
18:57
I mean, what's the point here? What's the point -- when I multiply a number by its
19:05
conjugate, a complex number by its conjugate, what do I get?
19:11
I get a n- the -- the imaginary part is gone. When I multiply a+ib by its conjugate, what's --
19:20
what's the result of that -- of each of those separate little multiplications? There's an a squared and -- and what -- how many -- what's --
19:29
b squared comes in with a plus or a minus? A plus.
19:35
i times minus i is a plus b squared. And what about the imaginary part?
19:41
Gone, right?
19:46
An iab and a minus iab. So this -- this is the right thing to do.
19:52
If you want a decent answer, then multiply numbers
20:00
by their conjugates. Multiply vectors by the conjugates of x transpose.
20:08
So this quantity is positive, this quantity is positive -- the whole thing is positive except for the zero vector
20:17
and that allows me to know that this is a positive number,
20:23
which I safely cancel out and I reach the conclusion. So actually, in this discussion here, I've done two things.
20:33
If I reached the conclusion that lambda's real, which I wanted to do.
20:38
But at the same time, we sort of saw what to do if things were complex.
20:43
If a vector is complex, then it's x bar transpose x, this is its length squared.
20:55
And as I said, the next lecture Monday, we'll -- we'll repeat that this is the right thing and then do
21:03
the right thing for matrixes and all other -- all other, complex possibilities.
21:10
Okay. But the main point, then, is that the eigenvalues
21:17
of a symmetric matrix, it just -- do you -- do -- where did we use symmetry, by the way?
21:23
We used it here, right? Let -- can I just -- let -- suppose A was a complex.
21:31
Suppose A had been a complex number. Could -- could I have made all this work?
21:37
If A was a complex number -- complex matrix, then here I should have written A bar.
21:45
I erased the bar because I assumed A was real. But now let's suppose for a moment it's not.
21:50
Then when I took this step, what should I have? What did I do on that step?
21:56
I transposed. So I should have A bar transpose.
22:03
In the symmetric case, that was A, and that's what made everything work, right?
22:08
This -- this led immediately to that. This one led immediately to this when the matrix was real,
22:17
so that didn't matter, and it was symmetric, so that didn't matter. Then I got A.
22:23
But -- so now I just get to ask you. Suppose the matrix had been complex.
22:30
What's the right equivalent of sym- symmetry?
22:38
So the good matrix -- so here, let me say -- good matrixes -- by good I mean real lambdas and perpendicular
22:52
x-s. And tell me now, which matrixes are good?
23:02
If they're -- If they're real matrixes, the good ones
23:08
are symmetric, because then everything went through. The -- so the good -- I'm saying now what's good.
23:15
This is -- this is -- these are the good matrixes. They have real eigenvalues, perpendicular eigenvectors --
23:21
good means A equal A transpose if real.
23:27
Then -- then that was what -- our proof worked. But if A is complex, all -- our proof will still work provided
23:35
A bar transpose is A. Do you see what I'm saying?
23:41
I'm saying if we have complex matrixes and we want to say are
23:47
they -- are they as good as symmetric matrixes, then we should not only transpose the thing,
23:56
but conjugate it. Those are good matrixes. And of course, the most important
24:03
s- the most important case is when they're real, this part doesn't matter and I just have
24:09
A equal A transpose symmetric. Do you -- I -- I'll just repeat that.
24:15
The good matrixes, if complex, are these.
24:20
If real, that doesn't make any difference so I'm just saying symmetric.
24:25
And of course, 99% of examples and applications to the matrixes are real and we don't have that
24:34
and then symmetric is the key property. Okay.
24:40
So that -- that's, these main facts and now let me just --
24:48
let me just -- so that's this x bar transpose x, okay.
24:53
So I'll just, write it once more in this form.
24:59
So perpendicular orthonormal eigenvectors, real eigenvalues,
25:05
transposes of orthonormal eigenvectors. That's the symmetric case, A equal A transpose.
25:13
Okay. Good. Actually, I'll even take one more step here.
25:23
Suppose -- I -- I can break this down to show you really
25:29
what that says about a symmetric matrix. I can break that down.
25:35
Let me here -- here go these eigenvectors. I -- here go these eigenvalues, lambda one, lambda two and so
25:46
on. Here go these eigenvectors transposed.
25:54
And what happens if I actually do out that multiplication? Do you see what will happen?
26:03
There's lambda one times q1 transpose. So the first row here is just lambda one q1 transpose.
26:11
If I multiply column times row -- you remember I could do that?
26:17
When I multiply matrixes, I can multiply columns times rows?
26:24
So when I do that, I get lambda one and then the column and then the row and then
26:31
lambda two and then the column and the row.
26:41
Every symmetric matrix breaks up into these pieces.
26:46
So these pieces have real lambdas and they have these
26:54
Eigen -- these orthonormal eigenvectors.
27:00
And, maybe you even could tell me what kind of a matrix have I got there?
27:07
Suppose I take a unit vector times its transpose?
27:12
So column times row, I'm getting a matrix. That's a matrix with a special name.
27:22
What's it's -- what kind of a matrix is it? We've seen those matrixes, now, in chapter four.
27:27
It's -- is A A transpose with a unit vector, so I don't have to divide by A transpose A.
27:36
That matrix is a projection matrix. That's a projection matrix.
27:41
It's symmetric and if I square it there'll be another -- there'll be a q1 transpose q1, which is one.
27:50
So I'll get that matrix back again. Every -- so every symmetric matrix --
27:57
every symmetric matrix is a combination of --
28:06
of mutually perpendicular -- so perpendicular projection
28:14
matrixes. Projection matrixes.
28:20
Okay. That's another way that people like to think of the spectral theorem,
28:27
that every symmetric matrix can be broken up that way. That -- I guess at this moment --
28:35
first I haven't done an example. I could create a symmetric matrix, check that it's --
28:41
find its eigenvalues, they would come out real, find its eigenvectors, they would come out perpendicular
28:47
and you would see it in numbers, but maybe I'll leave it here for the moment in letters.
28:54
Oh, I -- maybe I will do it with numbers, for this reason. Because there's one more remarkable fact.
29:02
Can I just put this further great fact about symmetric matrixes on the board?
29:07
When I have symmetric matrixes, I
29:13
know their eigenvalues are So then I can get interested in the question are they positive real. or negative?
29:22
And you remember why that's important. For differential equations, that decides between instability
29:27
and stability. So I'm -- after I know they're real, then the next question is are they positive,
29:34
are they negative? And I hate to have to compute those eigenvalues to answer
29:44
that question, right? Because computing the eigenvalues of a symmetric matrix of order let's say 50 --
29:51
compute its 50 eigenvalues -- is a job.
29:58
I mean, by pencil and paper it's a lifetime's job.
30:04
I mean, which -- and in fact, a few years ago -- well, say,
30:11
20 years ago, or 30, nobody really knew how to do it.
30:17
I mean, so, like, science was stuck on this problem. If you have a matrix of order 50 or 100,
30:24
how do you find its eigenvalues? Numerically, now, I'm just saying, because pencil and paper is -- we're going to run out of time
30:34
or paper or something before we get it. Well -- and you might think, okay,
30:41
get Matlab to compute the determinant of lambda minus A,
30:47
A minus lambda I, this polynomial of 50th degree, and then find the roots.
30:53
Matlab will do it, but it will complain,
30:59
because it's a very bad way to find the eigenvalues.
31:04
I'm sorry to be saying this, because it's the way I taught you to do it, right?
31:10
I taught you to find the eigenvalues by doing that determinant and taking the roots of that polynomial.
31:16
But now I'm saying, okay, I really meant that for two by twos and three by threes but I
31:21
didn't mean you to do it on a 50 by 50 and you're not too unhappy, probably,
31:27
because you didn't want to do it. But -- good, because it would be a very unstable way --
31:36
the 50 answers that would come out would be highly unreliable. So, new ways are -- are much better to find those 50
31:45
eigenvalues. That's a -- that's a part of numerical linear algebra. But here's the remarkable fact --
31:54
that Matlab would quite happily find the 50 pivots, right?
32:00
Now the pivots are not the same as the eigenvalues. But here's the great thing.
32:06
If I had a real matrix, I could find those 50 pivots and I could see maybe 28 of them are positive
32:14
and 22 are negative pivots. And I can compute those safely and quickly.
32:20
And the great fact is that 28 of the eigenvalues would be positive and 22 would be negative --
32:27
that the signs of the pivots -- so this is, like -- I hope you think this -- this is kind of a nice thing,
32:34
that the signs of the pivots --
32:39
for symmetric, I'm always talking about symmetric matrixes -- so I'm really, like, trying to convince you
32:45
that symmetric matrixes are better than the rest. So the signs of the pivots are same as the signs
32:58
of the eigenvalues. The same number.
33:04
The number of pivots greater than zero,
33:10
the number of positive pivots is equal to the number
33:16
of positive eigenvalues.
33:21
So that, actually, is a very useful -- that gives you a g- a good start on a decent way to compute eigenvalues,
33:31
because you can narrow them down, you can find out how many are positive, how many are negative.
33:37
Then you could shift the matrix by seven times the identity.
33:42
That would shift all the eigenvalues by seven. Then you could take the pivots of that matrix
33:48
and you would know how many eigenvalues of the original were above seven and
33:53
below seven. So this -- this neat little theorem, that,
33:59
symmetric matrixes have this connection between the --
34:05
nobody's mixing up and thinking the pivots are the eigenvalues --
34:10
I mean, the only thing I can think of is the product of the pivots equals
34:15
the product of the eigenvalues, why is that? So if I asked you for the reason on that,
34:21
why is the product of the pivots for a symmetric matrix the same as the product of the eigenvalues?
34:27
Because they both equal the determinant.
34:34
Right. The product of the pivots gives the determinant if no row exchanges, the product of the eigenvalues
34:40
always gives the determinant. So -- so the products -- but that doesn't tell you anything
34:47
about the 50 individual ones, which this does. Okay. So that's -- those are essential facts about symmetric matrixes.
34:57
Okay. Now I -- I said in the -- in the lecture description that I
35:06
would take the last minutes to start on positive definite
35:13
matrixes, because we're right there, we're ready to say what's a positive definite matrix?
35:22
35:31
It's symmetric, first of all. On -- always I will mean symmetric.
35:37
So this is the -- this is the next section of the book.
35:43
It's about this -- if symmetric matrixes are good, which was, like,
35:50
the point of my lecture so far, then positive, definite matrixes are --
35:57
a subclass that are excellent, okay.
36:03
Just the greatest. so what are they? They're matrixes -- they're symmetric matrixes,
36:10
so all their eigenvalues are real. You can guess what they are. These are symmetric matrixes with all --
36:20
the eigenvalues are --
36:25
okay, tell me what to write.
36:31
What -- well, it -- it's hinted, of course, by the name for these things. All the eigenvalues are positive.
36:39
Okay.
36:45
Tell me about the pivots. We can check the eigenvalues or we can check the pivots.
36:50
All the pivots are what?
36:58
And then I'll -- then I'll finally give an example. I feel awful that I have got to this point in the lecture
37:04
and I haven't given you a single example. So let me give you one. Five three two two.
37:13
That's symmetric, fine. It's eigenvalues are real, for sure.
37:21
But more than that, I know the signs of those eigenvalues.
37:27
And also I know the signs of those pivots, so what's the deal with the pivots?
37:34
The Ei- if the eigenvalues are all positive and if this little
37:39
fact is true that the pivots and eigenvalues have the same signs, then this must be true -- all the pivots are positive.
37:48
And that's the good way to test.
37:54
This is the good test, because I can -- what are the pivots for that matrix? The pivots for that matrix are five.
38:02
So pivots are five and what's the second pivot?
38:08
Have we, like, noticed the formula for the second pivot in a matrix?
38:14
It doesn't necessarily -- you know,
38:19
it may come out a fraction for sure, but what is that fraction? Can you tell me?
38:25
Well, here, the product of the pivots is the determinant. What's the determinant of this matrix?
38:31
Eleven? So the second pivot must be eleven over five,
38:41
so that the product is eleven. They're both positive.
38:47
Then I know that the eigenvalues of that matrix are both positive. What are the eigenvalues?
38:53
Well, I've got to take the roots of -- you know, do I put in a minus lambda? You mentally do this -- lambda squared minus how many lambdas?
39:03
Eight? Right. Five and three, the trace comes in there, plus what number comes here?
39:11
The determinant, the eleven, so I set that to zero.
39:17
So the eigenvalues are -- let's see, half of that is four, look at that positive number,
39:24
plus or minus the square root of sixteen minus eleven, I think five.
39:29
The eigenvalues -- well, two by two they're not so terrible,
39:35
but they're not so perfect. Pivots are really simple.
39:44
And this is a -- this is the family of matrixes that you really want in differential equations,
39:51
because you know the signs of the eigenvalues, so you know the stability or not.
39:58
Okay. There's one other related fact I can pop in here in --
40:03
in the time available for positive definite matrixes.
40:10
The related fact is to ask you about determinants. So what's the determinant?
40:15
40:24
What can you tell me if I -- remember, positive definite means all eigenvalues are positive,
40:32
all pivots are positive, so what can you tell me about the determinant? It's positive, too.
40:40
But somehow that -- that's not quite enough. Here -- here's a matrix minus one minus three,
40:50
what's the determinant of that guy? It's positive, right?
40:55
Is this a positive, definite matrix? Are the pivots -- what are the pivots? Well, negative.
41:02
What are the eigenvalues? Well, they're also the same. So somehow I don't just want the determinant of the whole
41:12
matrix. Here is eleven, that's great. Here the determinant of the whole matrix is three, that's positive.
41:20
I also -- I've got to check, like, little sub-determinants,
41:26
say maybe coming down from the left. So the one by one and the two by two have to be positive.
41:32
So there -- that's where I get the all. All -- can I call them sub-determinants --
41:41
are -- see, I have to -- I need to make the thing plural. I need to test n things, not just the big determinant.
41:51
All sub-determinants are positive. Then I'm okay.
41:58
Then I'm okay. This passes the test.
42:03
Five is positive and eleven is positive. This fails the test because that minus one there is negative.
42:12
And then the big determinant is positive three. So t- this --
42:18
these -- this fact -- you see that actually the course, like, coming together.
42:24
And that's really my point now. In the next -- in this lecture and particularly next Wednesday
42:33
and Friday, the course comes together. These pivots that we met in the first week,
42:41
these determinants that we met in the middle of the course, these eigenvalues that we met most recently --
42:50
all matrixes are square here, so coming together for square
42:56
matrixes means these three pieces come together and they come together in that beautiful fact, that if --
43:05
that all the -- that if I have one of these, I have the others. That if I -- but for symmetric matrixes.
43:12
So that -- this will be the positive definite section
43:17
and then the real climax of the course is to make everything come together for n by n matrixes,
43:27
not necessarily symmetric -- bring everything together there and that
43:33
will be the final thing. Okay. So have a great weekend and don't
43:38
forget symmetric matrixes. Thanks. 