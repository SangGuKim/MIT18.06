https://www.youtube.com/watch?v=M0Sa8fLOajA

0:00
0:08
Okay. This is a lecture where complex numbers come in. It's a -- complex numbers have slipped into this course
0:17
because even a real matrix can have complex eigenvalues.
0:23
So we met complex numbers there as the eigenvalues and complex eigenvectors.
0:30
And we -- or -- this is probably the last --
0:36
we have a lot of other things to do about eigenvalues and eigenvectors. And that will be mostly real.
0:43
But at one point somewhere, we have to see what you do when the numbers become complex numbers.
0:51
What happens when the vectors are complex, when the matrixes are complex, when the --
0:57
what's the inner product of two, the dot product of two complex vectors --
1:03
we just have to make the change, just see -- what is the change when numbers become complex?
1:09
Then, can I tell you about the most important example
1:14
of complex matrixes? It comes in the Fourier matrix.
1:20
So the Fourier matrix, which I'll describe, is a complex matrix. It's certainly the most important complex matrix.
1:28
It's the matrix that we need in Fourier transform.
1:34
And the -- really, the special thing that I want to tell you about is what's called the fast Fourier transform,
1:42
and everybody refers to it as the FFT and it's in all
1:48
computer and it's used -- it's being used as we speak in a thousand places,
1:54
because it has, like, transformed whole industries
1:59
to be able to do the Fourier transform fast, which means multiplying -- how do I multiply fast by that
2:09
matrix -- by that n by n matrix? Normally, multiplications by an n by n matrix --
2:16
would normally be n squared multiplications,
2:21
because I've got n squared entries and none of them is zero. This is a full matrix.
2:27
And it's a matrix with orthogonal columns. I mean, it's just, like, the best matrix.
2:34
And this fast Fourier transform idea reduces this n squared, which was slowing up
2:43
the calculation of Fourier transforms down to n log(n).
2:51
n log(n), log to the base two, actually. And it's this -- when that hit --
2:57
when that possibility hit, it made a big difference. Everybody realized gradually what, --
3:08
that this simple idea -- you'll see it's just a simple matrix factorization -- but it changed everything.
3:15
Okay. So I want to talk about complex vectors and matrixes in general, recap a little bit from last time,
3:22
and the Fourier matrix in particular. Okay.
3:28
So what's the deal? All right. The main point is, what about length?
3:36
I'm given a vector, I have a vector x. Or let me call it z as a reminder that it's complex,
3:44
for the moment. But I can -- later I'll call the components x. They'll be complex numbers.
3:50
But it's a vector -- z1, z2 down to zn.
3:56
So the only novelty is it's not in R^n anymore.
4:01
It's in complex n dimensional space. Each of those numbers is a complex number.
4:07
So this z,z1 is in C^n, n dimensional complex space
4:14
instead of R^n. So just a different letter there, but now the point
4:20
about its length is what? The point about its length is that z transpose z is no good.
4:29
4:34
z transpose z -- if I just put down z transpose here, it would be z1, z2, to zn.
4:42
Doing that multiplication doesn't give me
4:47
the right thing. W-Why not? Because the length squared should be positive.
4:57
And if I multiply -- suppose this is, like, 1 and i. What's the length of the vector with components 1 and i?
5:06
What if I do this, so n is just two. I'm in C^2, two dimensional space, complex space with the vector whose components are 1 and i.
5:16
All right. So if I took one times one and i times i and added,
5:23
z transpose z would be zero. But I don't -- that vector is not --
5:29
doesn't have length zero -- the vector with the components 1 and i -- this multiplication -- what I really want is z1 conjugate z1.
5:41
You remember that z1 conjugate z1 is -- so you see that first step will be z1 conjugate z1,
5:50
which is the magnitude of z1 squared, which is what I want.
5:56
That's, like, three squared or five squared. Now, if it's -- if z1 is i, then I multiplied by minus i gives
6:11
one plus one, so the component of length -- the component i,
6:19
its modulus squared is plus one. That's great. So what I want to do then is do that --
6:25
I want z1 bar z1, z2 bar z2, zn bar zn.
6:30
And remember that -- you remember this complex conjugate. So -- so there's the point.
6:38
Now I can erase the no good and put is good, because that now gives the answer zero for the zero
6:48
vector, of course, but it gives a positive length squared for any other vector.
6:55
So it's a -- it's the right definition of length, and essentially the message is that we're always going to be
7:04
taking -- when we transpose, we also take complex conjugate.
7:09
So let's -- let's find the length of one -- so the vector one i, that's z, that's that vector z.
7:19
Now I take the conjugate of one is one, the conjugate of i is minus i. I take this vector, I get one plus one --
7:29
I get two. So that's a vector and that's a vector of length -- square root
7:36
of two. Square root of two is the length and not the zero that we would have got from one minus i squared.
7:43
Okay. So the message really is whenever we transpose,
7:49
we also take conjugates. So here's a symbol -- one symbol to do both.
7:57
So that symbol H, it stands for a guy named Hermite,
8:03
who didn't actually pronounce the H, but let's pronounce it --
8:09
so I would call that z Hermitian z. I'll -- let me write that word, Herm- so his name was Hermite,
8:20
and then we make it into an adjective, Hermitian.
8:27
So z Hermitian z. z H z. Okay.
8:34
So, that's the -- that's the, length squared.
8:40
Now what's the inner product? Well, it should match. The inner product of two vectors --
8:49
so inner product is no longer --
8:56
used to be y transpose x. That's for real vectors.
9:01
For complex vectors, whenever we transpose, we also take the conjugate.
9:07
So it's y Hermitian x. Of course it's not real anymore, usually.
9:14
That -- the inner product will usually be complex number.
9:19
But if y and x are the same, if they're the same z, then we have z -- z H z, we have the length squared,
9:29
and that's what we want, the inner product of a vector with itself should be its length squared.
9:35
So this is, like, forced on us because this is forced on us. So -- so this z -- this --
9:43
everybody's picking up what this equals. This is z1 squared plus zn squared.
9:52
That's the length squared. And that's the inner product that we have to go with. So it could be a complex number now.
10:00
One more change. Well, two more changes. We've got to change the idea of a symmetric matrix.
10:08
So I'll just recap on symmetric matrixes. Symmetric means A transpose equals A, but not --
10:21
no good if A is complex.
10:30
So what do we instead --
10:37
that applies perfectly to real matrixes. But now if my matrixes were complex,
10:42
I want to take the transpose and the conjugate to equal A.
10:51
So there's -- that's the -- the right complex version of symmetry. The com- the symmetry now means when
10:59
I transpose it, flip across the diagonal and take conjugates. So, for example -- here would be an example.
11:07
On the diagonal, it had better be real, because when I flip it, the diagonal is still there and it
11:15
has to -- and then when I take the complex conjugate it has to be still there, so it better be a real number,
11:21
let me say two and five. What about entries off the diagonal?
11:27
If this entry is, say, three plus i, then this entry had better be --
11:36
because I want whatever this -- when I transpose, it'll show up here and i conjugate.
11:42
So I need three minus I there. So there's a matrix with --
11:51
that corresponds to symmetry, but it's complex. And those matrixes are called Hermitian matrixes.
12:01
Hermitian matrixes. A H equals A. Fine.
12:08
Okay, that's -- and those matrixes have real eigenvalues and they have perpendicular eigenvectors.
12:15
What does perpendicular mean? Perpendicular means the inner product --
12:21
so let's go on to perpendicular.
12:28
Well, when I had perpendicular vectors, for example, they were like q1, q2 up to qn.
12:37
That's my -- q is my letter that I use for perpendicular. Actually, I usually --
12:43
I also mean unit length. So those are perpendicular unit vectors.
12:49
But now what does -- so it's a -- orthonormal basis, I'll still use those words,
12:54
but how do I compute perpendicular? How do I check perpendicular?
13:00
This means that the inner product of qi with qj --
13:07
but now I not only transpose, I must conjugate, right, to get zero if i is not j and one if i is j.
13:20
So it's a unit vector, meaning unit length, orthogonal --
13:25
all the angles are right angles, but these are angles in complex n dimensional space.
13:33
So it's q1, q on- qi bar transpose.
13:39
Or, for short, qi H qj. So it will still be true -- so let me --
13:46
again I'll create a matrix out of those guys. The matrix will have these q-s in its columns, q2 to qn.
13:56
And I want to turn that into matrix language,
14:03
just like before. What does that mean? That means I want all these inner products,
14:08
so I take these columns of Q, multiply by their rows --
14:13
so it was -- it used to be Q -- it used to be Q transpose Q equals I, right?
14:20
This was an orthogonal matrix.
14:27
But what's changed? These are now complex vectors.
14:33
Their inner products are -- involve conjugating the first factor.
14:39
So it's not -- it's the conjugate of Q transpose. It's Q bar transpose Q.
14:47
Q H. So can I call this -- let me call it Q H Q, which is I.
14:55
So that's our new -- you -- you see I'm just translating, and the -- the book h- on one page gives a little dictionary of the right
15:07
words in the real case, R^n, and the corresponding words
15:12
in the complex case for the vector space C^n. Of course, C^n is a vector space,
15:19
the numbers we multiply are now complex numbers -- we're just moving into complex n dimensional space.
15:27
Okay. Now -- actually, I have to say we changed the word symmet-
15:34
symmetric to Hermitian for those matrixes. People also change this word orthogonal into another word
15:43
that happens to be unitary, as a word that applies --
15:52
that signals that we might be dealing with a complex matrix here. So what's a unitary matrix?
15:59
It's a -- it's just like an orthogonal matrix. It's a square, n by n matrix with orthonormal columns,
16:10
perpendicular columns, unit vectors -- unit vectors computed by -- and perpendicularity computed
16:20
by remembering that there's a conjugate as well as a transpose.
16:25
Okay. So those are the words. Now I'm ready to get into the substance of the lecture which
16:32
is the most famous complex matrix, which happens
16:38
to be one of these guys. It has orthogonal columns, and it's named after Fourier
16:50
because it comes into the Fourier transform, so it's the matrix that's all around us.
16:55
Okay. Let me tell you what it is first of all in the n by n case.
17:05
Then often I'll let n be four because four is a good size to work with.
17:11
But here's the n by n Fourier matrix.
17:17
Its first column is the vector of ones.
17:24
It's n by n, of course. Its second column is the powers, the --
17:30
actually, better if I move from the math department to EE for this one half hour and then, please,
17:39
let me move back again. Okay. What's the difference between those two departments?
17:46
It's just math starts counting with one and electrical engineers start counting at zero.
17:56
Actually, they're probably right. So anyway, we'll give them -- humor them. So this is really the zeroes column.
18:05
And the first column up to the n-1, that's the one inconvenient spot in electrical engineering.
18:11
All these expressions start at zero, no problem, but they end at n-1. Well, that's -- that's the difficulty of Course 6.
18:24
So what's -- they're the powers of a number that I'm going to call W -- W squared, W cubed, W to the -- now what is the W here?
18:34
What's the power? This was the zeroes power, first power, second power, this will be n minus first power.
18:43
That's the column. What's the next column? It's the powers of W squared, W to the fourth, W to the sixth,
18:51
W to the two n-1. And then more columns and more columns
18:58
and more columns and what's the last column? It's the powers of --
19:07
let's see. We -- actually, if we look around rows, w- this matrix is symmetric.
19:12
It's symmetric in the old not quite perfect way, not perfect because these numbers are complex.
19:21
And so it's -- that first row is all ones. One W W squared up to W to the n-1.
19:28
That's the last column is the powers of W to the n-1,
19:34
so this guy matches that, and finally we get W to something here.
19:43
I guess we could actually figure out what that something is. What are the entries of this matrix?
19:48
The i j entry of this matrix are --
19:57
I going to -- are you going to allow me to let i go from zero to n minus one? So i and g go from zero to n-1.
20:07
So the one -- the zero zero entry is a one -- it's just this same W guy to the power i times j.
20:15
20:21
Let's see. I'm jumping into formulas here and I have to tell you what W is and then you know everything about this matrix.
20:27
So W is the -- well, shall we finish here?
20:34
What was this -- this is the (n-1) (n-1) entry. This is W to the n-1 squared.
20:41
Everything's looking like a mess here, because we have -- not too bad, because all the entries are powers of W.
20:51
There -- none of them are zero. This is a full matrix. But W is a very special number.
20:59
W is the special number whose n-th power is one.
21:08
In fact -- well, actually, there are n numbers like that. One of them is one, of course.
21:14
But the one we -- the W we want is --
21:19
the angle is two pi over n.
21:28
Is that what I mean? n over two pi. No, two pi over n.
21:34
W is E to the I and the angle is two pi over n.
21:42
Right. Where is this W in the complex plane?
21:47
It's -- it's on the unit circle, right? This is -- it's the cosine of two pi over n plus I times
21:56
the sine of two pi over n. But actually, forget this.
22:02
It's never good to work with the real and imaginary parts,
22:09
the rectangular coordinates, when we're taking powers.
22:14
To take that to the tenth power, we can't see what we're doing. To take this form to the tenth power,
22:19
we see immediately what we're doing. It would be e to the i 20 pi over n.
22:26
So when our matrix is full of powers -- so it's this formula, and where is this on the complex plain?
22:32
Here are the real numbers, here's the imaginary axis, here's the unit circle of radius one,
22:40
and this number is on the unit circle at this angle, which is one n-th of the full way round.
22:48
So if I drew, for example, n equals six, this would be e to the two pi, two pi over six,
22:56
it would be one sixth of the way around, it would be 60 degrees.
23:03
And where is W squared? So I -- my W is e to the two pi I over six in this case,
23:13
in this six by -- for the six by six Fourier transform, it's totally constructed out of this number and its powers.
23:22
So what are its powers? Well, its powers are on the unit circle, right? Because when I square a number, a complex number,
23:32
I square its absolute value, which gives me one again.
23:37
All the powers have -- are on the unit circle. And the -- the angle gets doubled to a hundred
23:43
and twenty, so there's W squared, there's W cubed, there's W to the fourth, there's W to the fifth and there is W
23:50
to the sixth, as we hoped, W to the sixth coming back to one.
23:57
So those are the six -- can I say this on TV?
24:02
The six sixth roots of one, and it's this one, the primitive one we say, the first one, which is W.
24:14
Okay, so what -- let me change -- let me -- I said I would probably switch to n equal four.
24:21
What's W for that? It's the fourth root of one. W to the fourth will be one.
24:28
W will be e to the two pi i over four now.
24:38
What's that? This is e to the i pi over two. This is a quarter of the way around the unit circle,
24:46
and that's exactly i, a quarter of the way around.
24:53
And sure enough, the powers are i, i squared, which is minus one, i cubed, which
25:01
is minus i and finally i to the fourth which is one, right.
25:08
So there's W, W squared, W cubed, W to the fourth -- I'm really ready to write down this Fourier
25:15
matrix for the four by four case, just so we see that clearly.
25:21
Let me do it here. F4 is -- all right, one one one one one one one W -- it's I.
25:34
I squared. That's minus one. i cubed is minus i.
25:41
I'll -- I could write i squared and i cubed. Why don't I, just so we see the pattern for sure. i squared, i cubed, i squared, i cubed, i fourth, i sixth --
25:54
i fourth, i sixth and i ninth. You see the exponents fall in this nice --
26:01
the exponent is the row number times the column number, always starting at zero.
26:08
Okay. And now I can put in those numbers if you like -- one one one one, one i minus one minus i, one minus one,
26:20
one minus one and one minus i minus one i.
26:28
No. Yes. Right.
26:38
What's -- why do I think that matrix is so remarkable?
26:44
It's the four by four matrix that comes into the four point Fourier transform.
26:52
When we want to find the Fourier transform, the four point Fourier transform of a vector with four components,
27:01
we want to multiply by this F4 or we want to multiply by F4 inverse.
27:08
One way we're taking the transform, one way we're taking the inverse transform. Actually, they're so close that it's easy to confuse the two.
27:17
The inverse of this matrix will be a nice matrix also.
27:25
So -- and that's, of course, what makes it -- that -- I guess Fourier knew that.
27:32
He knew the inverse of this matrix. A- as you'll see, it just comes from the fact that the columns
27:39
are orthogonal -- from the fact that the columns are orthogonal, we will quickly figure out what is the inverse.
27:51
What Fourier didn't know -- didn't notice -- I think Gauss noticed it but didn't make a point of it
27:58
and then it turned out to be really important was the fact that this matrix is so special that you
28:04
can break it up into nice pieces with lots of zeroes, factors
28:10
that have lots of zeroes and multiply by it or by its inverse very, very fast.
28:16
Okay. But how did it get into this lecture first?
28:22
Because the columns are orthogonal. Can I just check that the columns of this matrix
28:29
are orthogonal? So the inner product of that column with that column
28:38
is zero. The inner product of column one with column three is zero.
28:48
How about the inner product of two and four?
28:55
Can I take the inner product of column two with column four?
29:01
Or even the inner product of two with three, let's -- let's see, does that --
29:06
let me do two and four.
29:15
Okay. What -- oh, I see, yes, hmm. Hmm. Let's see, I believe that those two columns are orthogonal.
29:30
So let me take their inner product and hope to get zero. Okay, now if you hadn't listened to the first half
29:36
of this lecture, when you took the inner product of that with that, you would have multiplied one by one,
29:42
i by minus i, and that would have given you one,
29:48
minus one by minus one would have given you another one minus I by I would have been minus I squared,
29:55
that's another one. So do I conclude that the inner product of columns --
30:04
I said columns two and four, that's because I forgot those are columns one and three.
30:12
I'm interested in their inner product and I'm hoping it's zero, but it doesn't look like zero.
30:17
Nevertheless, it is zero. Those columns are perpendicular. Why? Because the inner product --
30:25
we conjugate. Do you remember that the -- one of the vectors in the inner product has to get conjugated.
30:31
So when I conjugated, it changes that i to a minus i, changes this to a plus i, changes those --
30:37
that second sine and that fourth sine and I do get zero.
30:44
So those columns are orthogonal. So columns are orthogonal.
30:49
They're not quite orthonormal.
30:55
But I could fix that easily. They -- all those columns have length two.
31:02
Length squared is four, like this -- the four I had there -- this length squared, one plus -- one squared one squared one
31:10
squared one squared is four, square root is two -- so if I really wanted them -- suppose I really wanted to fix
31:17
life perfectly, I could divide by two, and now I have columns that are actually orthonormal.
31:26
31:33
So what? So I can invert right away, right? O- orthonormal columns means -- now I'm keeping this one half
31:41
in here for the moment -- c- means F4 Hermitian, can I use that,
31:48
conjugate transpose times F4 is i.
31:56
So I see what the inverse is. The inverse of F4 is -- it's just like an --
32:01
an orthogonal matrix. The inverse is the transpose -- here the inverse is the conjugate transpose.
32:08
So, fine. That -- that tells me that anything good that I learn
32:15
about F4 I'll know the same -- I'll know a similar fact about its inverse,
32:21
because its inverse is just its conjugate transpose. Okay, now -- so what's good?
32:28
Well, first, the columns are orthogonal. That's a key fact, then.
32:33
That's the thing that makes the inverse easy. But what property is it that leads to the fast Fourier
32:40
transform? So now I'm going to talk, in these last minutes, about the fast Fourier transform.
32:48
What -- here's the idea. F6, our six by six matrix, will c-
32:56
there's a neat connection to F3, half as big.
33:02
There's a connection of F8 to F4. There's a connection of F(64) to F(32).
33:09
Shall I write down what that connection is? What's the connection of F(64) to F(32)?
33:15
So F(64) is a 64 by 64 matrix whose W
33:22
is the 64th root of one. So it's one 64th of the way round in F(64).
33:31
And it -- do- and F(32) is a 32 by 32 matrix. Remember, they're different sizes.
33:37
And the W in that 32 by 32 matrix is the 32nd root of one,
33:45
which is twice as far -- that -- you sh- see that key point -- that's the -- that's how 32 and 64 are connected in the Ws.
33:55
The W for 64 is one 64th of the way -- so all I'm saying is that if I square the W --
34:05
W(64), that's what I'm using for the one over -- the -- W sixty f- this Wn is either the i two pi over n --
34:15
so W(64) is one 64th of the way around it. When I square that, what do I get but W(32)?
34:24
Right? If I square this matrix, I double the angle --
34:31
if I square this number, I double the angle, I get, the --
34:36
the W(32). So somehow there's a little hope here
34:43
to connect F(64) with F(32). And here's the connection.
34:49
Okay. Let me -- let me go back, -- yes, let me -- I'll do it here.
34:58
Here's the connection. F(64). The 64 by 64 Fourier matrix is connected
35:06
to two copies of F(32). Let me leave a little space for the connection.
35:13
So this is 64 by 64. Here's a matrix of that same size,
35:18
because it's got two copies of F(32) and two zero matrixes.
35:25
Those zero matrixes are the key, because when I multiply by this
35:31
matrix, just as it is, regular multiplication, I would take -- need 64 --
35:37
I would -- I would have 64 squared little multiplications to do. But this matrix is half zero.
35:45
Well, of course, the two aren't equal. I'm going to put an equals sign, but there has to be some fix up
35:52
factors -- one there and one there --
35:57
to make it true. The beauty is that these fix up factors will be really --
36:05
almost all zeroes. So that as soon as we get this formula right,
36:11
we've got a great idea for how to get from the sixty- from the 64 squared calculations -- so this original --
36:21
originally we have 64 squared calculations from there, but this one will give us -- this is -- this will --
36:29
we don't need that many -- we only need two times 32 squared,
36:34
because we've got that twice. And -- plus the fix-up.
36:42
So I have to tell you what's in this fix-up matrix.
36:47
The one on the right is actually a permutation matrix, a very simple odds and evens permutation matrix, the --
36:55
ones show up -- I haven't put enough ones, I really need a -- 32 of these guys at -- double space and then --
37:05
you see it's -- it's a permutation matrix.
37:12
What it does -- shall I call it P for permutation matrix?
37:18
So what that P does when it multiplies a vector,
37:23
it takes the odd -- the even numbered components first and then the odds.
37:30
You see this -- this one skipping every time is going to pick out x0, x2, x4, x6 and then below that will come --
37:40
will pick out x1, x3, x5.
37:45
And of course, that can be hard wired in the computer to be instantaneous.
37:52
So that says -- so far, what have we said? We're saying that the 64 by 64 Fourier matrix is really
38:00
separated into -- separate your vector into the odd -- into the even components and the odd components,
38:07
then do a 32 size Fourier transform onto those
38:12
separately, and then put the pieces together again. So the pieces -- putting them together turns them out to be I
38:21
and a diagonal matrix and I and a minus, that same diagonal matrix.
38:29
So the fix-up cost is really the cost of multiplying by D, this diagonal matrix, because there's essentially no cost
38:38
in -- in the I part or in the permutation part, so really it's -- the fix-up cost is essentially because D
38:50
is diagonal -- is 32 multiplications.
38:58
That's the -- there you're seeing -- of course we didn't check the formula or we didn't even say what D is yet, but I will --
39:05
this diagonal matrix D is powers of W -- one W W squared down to W to the 31st.
39:13
39:19
So you see that when I -- to do a multiplication by D, I need to do 32 multiplications.
39:25
There they are. Then -- but the other, the more serious work is to do the F(32)
39:33
twice on the -- separately on the even numbered and odd numbered components, so twice 32 squared.
39:41
So 64 squared is gone now. And that's the new count.
39:47
Okay, great, but what next?
39:54
So that's -- I -- we now have the key idea -- we would have to check the algebra,
40:01
but it's just checking a lot of sums that come out correctly.
40:06
This is right -- the right way to see the fast Fourier transform, or one right way to see it.
40:15
Then you've got to see what's the next idea. The next idea is to break the 32s down.
40:21
Break those 32s down. So we have this factor, and now we have the F(32),
40:27
but that breaks into some guy here --
40:32
F thirty- F six- F(16) -- F(16). Each -- each F(32) is breaking into two copies of F(16),
40:43
and then we have a permutation and then the -- so this is a -- like, this was a 64 size permutation,
40:52
this is a 32 size permutation -- I guess I've got it twice.
40:57
So it's -- I'm -- I'm just using the same idea recursively -- recursion is the key word -- that on each of those F(32)s --
41:05
so here's zero zero -- it's just -- to get F(32) --
41:13
this is the odd even permutations -- so you see, we're -- the combination of those
41:20
permutations, what's it doing? This guy separates into odds -- in -- into evens and odds,
41:28
and then this guy separates the evens into the ones --
41:34
the numbers that are mult- the even evens, which means zero four eight sixteen --
41:41
and even odds, which means two, six, ten, fourteen --
41:48
and then odd evens and odd odds. You see, together these permutations then break it --
41:55
break our vector down into x, even even and three other pieces.
42:02
Those are the four pieces that separately get multiplied by F(16) -- separately fixed up by these Is and Ds and Is and minus Ds --
42:12
so this count is now reduced.
42:19
This count is now -- what's it reduced to? So that's going to be gone, because 32 squared -- that's --
42:28
that's the change I'm making, right? The 32 squared -- w- so -- so it's this that's now reduced.
42:35
So I still have two times it, but now what's 32 squared?
42:40
It's gone in favor of two sixteen squareds plus sixteen.
42:47
That's -- and then the original 32 to fix.
42:54
43:00
Maybe you see what's happening. Even easier than this formula is w- what's -- when I do the recursion more and more times,
43:09
I get simpler and simpler factors in the middle. Eventually I'll be down to two point or one point
43:15
Fourier transforms. But I get more and more factors piling up on the right
43:21
and left. On the right, I'm just getting permutation matrixes. On the left, I'm getting these guys, these Is and Ds,
43:29
so that there was a 32 there and -- each one of these is costing 32.
43:35
Each one of those is costing And how many will there be?
43:42
So you see the 32 for this original fix up, because D had 32 numbers, 32 for this next fix
43:49
up, because D has 16 and 16 more. I keep going.
43:56
So the count in the middle goes down to zip, but these fix up counts are all that I'm left with,
44:04
and how many factors -- how many fix-ups have I got -- log in --
44:10
from 64, one step to 32, one step to 16, one step to eight, four, two and one.
44:17
Six steps. So I have six fix-up -- six fix up factors.
44:23
Finally I get to six times the That's my final count.
44:33
Instead of 64 squared, this is log to the base two of 64 times
44:41
64 -- actually, half of 64. So actually, the final count is n log to the base two of n --
44:51
that's the 32 -- a half.
44:57
So can I put a box around that wonderful, extremely important and satisfying conclusion --
45:07
that the fast Fourier transform multiplies by an n by n matrix,
45:12
but it does it not in n squared steps, but in one half n log n steps.
45:19
And if we just -- complete by doing a count, let's suppose --
45:25
suppose -- a typical case would be two to the
45:31
tenth. Now n squared is bigger than a million.
45:39
So it's a thousand twenty four times a thousand twenty four.
45:47
But what is n -- what is one half -- what is the new count, done the right way?
45:55
It's n -- the thousand twenty four times one half,
46:02
and what's the logarithm? It's ten. So times ten over two.
46:08
So it's five times -- it's five times a thousand twenty four, where this one was a thousand twenty four times a thousand
46:16
twenty four. We've reduced the calculation by a factor of 200 just
46:25
by factoring the matrix properly. This was a thousand times n, we're now down to five times n.
46:34
So we can do 200 Fourier transforms, where before we could do one, and
46:42
in real scientific calculations where Fourier transforms are happening all the time, we're saving
46:48
a factor of in one of the major steps of modern scientific computing.
46:55
So that's the idea of the fast Fourier transform, and you see the whole thing hinged on being a special matrix with orthonormal columns.
47:07
Okay, that's actually it for complex numbers.
47:13
I'm back next time really to -- to real numbers, eigenvalues and eigenvectors
47:22
and the key idea of positive definite matrixes is going to show up.
47:28
What's a positive definite matrix? And it's terrific that this course is going to reach positive definiteness,
47:35
because those are the matrixes that you see the most in applications. Okay, see you next time.
47:41
Thanks.
47:47
