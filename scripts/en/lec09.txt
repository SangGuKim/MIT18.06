https://www.youtube.com/watch?v=yjBerM5jWsc

# Introduction
0:00
0:09
OK, this is linear algebra lecture nine. And this is a key lecture, this is where we get these ideas
0:21
of linear independence, when a bunch of vectors are
0:27
independent -- or dependent, that's the opposite.
0:33
The space they span. A basis for a subspace or a basis for a vector
0:40
space, that's a central idea. And then the dimension of that subspace.
0:47
So this is the day that those words get assigned clear meanings.
0:52
And emphasize that we talk about a bunch of vectors
0:58
being independent. Wouldn't talk about a matrix being independent. A bunch of vectors being independent.
1:05
A bunch of vectors spanning a space. A bunch of vectors being a basis.
1:11
And the dimension is some number. OK, so what are the definitions?
1:17
Can I begin with a fact, a highly important fact, that, I didn't call directly attention to earlier.
1:28
Suppose I have a matrix and I look at Ax equals zero.
1:35
Suppose the matrix has a lot of columns, so that n is bigger than m.
1:43
So I'm looking at n equations -- I mean, sorry, m equations, a small number
1:48
of equations m, and more unknowns. I have more unknowns than equations.
1:55
Let me write that down. More unknowns than equations. More unknown x-s than equations.
2:03
Then the conclusion is that there's
2:15
something in the null space of A, other than just the zero vector.
2:21
The conclusion is there are some non-zero x-s such that Ax is zero.
2:28
There are some special solutions. And why? We know why.
2:33
I mean, it sort of like seems like a reasonable thing, more unknowns than equations, then it seems reasonable
2:42
that we can solve them. But we have a, a clear algorithm which starts with a system
2:49
and does elimination, gets the thing into an echelon
2:55
form with some pivots and pivot columns,
3:01
and possibly some free columns that don't have pivots. And the point is here there will be some free columns.
3:09
The reason, so the reason is there must --
3:14
there will be free variables, at least one.
3:20
That's the reason.
3:27
That we now have this -- a complete, algorithm, a complete systematic way to say,
3:36
OK, we take the system Ax equals zero, we row reduce, we identify the free variables,
3:42
and, since there are n variables and at most m pivots,
3:48
there will be some free variables, at least one, at least n-m in fact, left over.
3:55
And those variables I can assign non-zero values to.
4:00
I don't have to set those to zero. I can take them to be one or whatever I like, and then I can solve for the pivot variables.
4:07
So then it gives me a solution to Ax equals zero. And it's a solution that isn't all zeros.
4:15
So, that's an important point that we'll
4:21
use now in this lecture. So now I want to say what does it mean for a bunch of vectors
# Independence
4:30
to be independent. OK. So this is like the background that we know.
4:36
Now I want to speak about independence.
4:41
OK. Let's see.
4:48
I can give you the abstract definition, and I will,
4:55
but I would also like to give you the direct meaning.
5:03
So the question is, when vectors x1, x2 up to --
5:12
Suppose I have n vectors are independent if.
5:19
Now I have to give you -- or linearly independent --
5:25
I'll often just say and write independent for short. OK.
5:31
I'll give you the full definition. These are just vectors in some vector space.
5:36
I can take combinations of them. The question is, do any combinations give zero?
5:45
If some combination of those vectors gives the zero vector, other than the combination
5:51
of all zeros, then they're dependent. They're independent if no combination gives the zero
6:06
vector -- and then I have, I'll have to put in an except the zero
6:17
combination. So what do I mean by that? No combination gives the zero vector.
6:23
Any combination c1 x1+c2 x2 plus,
6:29
plus cn xn is not zero except for the zero combination.
6:39
This is when all the c-s, all the c-s are zero.
6:45
Then of course. That combination -- I know I'll get zero.
6:52
But the question is, does any other combination give zero? If not, then the vectors are independent.
6:59
If some other combination does give zero, the vectors are dependent. OK.
7:05
Let's just take examples. Suppose I'm in, say, in two dimensional space.
7:14
OK. I give you -- I'd like to first take an example --
7:19
let me take an example where I have a vector and twice that vector.
7:26
So that's two vectors, V and 2V. Are those dependent or independent?
7:33
Those are dependent for sure, right, because there's one vector is twice the other.
7:40
One vector is twice as long as the other, so if the word dependent means anything, these should be dependent.
7:46
And they are. And in fact, I would take two of the first --
7:51
so here's, here is a vector V and the other guy is a vector 2V, that's my --
7:59
so there's a vector V1 and my next vector V2 is 2V1.
8:04
Of course those are dependent, because two of these first vectors minus the second vector is zero.
8:14
That's a combination of these two vectors that gives the zero vector. OK, that was clear.
8:20
Suppose, suppose I have a vector -- here's another example.
8:25
It's easy example. Suppose I have a vector and the other guy is the zero vector.
8:31
Suppose I have a vector V1 and V2 is the zero vector.
8:36
Then are those vectors dependent or independent?
8:43
They're dependent again. You could say, well, this guy is zero times that one.
8:50
This one is some combination of those. But let me write it the other way. Let me say -- what combination, how many V1s and how many V2s
9:01
shall I take to get the zero vector? If, if V1 is like the vector two one and V2 is the zero vector,
9:11
zero zero, then I would like to show that some combination of those gives the zero vector.
9:18
What shall I take? How many V1s shall I take? Zero of them. Yeah, no, take no V1s.
9:25
But how many V2s? Six. OK.
9:31
Or five. Then -- in other words, the point is if the zero
9:38
vector's in there, if the zero -- if one of these vectors is the zero vector,
9:44
independence is dead, right? If one of those vectors is the zero vector then I could always
9:50
take -- include that one and none of the others, and I would get the zero answer, and I would show dependence.
9:59
OK. Now, let me, let me finally draw an example
10:04
where they will be independent. Suppose that's V1 and that's V2.
10:10
Those are surely independent, right? Any combination of V1 and V2, will not
10:19
be zero except, the zero combination. So those would be independent. But now let me, let me stick in a third vector, V3.
10:28
Independent or dependent now, those three vectors?
10:34
So now n is three here. I'm in two dimensional space, whatever, I'm in the plane.
10:42
I have three vectors that I didn't draw so carefully.
10:47
I didn't even tell you what exactly they were. But what's this answer on dependent or independent?
10:55
Dependent. How do I know those are dependent? How do I know that some combination of V1, V2, and V3
11:05
gives me the zero vector? I know because of that.
11:12
That's the key fact that tells me
# Connection
11:20
that three vectors in the plane have to be dependent. Why's that? What's the connection between the dependence of these three
11:28
vectors and that fact? OK. So here's the connection. I take the matrix A that has V1 in its first column, V2
11:42
in its second column, V3 in its third column. So it's got three columns.
11:48
And V1 -- I don't know, that looks like about two one to me. V2 looks like it might be one two.
11:56
V3 looks like it might be maybe two, maybe two and a half,
12:01
minus one.
12:07
OK. Those are my three vectors, and I put them in the columns of A.
12:13
Now that matrix A is two by three. It fits this pattern, that where we know we've got extra
12:22
variables, we know we have some free variables, we know that there's some combination --
12:28
and let me instead of x-s, let me call them c1, c2, and c3 --
12:34
that gives the zero vector. Sorry that my little bit of art got in the way.
12:41
Do you see the point? When I have a matrix, I'm interested
12:48
in whether its columns are dependent or independent.
12:53
The columns are dependent if there is something in the null space. The columns are dependent because this,
13:00
this thing in the null space says that c1 of that plus c2 of that plus c3 of this is zero.
13:09
So in other words, I can go out some V1, out some more V2, back on V3, and end up zero.
13:16
OK. So let -- here I've give the general, abstract definition,
13:25
but let me repeat that definition -- this is like repeat --
13:31
let me call them Vs now.
13:39
V1 up to Vn are the columns of a matrix A.
13:47
In other words, this is telling me that if I'm in m dimensional space,
13:57
like two dimensional space in the example, I can answer the dependence-independence
14:03
question directly by putting those vectors
14:10
in the columns of a matrix. They are independent if the null space of A, of A, is what?
14:30
If I have a bunch of columns in a matrix,
14:36
I'm looking at their combinations, but that's just A times the vector of c-s.
14:44
And these columns will be independent if the null space of A is the zero vector.
14:56
They are dependent if there's something else in there.
15:11
If there's something else in the null space, if A times c
15:17
gives the zero vector for some non-zero vector
15:25
c in the null space. Then they're dependent, because that's
15:30
telling me a combination of the columns gives the zero column. I think you're with be, because we've seen,
15:38
like, lecture after lecture, we're looking at the combinations of the columns and asking,
15:43
do we get zero or don't we? And now we're giving the official name,
# Independent
15:49
dependent if we do, independent if we don't. So I could express this in other words now.
15:57
I could say the rank -- what's the rank in this independent case? The rank r of the, of the matrix,
16:05
in the case of independent columns, is?
16:11
So the columns are independent. So how many pivot columns have I got.
16:18
All n. All the columns would be pivot columns, because free columns are telling me
16:25
that they're a combination of earlier columns. So this would be the case where the rank is n.
16:33
This would be the case where the rank is smaller than n.
16:39
So in this case the rank is n and the null space of A
16:44
is only the zero vector. And no free variables.
16:50
No free variables.
16:56
And this is the case yes free variables.
17:04
If you'll allow me to stretch the English language that far.
17:09
That's the case where we have, a combination
17:16
that gives the zero column. I'm often interested in the case when my vectors are
17:23
popped into a matrix. So the, the definition over there of independence
17:28
didn't talk about any matrix. The vectors didn't have to be vectors in N dimensional space.
17:36
And I want to give you some examples of vectors that aren't what you think
17:41
of immediately as vectors. But most of the time, this is -- the vectors we think of are
17:49
columns. And we can put them in a matrix.
17:54
And then independence or dependence comes back to the null space.
18:01
OK. So that's the idea of independence. Can I just, yeah, let me go on to spanning a
18:13
What does it mean for a bunch of vectors to span a space?
18:19
space. Well, actually, we've seen it already. You remember, if we had a columns in a matrix,
18:28
we took all their combinations and that gave us
18:33
the column space. Those vectors that we started with span that column space.
18:41
So spanning a space means -- so let me move that important stuff right up.
18:48
OK.
18:54
So vectors -- let me call them, say, V1 up to --
19:02
call you some different letter, say Vl -- span a space, a subspace, or just a vector space
19:15
I could say, span a space means, means
19:24
the space consists of all combinations of those vectors.
19:40
19:46
That's exactly what we did with the column space. So now I could say in shorthand the columns of a matrix
19:54
span the column space. So you remember it's a bunch of vectors that have this property
20:00
that they span a space, and actually if I give you a bunch of vectors and say --
20:06
OK, let S be the space that they span, in other words let S contain all their combinations,
20:14
that space S will be the smallest space with those vectors in it, right?
20:21
Because any space with those vectors in it must have all the combinations of those vectors in it.
20:28
And if I stop there, then I've got the smallest space,
20:34
and that's the space that they span. OK. So I'm just -- rather than, needing to say, take all linear combinations
20:44
and put them in a space, I'm compressing that into the word span.
20:50
Straightforward.
20:57
So if I think of a, of the column space of a OK. matrix.
21:03
I've got their -- so I start with the columns. I take all their combinations.
21:08
That gives me the columns space. They span the column space. Now are they independent?
21:16
Maybe yes, maybe no. It depends on the particular columns that went into that
21:23
matrix. But obviously I'm highly interested in a set
21:29
of vectors that spans a space and is independent.
21:36
That's, that means like I've got the right number of vectors. If I didn't have all of them, I wouldn't have my whole space.
21:47
If I had more than that, they probably wouldn't -- they wouldn't be independent. So, like, basis -- and that's the word that's coming --
21:56
is just right. So here let me put what that word means. A basis for a vector space is, is a, is a sequence of vectors
22:14
-- 
22:20
shall I call them V1, V2, up to let me say Vd now,
22:27
I'll stop with that letters -- that has two properties.
22:33
I've got enough vectors and not too many. It's a natural idea of a basis.
22:39
So a basis is a bunch of vectors in the space and it's a so it's a sequence of vectors with two properties,
22:47
with two properties.
22:54
One, they are independent.
23:05
And two -- you know what's coming? -- they span the space.
23:10
23:20
OK. Let me take -- so time for examples, of course.
# Examples
23:28
So I'm asking you now to put definition one, the definition of independence, together with definition two,
23:38
and let's look at examples, because this is -- this combination means the set I've --
23:44
of vectors I have is just right, and the -- so that this idea of a basis will be central.
23:51
I'll always be asking you now for a basis. Whenever I look at a subspace, if I ask you for --
23:58
if you give me a basis for that subspace, you've told me what it is. You've told me everything I need to know about that subspace.
24:07
Those -- I take their combinations and I know that I need all the combinations.
24:12
OK. Examples. OK, so examples of a basis. Let me start with two dimensional space.
24:20
Suppose the space -- say example.
24:26
The space is, oh, let's make it R^3. Real three dimensional space.
24:35
Give me one basis. One basis is?
24:43
So I want some vectors, because if I ask you for a basis,
24:49
I'm asking you for vectors, a little list of vectors. And it should be just right.
24:57
So what would be a basis for three dimensional space?
25:02
Well, the first basis that comes to mind, why don't we write that down. The first basis that comes to mind
25:09
is this vector, this vector, and this vector.
25:18
OK. That's one basis. Not the only basis, that's going to be my point.
25:27
But let's just see -- yes, that's a basis. Are, are those vectors independent?
25:33
So that's the like the x, y, z axes, so if those are not
25:40
independent, we're in trouble. Certainly, they are. Take a combination c1 of this vector plus c2 of this vector
25:48
plus c3 of that vector and try to make it give the zero vector.
25:55
What are the c-s? If c1 of that plus c2 of that plus c3 of that gives me 0 0 0,
26:03
then the c-s are all -- 0, right. So that's the test for independence.
26:09
In the language of matrices, which was under that board,
26:16
I could make those the columns of a matrix. Well, it would be the identity matrix.
26:22
Then I would ask, what's the null space of the identity matrix? And you would say it's only the zero vector.
26:30
And I would say, fine, then the columns are independent. The only thing -- the identity times a vector giving zero,
26:38
the only vector that does that is zero. OK. Now that's not the only basis.
26:45
Far from it. Tell me another basis, a second basis, another basis.
26:57
So, give me -- well, I'll just start it out.
27:03
One one two. Two two five.
27:10
Suppose I stopped there. Has that little bunch of vectors got the properties that
27:20
I'm asking for in a basis for R^3? We're looking for a basis for R^3.
27:26
Are they independent, those two column vectors? Yes. Do they span R^3?
27:33
No. Our feeling is no. Our feeling is no. Our feeling is that there're some vectors in R3 that
27:41
are not combinations of those. OK. So suppose I add in --
27:47
I need another vector then, because these two don't span the space. OK. Now it would be foolish for me to put in three three seven,
27:56
right, as the third vector. That would be a goof. Because that, if I put in three three seven,
28:03
those vectors would be dependent, right? If I put in three three seven, it
28:09
would be the sum of those two, it would lie in the same plane as those.
28:15
It wouldn't be independent. My attempt to create a basis would be dead.
28:21
But if I take -- so what vector can I take? I can take any vector that's not in that plane.
28:30
Let me try -- I hope that 3 3 8 would do it.
28:37
At least it's not the sum of those two vectors. But I believe that's a basis.
28:44
And what's the test then, for that to be a basis?
28:49
Because I just picked those numbers, and if I had picked, 5 7 -14 how would we know do we have a basis or don't we?
29:02
You would put them in the columns of a matrix, and you would do elimination, row reduction --
29:09
and you would see do you get any free variables
29:15
or are all the columns pivot columns. Well now actually we have a square --
29:20
the matrix would be three by three. So, what's the test on the matrix then?
29:26
The matrix -- so in this case, when my space is R^3 and I have
29:34
three vectors, my matrix is square and what I asking about
29:44
that matrix in order for those columns to be a basis?
29:49
So in this -- for R^n, if I have -- n vectors give a basis if the n by n
30:06
matrix with those columns, with those columns, is what?
30:19
What's the requirement on that matrix? Invertible, right, right.
30:27
The matrix should be invertible. For a square matrix, that's the, that's the perfect answer.
30:33
Is invertible.
30:38
So that's when, that's when the space is the whole space R^n.
30:43
Let me, let me be sure you're with me here.
30:50
Let me remove that. Are those two vectors a basis for any space at all?
31:01
Is there a vector space that those really are a basis for, those, that pair of vectors, this guy
31:08
and this 1, 1 1 2 and 2 2 5? Is there a space for which that's a basis?
31:15
Sure. They're independent, so they satisfy the first requirement,
31:21
so what space shall I take for them to be a basis of? What spaces will they be a basis for?
31:29
The one they span. Their combinations. It's a plane, right? It'll be a plane inside R^3.
31:36
So if I take this vector 1 1 2, say it goes there, and this vector 2 2 5, say it goes there,
31:44
those are a basis for -- because they span a plane.
31:50
And they're a basis for the plane, because they're independent. If I stick in some third guy, like 3 3 7,
31:56
which is in the plane -- suppose I put in, try to put in 3 3 7, then the three vectors would still span the plane,
32:05
but they wouldn't be a basis anymore because they're not independent anymore.
32:12
So, we're looking at the question of --
32:19
again, OK. the case with independent columns
32:25
is the case where the column vectors span the column space.
32:32
They're independent, so they're a basis for the column space. OK. So now there's one bit of intuition.
32:42
Let me go back to all of R^n. So I -- where I put 3 3 8.
32:48
OK. The first message is that the basis is not unique, right.
32:56
There's zillions of bases. I take any invertible three by three matrix,
33:02
its columns are a basis for R^3.
33:07
The column space is R^3, and if those, if that matrix is invertible, those columns are independent,
33:15
I've got a basis for R^3. So there're many, many bases. But there is something in common for all those bases.
33:27
There's something that this basis shares with that basis
33:32
and every other basis for R^3. And what's that?
33:40
Well, you saw it coming, because when I stopped here and asked
33:47
if that was a basis for R^3, you said no. And I know that you said no because you knew there
33:54
weren't enough vectors there. And the great fact is that there're many, many bases,
34:03
but -- let me put in somebody else, just for variety.
34:12
There are many, many bases, but they all have the same number of vectors.
34:18
If we're talking about the space R^3, then that number of vectors is three.
34:25
If we're talking about the space R^n, then that number of vectors is n.
34:31
If we're talking about some other space,
34:36
the column space of some matrix, or the null space of some
34:41
matrix, or some other space that we haven't even thought of, then that still is true that every basis --
# Dimension
34:52
that there're lots of bases but every basis has the same number of vectors.
34:57
Let me write that great fact down. Every basis -- we're given a space.
35:07
Given a space.
35:13
R^3 or R^n or some other column space of a matrix or the null space of a matrix or some other vector space.
35:21
Then the great fact is that every basis for this, for the space has the same number of vectors.
35:40
35:47
If one basis has six vectors, then every other basis has six vectors.
35:52
So that number six is telling me like it's telling me how big is the space.
35:59
It's telling me how many vectors do I have to have to have a basis.
36:04
And of course we're seeing it this way. That number six, if we had seven vectors,
36:10
then we've got too many. If we have five vectors we haven't got enough.
36:17
Sixes are like just right for whatever space that is. And what do we call that number?
36:24
That number is -- now I'm ready for the last definition today. It's the dimension of that space.
36:33
So every basis for a space has the same number of vectors in it. Not the same vectors, all sorts of bases --
36:41
but the same number of vectors is always the same, and that number is the dimension.
36:47
This is definitional. This number is the dimension of the space.
36:58
37:03
OK. OK.
37:08
Let's do some examples. Because now we've got definitions.
37:14
Let me repeat the four things, the four words that have now got defined. Independence, that looks at combinations not
37:23
being zero. Spanning, that looks at all the combinations. Basis, that's the one that combines
37:30
independence and spanning. And now we've got the idea of the dimension of a space.
37:36
It's the number of vectors in any basis, because all bases have the same number.
37:43
OK. Let's take examples. Suppose I take, my space is -- examples now --
# Example
37:53
space is the, say, the column space of this matrix. Let me write down a matrix.
38:00
1 1 1, 2 1 2, and I'll -- just to make it clear,
38:06
I'll take the sum there, 3 2 3, and let me take the sum of all
38:12
-- oh, let me put in one -- yeah, I'll put in one one one again.
38:18
OK. So that's four vectors.
38:24
OK, do they span the column space of that matrix? Let me repeat, do they span the column space of that matrix?
38:35
By definition, that's what the column space -- Yes. where it comes from. Are they a basis for the column space?
38:41
Are they independent? No, they're not independent. There's something in that null space.
38:49
Maybe we can -- so let's look at the null space of the matrix.
38:55
Tell me a vector that's in the null space of that matrix.
39:01
So I'm looking for some vector that combines those columns and produces the zero column.
39:08
Or in other words, I'm looking for solutions to A X equals zero. So tell me a vector in the null space.
39:16
Maybe -- well, this was, this column was that one plus that one, so maybe if I have one of those and minus one of those
39:25
that would be a vector in the null space. So, you've already told me now, are those vectors independent,
39:33
the answer is -- those column vectors, the answer is -- no.
39:38
Right? They're not independent. Because -- you knew they weren't independent.
39:44
Anyway, minus one of this minus one of this plus one of this zero of that is the zero vector.
39:50
OK, so they're not independent.
39:55
OK. They span, but they're not independent. Tell me a basis for that column space.
40:04
What's a basis for the column space? These are all the questions that the homework asks, the quizzes ask, the final exam will ask.
40:11
Find a basis for the column space of this matrix.
40:17
OK. Now there's many answers, but give me the most natural answer.
40:25
Columns one and two. Columns one and two.
40:31
That's the natural answer. Those are the pivot columns, because, I mean, we s- we begin systematically.
40:37
We look at the first column, it's OK. We can put that in the basis. We look at the second column, it's OK.
40:43
We can put that in the basis. The third column we can't put in the basis.
40:48
The fourth column we can't, again.
40:54
So the rank of the matrix is -- what's the rank of our matrix?
40:59
Two. Two. And, and now that rank is also -- we also have another word.
41:06
We, we have a great theorem here. The rank of A, that rank r, is the number of pivot columns
41:23
and it's also -- well, so now please use my new word.
41:32
This, it's the number two, of course, two is the rank of my matrix, it's
41:41
the number of pivot columns, those pivot columns form a basis, of course, so what's two?
41:49
It's the dimension. The rank of A, the number of pivot columns,
41:55
is the dimension of the column space.
42:02
Of course, you say.
42:08
It had to be. Right. But just watch, look for one moment at the,
42:17
the language, the way the English words get involved here. I take the rank of a matrix, the rank of a matrix.
42:28
It's a number of columns and it's the dimension of --
42:34
not the dimension of the matrix, that's what I want to say. It's the dimension of a space, a subspace, the column space.
42:43
Do you see, I don't take the dimension of A. That's not what I want.
42:49
I'm looking for the dimension of the column space of A. If you use those words right, it shows you've got the idea
42:56
right. Similarly here. I don't talk about the rank of a subspace.
43:03
It's a matrix that has a rank. I talk about the rank of a matrix. And the beauty is that these definitions just
43:11
merge so that the rank of a matrix is the dimension of its column space. And in this example it's two.
43:18
And then the further question is, what's a basis? And the first two columns are a basis.
43:25
Tell me another basis. Another basis for the columns space. You see I just keep hammering away.
43:31
I apologize, but it's, I have to be sure you have the idea of basis.
43:36
Tell me another basis for the column space. Well, you could take columns one and three.
43:47
That would be a basis for the column space. Or columns two and three would be a basis.
43:53
Or columns two and four. Or tell me another basis that's not made out of those columns
43:58
at all? So -- I guess I'm giving you infinitely many possibilities,
44:07
so I can't expect a unanimous answer here. I'll tell you -- but let's look at another basis, though.
44:14
I'll just -- because it's only one out of zillions, I'm going to put it down and I'm going to erase it.
44:19
Another basis for the column space would be --
44:26
let's see. I'll put in some things that are not there. Say, oh well, just to make it -- my life easy, 2 2 2.
44:33
That's in the column space. And, that was sort of obvious.
44:40
Let me take the sum of those, say 6 4 6.
44:46
Or the sum of all of the columns, 7 5 7, why not.
44:52
That's in the column space. Those are independent and I've got the number right,
44:59
I've got two. Actually, this is a key point.
45:05
If you know the dimension of the space you're working with, and we know that this column -- we know that the dimension,
45:14
DIM, the dimension of the column space is two.
45:23
If you know the dimension, then --
45:30
and we have a couple of vectors that are independent, they'll automatically be a
45:35
basis. If we've got the number of vectors right, two vectors in this case, then if they're independent,
45:43
they can't help but span the space. Because if they didn't span the space, there'd be a third guy to help span the space,
45:52
but it couldn't be independent. So, it just has to be independent
45:58
if we've got the numbers right. And they span. OK.
46:03
Very good. So you got the dimension of a space. So this was another basis that I just invented.
46:08
OK. Now, now I get to ask about the null space.
46:16
What's the dimension of the null space? So we, we got a great fact there, the dimension of the column space is the rank.
46:28
Now I want to ask you about the null space. That's the other part of the lecture,
46:34
and it'll go on to the next lecture. OK.
46:40
So we know the dimension of the column space is two, the rank. What about the null space?
46:46
This is a vector in the null space. Are there other vectors in the null space?
46:52
Yes or no? Yes. So this isn't a basis because it's doesn't span, right?
46:58
There's more in the null space than we've got so far. I need another vector at least.
47:04
So tell me another vector in the null space. Well, the natural choice, the choice you naturally think of
47:12
is I'm going on to the fourth column, I'm letting that free variable be a one,
47:20
and that free variable be a zero, and I'm asking is that fourth column a combination
47:26
of my pivot columns? Yes, it is. And it's -- that will do.
47:31
So what I've written there are actually the two
47:37
special solutions, right? I took the two free variables, free and free.
47:44
I gave them the values 1 0 or 0 I figured out the rest.
47:50
So do you see, let me just say it in words. This vector, these vectors in the null space are telling me,
47:58
they're telling me the combinations of the columns that give zero. They're telling me in what way the, the columns are dependent.
48:08
That's what the null space is doing. Have I got enough now?
48:13
And what's the null space now? We have to think about the null space. These are two vectors in the null space.
48:20
They're independent. Are they a basis for the null space? What's the dimension of the null space?
48:27
You see that those questions just keep coming up all the time. Are they a basis for the null space?
48:34
You can tell me the answer even though we haven't written out a proof of that. Can you?
48:40
Yes or no? Do these two special solutions form a basis for the null space?
48:46
In other words, does the null space consist of all combinations of those two guys?
48:52
Yes or no? Yes. Yes. The null space is two dimensional.
48:58
The null space, the dimension of the null space, is the number of free variables. So the dimension of the null space
49:09
is the number of free variables.
49:17
And at the last second, give me the formula. This is then the key formula that we know.
49:24
How many free variables are there in terms of R, the rank,
49:29
m -- the number of rows, n, the number of columns?
49:35
What do we get? We have n columns, r of them are pivot columns,
49:42
so n-r is the number of free columns, free variables.
49:48
And now it's the dimension of the null space. OK. That's great.
49:53
That's the key spaces, their bases, and their dimensions. Thanks.
