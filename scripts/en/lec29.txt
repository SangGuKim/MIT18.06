https://www.youtube.com/watch?v=TX_vooSnhm8

# Introduction
0:00
0:14
Okay. This is the lecture on the singular value decomposition.
0:21
But everybody calls it the SVD.
0:27
So this is the final and best factorization of a matrix.
0:34
Let me tell you what's coming. The factors will be, orthogonal matrix, diagonal matrix,
0:43
orthogonal matrix. So it's things that we've seen before,
0:48
these special good matrices, orthogonal diagonal. The new point is that we need two orthogonal matrices.
0:58
A can be any matrix whatsoever. Any matrix whatsoever has this singular value decomposition,
1:07
so a diagonal one in the middle, but I need two different -- probably different orthogonal matrices to be able to do this.
1:17
Okay. And this factorization has jumped into importance and is properly, I think, maybe the bringing together
1:27
of everything in this course. One thing we'll bring together is the very good family
1:36
of matrices that we just studied, symmetric, positive, definite. Do you remember the stories with those guys?
1:43
Because they were symmetric, their eigenvectors were
1:49
orthogonal, so I could produce an orthogonal matrix -- this is my usual one.
1:56
My usual one is the eigenvectors and eigenvalues In the symmetric case, the eigenvectors are orthogonal,
2:05
so I've got the good -- my ordinary s has become an especially good Q.
2:12
And positive definite, my ordinary lambda has become a positive lambda.
2:18
So that's the singular value decomposition in case
2:24
our matrix is symmetric positive definite -- in that case, I don't need two --
2:30
U and a V -- one orthogonal matrix will do for both sides.
2:35
So this would be no good in general,
2:41
because usually the eigenvector matrix isn't orthogonal. So that's not what I'm after.
2:49
I'm looking for orthogonal times diagonal times orthogonal.
2:56
And let me show you what that means and where it comes from. Okay.
# Linear Transformation
3:01
What does it mean? You remember the picture of any linear transformation.
3:10
This was, like, the most important figure in
3:16
And what I looking for now? A typical vector in the row space --
3:26
typical vector, let me call it v1, gets taken over to some vector in the column space, say u1.
3:35
So u1 is Av1.
3:41
Okay. Now, another vector gets taken over here somewhere.
3:48
What I looking for? In this SVD, this singular value decomposition,
3:54
what I'm looking for is an orthogonal basis here
3:59
that gets knocked over into an orthogonal basis over there.
4:06
See that's pretty special, to have an orthogonal basis
4:11
in the row space that goes over into an orthogonal basis --
4:18
so this is like a right angle and this is a right angle -- into an orthogonal basis in the column space.
4:25
So that's our goal, is to find --
4:30
do you see how things are coming together? First of all, can I find an orthogonal basis
4:37
for this row space? Of course. No big deal to find an orthogonal basis.
4:44
Graham Schmidt tells me how to do it. Start with any old basis and grind through Graham Schmidt,
4:50
out comes an orthogonal basis. But then, if I just take any old orthogonal basis, then
4:58
when I multiply by A, there's no reason why it should be orthogonal over here.
5:04
So I'm looking for this special set up where A takes these basis vectors into orthogonal vectors
5:13
over there. Now, you might have noticed that the null space I didn't include.
5:19
Why don't I stick that in? You remember our usual figure had a little null space
5:25
and a little null space. And those are no problems.
5:31
Those null spaces are going to show up as zeroes on the diagonal of sigma,
5:37
so that doesn't present any difficulty.
5:43
Our difficulty is to find these. So do you see what this will mean? This will mean that A times these v-s, v1, v2, up to --
5:57
what's the dimension of this row space? Vr.
6:03
Sorry, make that V a little smaller -- up to vr.
6:10
So that's -- Av1 is going to be the first column,
6:16
so here's what I'm achieving. Oh, I'm not only going to make these orthogonal,
6:24
but why not make them orthonormal? Make them unit vectors. So maybe the unit vector is here, is the u1,
6:34
and this might be a multiple of it. So really, what's happening is Av1 is some multiple of u1,
6:45
right? These guys will be unit vectors and they'll go over into multiples of unit vectors
6:52
and the multiple I'm not going to call lambda anymore. I'm calling it sigma.
6:58
So that's the number -- the stretching number. And similarly, Av2 is sigma two u2.
7:06
This is my goal. And now I want to express that goal in matrix language.
7:13
That's the usual step. Think of what you want and then express it as a matrix multiplication.
7:20
So Av1 is sigma one u1 --
7:25
actually, here we go. Let me pull out these -- u1, u2 to ur and then a matrix with the sigmas.
7:36
Everything now is going to be in that little part
7:45
of the blackboard. Do you see that this equation says what I'm
7:52
trying to do with my figure. A times the first basis vector should be sigma one times
8:00
the other basis -- the other first basis vector. These are the basis vectors in the row space,
8:08
these are the basis vectors in the column space and these are the multiplying factors.
8:14
So Av2 is sigma two times u2, Avr is sigma r times ur.
8:23
And then we've got a whole lot of zeroes and maybe some zeroes at the end, but that's the heart of it.
8:31
And now if I express that in --
8:40
as matrices, because you knew that was coming -- that's what I have.
8:45
So, this is my goal, to find an orthogonal basis
8:51
in the orthonormal, even -- basis in the row space and an orthonormal basis in the column space so that I've sort
9:00
of diagonalized the matrix. The matrix A is, like, getting converted
9:05
to this diagonal matrix sigma. And you notice that usually I have to allow myself
9:13
two different bases. My little comment about symmetric positive definite
9:19
was the one case where it's A Q equal Q sigma,
9:24
where V and U are the same Q. But mostly, you know, I'm going to take a matrix like -- oh,
9:32
let me take a matrix like four four minus three three.
9:38
Okay. There's a two by two matrix. It's invertible, so it has rank two.
9:48
So I'm going to look for two vectors, v1 and v2 in the row space, and U --
9:55
so I'm going to look for v1, v2 in the row space,
10:00
which of course is R^2. And I'm going to look for u1, u2 in the column space,
10:10
which of course is also R^2, and I'm going to look for numbers
10:15
sigma one and sigma two so that it all comes out right.
10:21
So these guys are orthonormal, these guys are orthonormal and these are the scaling factors.
10:28
So I'll do that example as soon as I get the matrix picture straight.
10:35
Okay. Do you see that this expresses what I want? Can I just say two words about null spaces?
10:45
If there's some null space, then we
10:52
want to stick in a basis for those, for that. So here comes a basis for the null space, v(r+1) down to vm.
11:02
So if we only had an r dimensional row space and the other n-r dimensions were in the null space -- okay,
11:11
we'll take an orthogonal -- orthonormal basis there. No problem. And then we'll just get zeroes.
11:18
So, actually, w- those zeroes will come out on the diagonal matrix.
11:25
So I'll complete that to an orthonormal basis for the whole
11:33
space, R^m. I complete this to an orthonormal basis for the whole space R^n and I complete that with zeroes.
11:42
Null spaces are no problem here. So really the true problem is in a matrix like that,
11:52
which isn't symmetric, so I can't use its eigenvectors, they're not orthogonal --
11:58
but somehow I have to get these orthogonal -- in fact, orthonormal guys that make it work.
12:09
I have to find these orthonormal guys, these orthonormal guys and I want Av1 to be sigma one u1 and Av2 to be sigma two u2.
12:22
Okay. That's my goal.
# Orthogonal matrices
12:28
Here's the matrices that are going to get me there.
12:34
Now these are orthogonal matrices. I can put that -- if I multiply on both sides by V inverse,
12:41
I have A equals U sigma V inverse,
12:47
and of course you know the other way I can write V inverse.
12:53
This is one of those square orthogonal matrices, so it's the same as U sigma V transpose.
13:02
Okay. Here's my problem.
# Two orthogonal matrices
13:08
I've got two orthogonal matrices here.
13:14
And I don't want to find them both at once. So I want to cook up some expression that
13:21
will make the Us disappear.
13:26
I would like to make the Us disappear and leave me only with the Vs. And here's how to do it.
13:34
It's the same combination that keeps showing up whenever we have a general rectangular matrix,
13:41
then it's A transpose A, that's the great matrix.
13:46
That's the great matrix. That's the matrix that's symmetric, and in fact positive definite or at least
13:53
positive semi-definite. This is the matrix with nice properties, so let's see what will it be?
13:59
So if I took the transpose, then, I would have -- A transpose A will be what?
14:06
What do I have? If I transpose that I have V sigma transpose U transpose,
14:12
that's the A transpose. Now the A --
14:18
and what have I got? Looks like worse, because it's got six things now together,
14:25
but it's going to collapse into something good. What does U transpose U collapse into?
14:34
I, the identity. So that's the key point. This is the identity and we don't have U anymore.
14:42
And sigma transpose times sigma, those are diagonal matrixes, so their product is just
14:48
going to have sigma squareds on the diagonal. So do you see what we've got here? This is V times this easy matrix sigma
14:57
one squared sigma two squared times V transpose.
15:03
This is the A transpose A. This is -- let me copy down -- A transpose A is that.
15:09
Us are out of the picture, now. I'm only having to choose the Vs, and what are these Vs?
15:17
And what are these sigmas? Do you know what the Vs are?
15:26
They're the eigenvectors that -- see, this is a perfect eigenvector, eigenvalue,
15:34
Q lambda Q transpose for the matrix A transpose A.
15:41
A itself is nothing special. But A transpose A will be special.
15:48
It'll be symmetric positive definite, so this will be its eigenvectors and this'll be its eigenvalues.
15:55
And the eigenvalues'll be positive because this thing's positive definite.
16:02
So this is my method. This tells me what the Vs are. And how I going to find the Us?
16:09
Well, one way would be to look at A A transpose.
16:18
Multiply A by A transpose in the opposite order. That will stick the Vs in the middle,
16:24
knock them out, and leave me with the Us. So here's the overall picture, then.
16:29
The Vs are the eigenvectors of A transpose A.
16:36
The Us are the eigenvectors of A A transpose, which are different. And the sigmas are the square roots
16:43
of these and the positive square roots, so we have positive sigmas.
16:50
Let me do it for that example. This is really what you should know
16:56
and be able to do for the SVD. Okay.
17:02
Let me take that matrix. So what's my first step? Compute A transpose A, because I want its eigenvectors.
17:12
Okay. So I have to compute A transpose A. So A transpose is four four minus three three,
17:22
and A is four four minus three three, and I do that multiplication and I get sixteen --
17:30
I get twenty five -- I get sixteen minus nine --
17:36
is that seven? And it better come out symmetric. And -- oh, okay, and then it comes out 25.
17:43
Okay. So, I want its eigenvectors and its eigenvalues.
17:52
Its eigenvectors will be the Vs, its eigenvalues will be the squares of the sigmas.
17:58
Okay. What are the eigenvalues and eigenvectors of this guy?
18:04
Have you seen that two by two example enough to recognize
18:10
that the eigenvectors are -- that one one is an eigenvector?
18:16
So this here is A transpose A. I'm looking for its eigenvectors.
18:22
So its eigenvectors, I think, are one one and one minus one,
18:27
because if I multiply that matrix by one one, what do I get? If I multiply that matrix by one one, I get 32 32,
18:37
which is 32 of one one. So there's the first eigenvector,
18:45
and there's the eigenvalue for A transpose A. So I'm going to take its square root for sigma.
18:59
Okay. What's the eigenvector that goes -- eigenvalue that goes with this one? If I do that multiplication, what do I get?
19:05
I get some multiple of one minus one, and what is that multiple?
19:12
Looks like 18. Okay. So those are the two eigenvectors, but -- oh,
19:20
just a moment, I didn't normalize them. To make everything absolutely right,
19:27
I ought to normalize these eigenvectors, divide by their length, square root of two.
19:33
So all these guys should be true unit vectors and, of course,
19:42
that normalization didn't change the 32 and the 18. Okay.
19:48
So I'm happy with the Vs. Here are the Vs. So now let me put together the pieces here.
19:57
Here's my A. Here's my A. Let me write down A again.
20:03
If life is right, we should get U, which I don't yet know --
20:16
U I don't yet know, sigma I do now know. What's sigma? So I'm looking for a U sigma V transpose.
20:24
U, the diagonal guy and V transpose.
20:31
Okay. Let's just see that come out right. So what are the sigmas?
20:36
They're the square roots of these things. So square root of 32 and square root of 18.
20:43
Zero zero.
20:49
Okay. What are the Vs? They're these two. And I have to transpose --
20:56
maybe that just leaves me with ones -- with one over square root of two in that row and the other one
21:03
is one over square root of two minus one over square root of two.
21:11
Now finally, I've got to know the Us. Well, actually, one way to do -- since I now know all the other
21:18
pieces, I could put those together and figure out what the Us are. But let me do it the A A transpose way.
21:25
Okay. Find the Us now.
21:31
u1 and u2. And what are they?
21:37
I look at A A transpose -- so A is supposed to be U sigma V transpose, and then
21:47
when I transpose that I get V sigma transpose U transpose.
21:52
So I'm just doing it in the opposite order,
21:59
A times A transpose, and what's the good part here? That in the middle, V transpose V is going to be the identity.
22:10
So this is just U sigma sigma transpose, that's some diagonal matrix with sigma squareds and U transpose.
22:22
So what I seeing here? I'm seeing here, again, a symmetric positive definite
22:29
or at least semi-definite matrix and I'm seeing its eigenvectors and its eigenvalues.
22:36
So if I compute A A transpose, its eigenvectors will be the things that go into U.
22:44
Okay, so I need to compute A A transpose. I guess I'm going to have to go --
22:50
can I move that up just a little? Maybe a little more and do A A transpose.
22:56
So what's A?
23:01
Four four minus three and three. And what's A transpose?
23:07
Four four minus three and three. And when I do that multiplication, what do I get?
23:15
Sixteen and sixteen, thirty two. Uh, that one comes out zero.
23:21
Oh, so this is a lucky case and that one comes out 18.
23:26
So this is an accident that A A transpose happens to come out diagonal, so we know easily its eigenvectors
23:38
and eigenvalues. So its eigenvectors -- what's the first eigenvector for this
23:43
A A transpose matrix? It's just one zero, and when I do that multiplication,
23:49
I get 32 times one zero. And the other eigenvector is just zero one
23:57
and when I multiply by that I get 18. So this is A A transpose.
24:04
Multiplying that gives me the 32 A A transpose. Multiplying this guy gives me First of all,
24:14
I got 32 and 18 again. Am I surprised? You know, it's clearly not an accident.
24:24
The eigenvalues of A A transpose were exactly the same as the eigenvalues of -- this one was A transpose A.
24:37
That's no surprise at all. The eigenvalues of A B are the same as the eigenvalues of B A.
24:47
That's a very nice fact, that eigenvalues stay the same if I switch the order of multiplication.
24:55
So no surprise to see 32 and What I learned --
25:01
first the check that things were numerically correct, but now I've learned these eigenvectors,
25:07
and actually they're about as nice as can be. They're the best orthogonal matrix, just the identity.
25:16
Okay.
25:21
So my claim is that it ought to all fit together, that these numbers should come out right.
25:31
The numbers should come out right because the matrix multiplications use
25:41
the properties that we want. Okay. Shall we just check that? Here's the identity, so not doing anything --
25:47
square root of 32 is multiplying that row, so that square root of 32 divided by square root of two
25:53
means square root of 16, four, correct? And square root of 18 is divided by square root of two,
26:01
so that leaves me square root of 9, which is three, but --
26:07
well, Professor Strang, you see the problem? Why is that --
26:12
okay. Why I getting minus three three here and here I'm getting three minus three?
26:19
Phooey.
26:26
I don't know why. It shouldn't have happened, but it did.
26:34
Now, okay, you could say, well, just --
26:41
the eigenvector there could have -- I could have had the minus sign here for that eigenvector,
26:47
but I'm not happy about that. Hmm. Okay.
26:55
So I realize there's a little catch here somewhere and I may not see it until Wednesday.
27:02
Which then gives you a very important reason to come back on Wednesday, to catch that sine difference.
27:09
So what did I do illegally? I think I put the eigenvectors in that matrix V transpose --
27:22
okay, I'm going to have to think. Why did that come out with with the opposite sines?
27:29
So you see -- I mean, if I had a minus there, I would be all right,
27:35
but I don't want that. I want positive entries down the diagonal of sigma squared.
27:45
Okay. It'll come to me, but, I'm going to leave this example
27:51
to finish.
27:57
Okay. And the beauty of, these sliding boards is I can make that go away.
28:05
Can I,-- let me not do it, though, yet.
28:10
Let me take a second example. Let me take a second example where the matrix is singular.
28:18
So rank one.
28:24
Okay, so let me take as an example two,
28:32
where my matrix A is going to be rectangular again --
28:38
let me just make it four three eight six.
28:47
Okay. That's a rank one matrix. So that has a null space and only a one dimensional row
28:57
space and column space. So actually, my picture becomes easy for this matrix,
29:05
because what's my row space for this one? So this is two by two.
29:12
So my pictures are both two dimensional. My row space is all multiples of that vector four three.
29:22
So the whole -- the row space is just a line, right?
29:27
That's the row space. And the null space, of course, is the perpendicular line.
29:33
So the row space for this matrix is multiples of four three.
29:46
Typical row. Okay. What's the column space? The columns are all multiples of four eight, three six, one two.
29:55
The column space, then, goes in, like, this direction.
30:03
So the column space -- when I look at those columns, the column space --
30:09
so it's only one dimensional, because the rank is one. It's multiples of four eight.
30:21
Okay. And what's the null space of A transpose? It's the perpendicular guy.
30:30
So this was the null space of A and this is the null space of A transpose.
30:38
Okay. What I want to say here is that choosing these orthogonal bases
30:48
for the row space and the column space is, like, no problem.
30:53
They're only one dimensional. So what should V be? V should be -- v1, but -- yes, v1, rather --
31:02
v1 is supposed to be a unit vector. There's only one v1 to choose here,
31:08
only one dimension in the row space. I just want to make it a unit vector.
31:13
So v1 will be -- it'll be this vector, but made into a unit vector, so four --
31:24
point eight point six. Four fifths, three fifths.
31:30
And what will be u1? u1 will be the unit vector there.
31:35
So I want to turn four eight or one two into a unit vector,
31:40
so u1 will be -- let's see, if it's one two, then what multiple of one two
31:47
do I want? That has length square root of five, so I have to divide by square root of five.
31:53
Let me complete the singular value
31:58
decomposition for this matrix. So this matrix, four three eight six, is --
32:09
so I know what u1 -- here's A and I want to get U the basis in the column space.
32:19
And it has to start with this guy, one over square root of five two over square root of five.
32:27
Then I want the sigma.
32:36
Okay. What are we expecting now for sigma?
32:44
This is only a rank one matrix. We're only expecting a sigma one, which I have to find,
32:51
but zeroes here. Okay. So what's sigma one?
32:57
It should be the --
33:02
where did these sigmas come from? They came from A transpose A, so I --
33:08
can I do that little calculation over here? A transpose A is four three -- four three eight six times four
33:19
three eight six. This had better -- this is a rank one matrix,
33:26
this is going to be -- the whole thing will have rank one, that's 16 and 64 is 80, 12 and 48 is 60, 12 and 48 is 60,
33:40
9 and 36 is 45. Okay.
33:45
It's a rank one matrix. Of course. Every row is a multiple of four three.
33:52
And what's the eigen -- what are the eigenvalues of that matrix? So this is the calculation -- this is like practicing,
33:59
now. What are the eigenvalues of this rank one matrix? Well, tell me one eigenvalue, since the rank is only one,
34:08
one eigenvalue is going to be zero. And then you know that the other eigenvalue
34:14
is going to be a hundred and twenty five.
34:19
So that's sigma squared, right, in A transpose A. So this will be the square root of a hundred and twenty five.
34:28
And then finally, the V transpose --
34:34
the Vs will be -- there's v1, and what's v2?
34:41
What's v2 in the -- how do I make this into an orthonormal basis?
34:50
Well, v2 is, in the null space direction. It's perpendicular to that, so point six and minus point
34:59
eight. So those are the Vs that go in here. Point eight, point six and point six minus point eight.
35:11
Okay. And I guess I better finish this guy.
35:17
So this guy, all I want is to complete the orthonormal basis -- it'll be coming from there.
35:23
It'll be a two over square root of five and a minus one
35:28
over square root of five. Let me take square root of five out of that matrix
35:35
to make it look better. So one over square root of five times one two two minus one.
35:44
Okay.
35:49
So there I have -- including the square root of five -- that's an orthogonal matrix, that's an orthogonal matrix,
35:56
that's a diagonal matrix and its rank is only one. And now if I do that multiplication,
36:03
I pray that it comes out right.
36:10
The square root of five will cancel into that square root of one twenty five and leave me with the square root of 25, which
36:16
is five, and five will multiply these numbers and I'll get whole numbers and out will come A.
36:24
Okay. That's like a second example showing how the null space guy
36:31
-- so this -- this vector and this one were multiplied
36:39
by this zero. So they were easy to deal with.
36:46
Tthe key ones are the ones in the column space and the row space.
36:51
Do you see how I'm getting columns here, diagonal here,
36:57
rows here, coming together to produce A. Okay, that's the singular value decomposition.
37:06
So, let me think what I want to add to complete this topic.
37:13
So that's two examples.
37:22
And now let's think what we're really doing. We're choosing the right basis for the four subspaces
37:33
of linear algebra. Let me write this down.
37:39
So v1 up to vr is an orthonormal basis for the row space.
37:52
37:58
u1 up to ur is an orthonormal basis for the column space.
38:05
And then I just finish those out by v(r+1),
38:14
the rest up to vn is an orthonormal basis for the null
38:20
space. And finally, u(r+1) up to is an orthonormal basis for the null
38:35
space of A transpose. Do you see that we finally got the bases right?
38:45
They're right because they're orthonormal, and also --
38:51
again, Graham Schmidt would have done this in chapter four. Here we needed eigenvalues, because these bases
39:00
make the matrix diagonal. A times V I is a multiple of U I.
39:09
So I'll put "and" --
39:14
the matrix has been made diagonal. When we choose these bases, there's no coupling between Vs
39:24
and no coupling between Us. Each A -- A times each V is in the direction
39:30
of the corresponding U. So it's exactly the right basis for the four
39:36
fundamental subspaces. And of course, their dimensions are what we know. The dimension of the row space is
39:44
the rank r, and so is the dimension of the column space. The dimension of the null space is
39:51
n-r, that's how many vectors we need, and m-r basis vectors for the left null space, the null space
39:59
of A transpose. Okay. I'm going to stop there.
40:05
I could develop further from the SVD, but we'll see it again in the very last lectures
40:14
of the course. So there's the SVD. Thanks. 